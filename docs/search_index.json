[["index.html", "Data Analysis for Researchers 2021 About Course Contents", " Data Analysis for Researchers 2021 DS SL 2022-11-30 About This is a lecture note of a course jointly taught in the Winter term AY2021. First we start with the contents of the slides and gradually include other contents into this book. Course Contents Introduction: - About the Course - Data Science - Data: Big Data, Public Data, etc. - R, RStudio, [RStudio Cloud] - [A] Self-introduction, etc. Exploratory Data Analysis (EDA) 1 - R Basics with RStudio and/or RStudio.cloud; R Script - [P] swirl - [A] Quiz on Moodle Exploratory Data Analysis (EDA) 2 - R Markdown; R Notebook - Data Visualization - Package: tidyverse: Introduction to ggplot2, Explore, I - Data: datasets - [P] RStudio Primers: Programming Basics, Visualization Basics - [A] R Notebook using datasets package data Exploratory Data Analysis (EDA) 3 - Data Wrangling - Package: tidyverse, WDI: Introduction to tibble, dplyr, WDI, WDIsearch - Data: World Bank Data - [P] RStudio Primers: Work with Data, Wrangle, I - [A] WDI: submit R Notebook file with a4_ID.nb.html Exploratory Data Analysis (EDA) 4 - Tidy Data - Package: tidyverse, dplyr: pivoting - Data: WDI, UN data, CLASS.xlsx, JHU Covid-19 Data - [P] RStudio Primers: Visualize Data – Explore, II - [A] EDA on Public Data: submit R Notebook file with a5_ID.nb.html Exploratory Data Analysis (EDA) 5 - Data Modeling and EDA - Package: modelr, HistData - Data: datasets::cars, datasets::iris, HistData::GaltonFamilies - [P] RStudio Primers: Tidy Your Data – Wrangle, II - [A] Modeling and EDA: submit R Notebook file with a6_ID.nb.html Topics 1 - Inference Statistics (Regression, hypothesis testing, classification, etc.) - Standardization, PPDAC - Package: tidyverse, WDI, car, modelsummary - Data: GDP using WDI, - [A] Assignment on regression analysis and PPDAC cycle Topics 2 - Categorical Variables, Analysis of Variance (ANOVA) - Classification, Causal Inference - Data: NHEFS data, datasets::iris Topics 3: Guest Lecture Presentation - Students’ Presentations - Course Round-up [P]: Practice, [A]: Assignment "],["course.html", "Chapter 1 About the Course 1.1 Coures Information 1.2 Introduction by Professor Kaizoji 1.3 Introduction in AY2020 by Suzuki", " Chapter 1 About the Course 1.1 Coures Information 1.1.1 About ‘QALL401 Data Analysis for Researchers’ An introduction to data science (DS). It is an exploratory data analysis (EDA) that is an essential part of scientific research and an evidence-based decision making of a responsible global citizen. Students acquire the knowledge and learn the necessary principles for appropriate computer utilization in making research results public in order to communicate the outcomes. Since data science supports technologies of artificial intelligence, ethical issues are becoming more and more important. We introduce R, a widely used free software environment for statistical computing and graphics, and Rmarkdown, an authoring format that enables easy creation of dynamic documents, presentations, and reports from R, supporting reproducible research and literate programming. We will experience the process of data science and set a foundation to delovep data science skills and take time to think about the ethical issues of its outcomes. Instructors: Keisuke Ishibashi, Taisei Kaizoji and Hiroshi Suzuki Description: This course will help students from many academic fields develop skills to obtain necessary information from open data, as well as make charting and graphing for visualization. Students also learn fundamentals of data analysis and write short articles including data reasoning. The laboratory work uses open software such as R, and guest lectures on data analysis for research are included. Key Words: open data, data visualization, data analysis, data reasoning, R Features: laboratory work - practicum, write short articles, guest lectures 1.2 Introduction by Professor Kaizoji 1.2.1 Introduction (Slide Presentation 1) Necessity of Information Literacy and Statistical Analysis Big Data Big Data Five V’s: Volume, Velocity, Variety, Veracity, Value HADOOP A new generation of Open Data: US Government: DATA.GOV Google Public Data Explorer Youtube Video Our World in Data Our World in Data: COVID-19 Data Analysis: COVID-19 Community Mobility Reports Explanatory Data Analysis(EDA) Getting Data Visualizing the data Statistically analyzing Making new discoveries Statistical hypothesis testing Building up a hypothesis Getting data Analyzing the data Testing the hypothesis R-Project The R language is an open source and free software programming language for statistical analysis. The R language was created by Ross Ihaka and Robert Clifford Gentleman at the University of Auckland, New Zealand. It is now maintained and extended by the R Development Core Team. R programming for beginners - Why you should use R The R Graph Gallery Extensive statistical packages 1.2.2 Data Analytics / Data Science Overview (Slide Presentation 2) What is data science / data analytics? Extract knowledge from data for decision-making and/or understanding phenomena understanding phenomena : infer mechanism that generates the observed data decision-making: determine what action to obtain the optimal output from the observed data Examples Education Is early English learning effective? Medical Is COVID-19 Vaccine effective? Politics Was lockdown beneficial in decreasing the number of new cases in infection? How about the effect for economics? Business Does online personal ad increase the sales? How much? Climate Is increase of CO2 related to the temperature increase? Health Is carbohydrate restriction effective for diet? Sports Does sacrifice bunt increase the probability of game win? Understanding phenomena and making decisions based on data Make decisions based on assumptions without basis or a few examples Understanding phenomena and making decisions based on data, while leaving the mechanisms that cause the phenomena as a black box to some extent. Inductive reasoning Elucidate the mechanisms that cause phenomena and make decisions based on those mechanisms. Example of data science Kepler Infer the planetary orbits as ellipse from the observed data (Finding a phenomenon) Before Newton discovered universal gravitation. Nightingale Find that the main cause of death among soldiers was poor sanitation in hospitals by analyzing the data on dead and sick soldiers in the Crimean War. Reduced the mortality rate from 42% to 5% (Decision- making). Before the German bacteriologist Koch discovered that bacteria were the cause of infections. Why data science / data analytics? The following two movement put the data-science approach Increase of available data =&gt; Open data (Prof. Kaizoji) Increase of computational power that can process the above huge data My Personal View of Data Science 1.2.3 R and R Studio The Comprehensive R Archive Network R Studio IDE 1.2.4 Schedule (tentative) 2021.12.08 Week 1 Guidance, R setup 2021.12.15 Week 2 Exploratory Data Analysis (EDA) 1 2021.12.22 Week 3 Exploratory Data Analysis (EDA) 2 2022.01.12 Week 4 Exploratory Data Analysis (EDA) 3 2022.01.19 Week 5: Exploratory Data Analysis (EDA) 4 2022.01.26 Week 6: Exploratory Data Analysis (EDA) 5 2022.02.02 Week 7 : Inference Statistics (Regression, hypothesis testing, classification, etc.) 1 2022.02.09 Week 8: Inference Statistics (Regression, hypothesis testing, classification, etc.) 2 2022.02.16 Week 9: Inference Statistics (Regression, hypothesis testing, classification, etc.) 3 2022.02.23 Week 10 : Project Presentation 1.2.5 Responses of Students I took an undergraduate R studio programming last year and did quite poorly. I am not very good with coding and I’m a bit out of practice with math so I had a very hard time with the class. I’m nervous about this course as programming isn’t my strong suit. But I figured I should give R studio and data analysis another try and enrolled in this class. For my bachelor thesis, I am studying the various political and social factors that influence vaccine hesitancy in the United States and Japan. A lot of my writing and research involves reading published statistical data about COVID-19 cases and COVID-19 vaccination rates. I don’t have to do any data gathering of my own for my bachelor’s thesis, but I believe I may need to for my master’s dissertation next year. I feel that the content that will be covered in this course will greatly help me learn how to navigate my own research and analysis of evidence next year. I also have one comment regarding the logistics of the class. When the professors wrote information on the whiteboard during the class, it was very difficult to see over Zoom. It was also hard to follow along with the content as we couldn’t see what was being written. Is there any way that a picture of the board can be taken and uploaded to Moodle after class, or for the professors to use an online writing tool and write on the screen of their laptops so everyone can see it instead? It was also hard to hear whenever someone would ask a question in the classroom as they did not turn their mic on to talk through Zoom and the question/answer was not repeated to the people in the Zoom call. I’m looking forward to this class! よろしくお願い致します。 I have no questions since it is still the first class but I am looking forward to learning R here. Have a wonderful week/weekend! I have an interest in getting to know the data aspects of the research, including how to best use R for data analysis. I always avoided R due to its syntax / programming / coding nature. But now, I feel I am in a position to learn it, even though it is available in Graphical User Interface format as SPSS/Stata, etc. So, I am lucky to have the chance to learn through this course. And glad to be able to learn R while studying for Masters in Public Economics at ICU. I have already downloaded R, and I am eagerly looking forward to learn it. I shall post any questions / comments I have in near future. EDIT: I have downloaded RStudio as well, as instructed during today’s lecture. Dear sensei This course will help for analyzing data of my research in ICU. The Knowledge which will gain from this course will help for my further academic development also. Thank you I decided to take this course because I am conducting a quantitative survey for my Master’s thesis and do not know how I should analyze my data since it has been a while since I have taken a course related to statistics. I hope that through this course I will be able to learn ways to analyze the data I have gathered and gain an understanding of using different data analyzing tools, such as R. Thank you so much for today’s lecture and I look forward to next week’s class. Regarding the Research presentation how could I find data which is not available in public data bases. Is it ok for we find some data from our country sources. I am very excited to have the opportunity to use R again in this class. I was a data science minor in undergrad, so I have much experience using R. However, I have not had much reason to use it for a while so this class gives me a good way to get back into it and refresh my memory. I hope I can find a way to use data analysis in R for my master’s thesis and gain inspiration from this class. I believe it is a very great tool with a variety of uses. I have no background in data science, so I am a very fresh beginner, but I was very intrigued to learn about the many ways data is a large part of our lives. Particularly the examples of how to use data sites were fascinating. I understand that using Rstudio will also maybe help me become more of a critical thinker. So far, I have used Rstudio before class as a very lovely computer calculator. I hope to apply R to humanitarian data research and make that connection, as I have steered clear of computer programming-based languages so far. I am hoping to become somewhat fluent by the end of this course. Thank you very much. Thank you for the lecture today. As a psychology major still in the undergraduate year I have never used R and have always used SPSS so I am looking forward to learning a new program. As I have heard from my graduate friends that R is better I was always unsure of why we started out with spss. It seems a little daunting as it looks like coding but it does seem a lot easier as you can automate the process once it has been typed out once. I was also wondering if I was allowed to go offline somedays and online somedays or would this cause any trouble? Through today’s class, I re-recognized the importance of data analysis in my research. I am looking forward to learning data analysis using useful softwares. As mentioned in class, understanding the mechanism is important in research of physics. Therefore, I want to acquire techniques that predict mechanism of phenomena from bunch of data. Thank you so much for your lecture today. R studio is a great software. It is not “instant” software like some other software I used, so I will try to learn more about it via your instruction. Could I have a question, please? For panel data, is R studio stronger than other software like STATA? Thank you The diagram in Ishibashi sensei’s explanation with the way data could be connected to predictions and interpreted to form hypothesis was interesting. I had been wondering where unsupervised learning sits because it is so different from supervised learning but seeing their general relation was useful. Thank you so much for introducing the R tool. Could the package be written by the individuals by using the R tool? Thank you very much to the three professors for their wonderful lectures! This semester, I will conduct the data analysis part of my graduation thesis, so this course will be very helpful to me. I hope I can master the use method of R, so that I can independently complete data analysis in the future. Thank you for the lecture today. I hope i can get familiar with R-code and use to analysis data in my research report. Thank you, Professors, for the intuitive instruction. Thank you for your efforts. I have a little experience with a few types of programming languages but R is the first time for me. I want to try my best for familiarizing data analysis using R. I missed the first class because not yet register the course. So, if it is allow I would like to join this course. I just watch the week 1 zoom video. I think, to be able to manipulating, analysis and presenting data in the way people can understand is important skill for researchers. In am interested to learn more on how to do that, and also to learn R language as one of tools to handle big data. I have question : for the class practice as mention, it will took data from public data. How about for the final project/paper for this course? First, I would like to thank the three professors for your patience and guidance, and I must apologize for my late submission of the comment, as I had submitted it incorrectly before，it will not happen again. Through the first week’s study, I generally understand the concept of data science, data analysis, the R-project, big data, and the way to find data resources. And I mainly focus on the content that researchers can achieve two objectives: understanding phenomena and decision-making through a series of activities such as data analysis, setting hypotheses (prediction), generating data models, and testing hypotheses (black boxes). 1.3 Introduction in AY2020 by Suzuki 1.3.1 Requirements and Grades Not Much The knowledge of college level linear algebra and calculus are helpful but not required Experience of computer programming is helpful but not required Important Empirical studies require asking questions and hands-on-activities Questions Interactive and creative activities Instructors will support your learning Use Moodle Collaboration and cooperation 1.3.1.1 Grades Class participation and online quizzes - 30 % Short paper: research proposal - 20 % Presentation - 10 % Final paper - 40 % 1.3.2 Data Science (DS) Data Analysis (EDA): The core of Data Science Statistical Analysis Statistics? Mathematics? Computer Science? DS is based on statistical theory and mathematics but an empirical study empirical: based on, concerned with, or verifiable by observation or experience rather than theory or pure logic Artificial Intelligence (AI): AI is a broad area Machine Learning (ML), Reinforcement Learning (RL), Deap Learning (DL) Recommendation system, development of medicine, public health issues, managements Ethical issues of AI Is AI a black box? DS supports the technology of AI as application 1.3.3 EDA by R Language and R Programming Reproducibility and literate programming Questions &gt; Data &gt; Exploration (Observation, Visualization and Modeling) * Communication Use R and R Studio, locally and online. Mainly use base R and tidyverse packages Introduce Public, Open Data and API 1.3.3.1 EDA (A diagram from R4DS by H.W. and G.G.) EDA from r4ds 1.3.4 Visualization in EDA Let’s look at examples. 1.3.4.1 Florence Nightingale (1820 – 1910) Florence Nightingale was an English social reformer, statistician and the founder of modern nursing. (wikipedia) Diagram of the Causes of Motality in the Army in the East Insights in Social History, Books and Research by Hugh Small Florence Nightingale’s Statistical Diagrams: https://www.york.ac.uk/depts/maths/histstat/small.htm Florence Nightingale Museum https://www.florence-nightingale.co.uk/learning/ Meet Miss Nightingale: https://www.florence-nightingale.co.uk/meet-miss-nightingale/ Book: A contribution to the sanitary history of the British army during the late war with Russia Project Gutenberg: Books by Nightingale, Florence Notes on Nursing: What It Is, and What It Is Not Nightingale: The Journal of the Data Visualization Society, Medium 1.3.4.2 Hans Rosling (1948 – 2017) Hans Rosling was a Swedish physician, academic, and public speaker. He was a professor of international health at Karolinska Institute[4] and was the co-founder and chairman of the Gapminder Foundation, which developed the Trendalyzer software system. (wikipedia) Books: Factfulness: Ten Reasons We’re Wrong About The World - And Why Things Are Better Than You Think, 2018 How I Learned to Understand the World: A Memoir, 2020 Gapminder: https://www.gapminder.org You are probably wrong about: Upgrade Your World View Bubble Chart: Income vs Life Expectancy over time, 1800 - 2020 How many variables? Videos: The best stats you’ve ever seen, Hans Rosling Google Public Data: Example: World Development Indicator 1.3.4.2.1 Factfulness is … From the book recognizing when a decision feels urgent and remembering that it rarely is. To control the urgency instinct, take small steps. Take a breath. When your urgency instinct is triggered, your other instincts kick in and your analysis shuts down. Ask for more time and more information. It’s rarely now or never and it’s rarely either/or. Insist on the data. If something is urgent and important, it should be measured. Beware of data that is relevant but inaccurate, or accurate but irrelevant. Only relevant and accurate data is useful. Beware of fortune-tellers. Any prediction about the future is uncertain. Be wary of predictions that fail to acknowledge that. Insist on a full range of scenarios, never just the best or worst case. Ask how often such predictions have been right before. Be wary of drastic action. Ask what the side effects will be. Ask how the idea has been tested. Step-by-step practical improvements, and evaluation of their impact, are less dramatic but usually more effective. 1.3.5 Data Science (Wikipedia) An inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data. Data science is related to data mining, machine learning and big data. A “concept to unify statistics, data analysis and their related methods” in order to “understand and analyze actual phenomena” with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, domain knowledge and information science. Turing award winner Jim Gray imagined data science as a “fourth paradigm” of science (empirical, theoretical, computational and now data-driven) and asserted that “everything about science is changing because of the impact of information technology” and the data deluge. 1.3.6 Data for Data Science 1.3.6.1 We will use: Create a small dataset to understand the operations, and a reasonably large dataset by a simulation Collection of datasets attached to R and packages ready for EDA Real world data - open, public data 1.3.6.2 For data or chart representing statistical data Find the source of data Find the definitions and metadata of the data How is the data collected? What does each variable represent? Can we use it withtout permission? Ask questions What does the data tell us? Related questions? For evidence based data analysis leading to extract knowledge and insights for decision making. 1.3.7 Open and Public Data 1.3.7.1 World Bank: Open Data Defined The term ``Open Data’’ has a very precise meaning. Data or content is open if anyone is free to use, re-use or redistribute it, subject at most to measures that preserve provenance and openness. The data must be , which means they must be placed in the public domain or under liberal terms of use with minimal restrictions. The data must be , which means they must be published in electronic formats that are machine readable and non-proprietary, so that anyone can access and use the data using common, freely available software tools. Data must also be publicly available and accessible on a public server, without password or firewall restrictions. To make Open Data easier to find, most organizations create and manage Open Data catalogs. 1.3.8 A List of Open Data Catalogue 1.3.8.1 International Institutions World Bank: New Ways of Looking at Poverty Open Data: https://data.worldbank.org World Development Indicators: http://datatopics.worldbank.org/world-development-indicators/ UN Data: http://data.un.org WHO Data: https://www.who.int/gho/en/ OECD: https://data.oecd.org European Union: http://data.europa.eu/euodp/en/home African Union: https://au.int/en/ea/statistics 1.3.8.2 Goverments United States: https://www.data.gov United Kingdom: https://data.gov.uk China: http://www.stats.gov.cn/english/ Japan: https://www.data.go.jp/list-of-database/?lang=en 1.3.8.3 Other Open Public Data Google Public Data Explore: https://www.google.com/publicdata/directory?hl=en_US Google Dataset Search: https://toolbox.google.com/datasetsearch Google Trends: https://trends.google.com/trends/?geo=US Open Knowledge Foundation: https://okfn.org Global Open Data Index: https://index.okfn.org A global, non-profit network that promotes and shares information at no charge, including both content and data. It was founded by Rufus Pollock on 20 May 2004 and launched on 24 May 2004 in Cambridge, UK. It is incorporated in England and Wales as a company limited by guarantee. (Wikipedia) Our World in Data: https://ourworldindata.org A scientific online publication that focuses on large global problems such as poverty, disease, hunger, climate change, war, existential risks, and inequality. The publication’s founder is the social historian and development economist Max Roser. The research team is based at the University of Oxford. (Wikipedia) 1.3.9 What is DS? Why DS? For researchers? Creation of Values Starts from Good Questions 1.3.9.1 Covid-19 JHU: https://coronavirus.jhu.edu/map.html WHO: https://covid19.who.int Our World in Data: https://ourworldindata.org/coronavirus jag-Japan: Toyo Keizai: https://toyokeizai.net/sp/visual/tko/covid19/en.html Public Health On Call: https://www.jhsph.edu/podcasts/public-health-on-call/ 001 001 - Global Preparedness, Misinformation and Community Transmission 2019 Global Health Security Index https://www.ghsindex.org Menu: Country Ranking 1.3.10 The First Step: Type of Variables What are varibles? How many variables? Quantitative variable? Qualitative variable? Numerical variable? Categorical variable? In R, there are six types: Double Integer Character Logical Raw Complex Study a, b, c, d carefully. 0, 1, 2, … can be double, integer, character, and logical symbols T and F can be computed as 1 and 0 "],["eda1.html", "Chapter 2 Exploratory Data Analysis (EDA) 1 2.1 R with R Studio and/or R Studio.cloud 2.2 Practicum: Swirl and more on R Script", " Chapter 2 Exploratory Data Analysis (EDA) 1 2.1 R with R Studio and/or R Studio.cloud 2.1.1 Course Contents 2020-12-08: Introduction: About the course - An introduction to open and public data, and data science 2020-12-15: Exploratory Data Analysis (EDA) 1 [lead by hs] - R Basics with RStudio and/or RStudio.cloud; R Script, swirl 2021-12-22: Exploratory Data Analysis (EDA) 2 [lead by hs] - R Markdown; Introduction to tidyverse; RStudio Primers 2022-01-12: Exploratory Data Analysis (EDA) 3 [lead by hs] - Introduction to tidyverse; Public Data, WDI, etc 2022-01-19: Exploratory Data Analysis (EDA) 4 [lead by hs] - Introduction to tidyverse; WDI, UN, WHO, etc 2022-01-26: Exploratory Data Analysis (EDA) 5 [lead by hs] - Introduction to tidyverse; WDI, OECD, US gov, etc 2022-02-02: Inference Statistics 1 2022-02-09: Inference Statistics 2 2022-02-16: Inference Statistics 3 2022-02-23: Project Presentation 2.1.2 Learning Resources, I 2.1.2.1 Textbooks “R for Data Science” by Hadley Wickham and Garrett Grolemund: Free Online Book: https://r4ds.had.co.nz “R for Data Science: Exercise Solutions” by Jeffrey B. Arnold Free Online Book: https://jrnold.github.io/r4ds-exercise-solutions/ 2.1.2.2 Other Resources (MOOCs) edX: HarvardX Data Science - 9 courses. Textbook: “Introduction to Data Science” by Rafael A. Irizarry. Free Online Book by Rafael A. Irizarry. coursera: JHU Data Science - 10 courses. List of Companion Books: “R Programming for Data Science” by Roger Peng. Free Online Book by Roger Peng. “Exploratory Data Analysis with R” by Roger Peng. Free online Book by Roger Peng. “Report Writing for Data Science in R” by Roger Peng “Statistical Inference for Data Science” by Brian Caffo “Regression Modeling for Data Science in R” by Brian Caffo 2.1.3 EDA1: Contents What is R? Why R? the First Example What is R Studio and R Studio Cloud? Installation of R and R Studio   R Studio Basics R Studio Cloud Basics Project, R Console R Basics using an R Script {swirl}: Learn R, in R EDA: Coronavirus, the first example Assignment 1 and Assignment 2 in Moodle 2.1.4 What is R? 2.1.4.1 R (programming language), Wikipedia R is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis. A GNU package, the official R software environment is written primarily in C, Fortran, and R itself (thus, it is partially self-hosting) and is freely available under the GNU General Public License. 2.1.4.2 History of R and more “R Programming for Data Science” by Roger Peng Chapter 2. History and Overview of R Overview and History of R: Youtube video 2.1.5 Why R? – Responses by Hadley Wickham 2.1.5.1 r4ds: R is a great place to start your data science journey because R is an environment designed from the ground up to support data science. R is not just a programming language, but it is also an interactive environment for doing data science. To support interaction, R is a much more flexible language than many of its peers. 2.1.5.2 Why R today? When you talk about choosing programming languages, I always say you shouldn’t pick them based on technical merits, but rather pick them based on the community. And I think the R community is like really, really strong, vibrant, free, welcoming, and embraces a wide range of domains. So, if there are like people like you using R, then your life is going to be much easier. That’s the first reason. Interview: “Advice to Young (and Old) Programmers, H. Wickham” 2.1.6 The First Example plot(cars) plot(cars) # cars: Speed and Stopping Distances of Cars abline(lm(cars$dist~cars$speed)) head(cars) ## speed dist ## 1 4 2 ## 2 4 10 ## 3 7 4 ## 4 7 22 ## 5 8 16 ## 6 9 10 summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 summary(lm(cars$dist~cars$speed)) ## ## Call: ## lm(formula = cars$dist ~ cars$speed) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.069 -9.525 -2.272 9.215 43.201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.5791 6.7584 -2.601 0.0123 * ## cars$speed 3.9324 0.4155 9.464 1.49e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.38 on 48 degrees of freedom ## Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 ## F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 2.1.7 What is RStudio? https://rstudio.com RStudio is an integrated development environment, or IDE, for R programming. 2.1.7.1 R Studio (Wikipedia) RStudio is an integrated development environment (IDE) for R, a programming language for statistical computing and graphics. It is available in two formats: RStudio Desktop is a regular desktop application while RStudio Server runs on a remote server and allows accessing RStudio using a web browser. 2.1.7.2 R Studio Cloud https://rstudio.cloud RStudio Cloud is a lightweight, cloud-based solution that allows anyone to do, share, teach and learn data science online. 2.1.8 Installation of R and R Studio 2.1.8.1 R Installation To download R, go to CRAN, the comprehensive R archive network. CRAN is composed of a set of mirror servers distributed around the world and is used to distribute R and R packages. Don’t try and pick a mirror that’s close to you: instead use the cloud mirror, https://cloud.r-project.org, which automatically figures it out for you. A new major version of R comes out once a year, and there are 2-3 minor releases each year. It’s a good idea to update regularly. 2.1.8.2 R Studio Installation Download and install it from http://www.rstudio.com/download. RStudio is updated a couple of times a year. When a new version is available, RStudio will let you know. 2.1.9 R Studio 2.1.9.1 The First Step Start R Studio Application Top Menu: File &gt; New Project &gt; New Directory &gt; New Project &gt; Directory name or Browse the directory and choose the parent directory you want to create the directory 2.1.9.2 When You Start the Project Go to the directory you created Double click _‘Directory Name’.Rproj Or, Start R Studio File &gt; Open Project (or choose from Recent Project) In this way the working directory of the session is set to the project directory and R can search releted files without difficulty (getwd(), setwd()) 2.1.10 R Studio Cloud 2.1.10.1 Cloud Free Up to 15 projects total 1 shared space (5 members and 10 projects max) 15 project hours per month Up to 1 GB RAM per project Up to 1 CPU per project Up to 1 hour background execution time 2.1.10.2 How to Start R Studio Cloud Go to https://rstudio.cloud/ Sign Up: top right Email address or Google account New Project: Project Name R Console 2.1.11 Let’s Try R Basics 2.1.11.1 R Basics Let’s Try R on R Studio and/or R Studio Cloud 2.1.11.2 R Scripts Copy a script in Moodle: basics.R In RStudio (Workspace in RStudio.cloud, Project in RStudio) choose File &gt; New File &gt; R Script and paste it. Choose File &gt; Save with a name; e.g. basics (.R will be added automatically) 2.1.11.3 Helpful Resources Cheet Sheet in RStudio: https://www.rstudio.com/resources/cheatsheets/ RStudio IED Base R Cheat Sheet ‘Quick R’ by DataCamp: https://www.statmethods.net/management 2.1.12 More on R Script: Examples 2.1.12.1 R Scripts in Moodle basics.R coronavirus.R Copy a script in Moodle: {file name}.R In RStudio (Workspace in RStudio.cloud, Project in RStudio) choose File &gt; New File &gt; R Script and paste it. Choose File &gt; Save with a name; e.g. {file names} (.R will be added automatically) 2.1.13 Practicum: R Studio Cloud (or R Studio) and R basics 2.1.13.1 Let’s Try R Basics R Studio Cloud Create an account Create a Project R Studio Basics R Basics basics.R 2.1.13.2 Basics.R The script with the outputs. ################# # # basics.R # ################ # &#39;Quick R&#39; by DataCamp may be a handy reference: # https://www.statmethods.net/management/index.html # Cheat Sheet at RStudio: https://www.rstudio.com/resources/cheatsheets/ # Base R Cheat Sheet: https://github.com/rstudio/cheatsheets/raw/main/base-r.pdf # To execute the line: Control + Enter (Window and Linux), Command + Enter (Mac) ## try your experiments on the console ## calculator 3 + 7 ## [1] 10 ### +, -, *, /, ^ (or **), %%, %/% 3 + 10 / 2 ## [1] 8 3^2 ## [1] 9 2^3 ## [1] 8 2*2*2 ## [1] 8 ### assignment: &lt;-, (=, -&gt;, assign()) x &lt;- 5 x ## [1] 5 #### object_name &lt;- value, &#39;&lt;-&#39; shortcut: Alt (option) + &#39;-&#39; (hyphen or minus) #### Object names must start with a letter and can only contain letter, numbers, _ and . this_is_a_long_name &lt;- 5^3 this_is_a_long_name ## [1] 125 char_name &lt;- &quot;What is your name?&quot; char_name ## [1] &quot;What is your name?&quot; #### Use &#39;tab completion&#39; and &#39;up arrow&#39; ### ls(): list of all assignments ls() ## [1] &quot;char_name&quot; &quot;this_is_a_long_name&quot; &quot;x&quot; ls.str() ## char_name : chr &quot;What is your name?&quot; ## this_is_a_long_name : num 125 ## x : num 5 #### check Environment in the upper right pane ### (atomic) vectors 5:10 ## [1] 5 6 7 8 9 10 a &lt;- seq(5,10) a ## [1] 5 6 7 8 9 10 b &lt;- 5:10 identical(a,b) ## [1] TRUE seq(5,10,2) # same ase seq(from = 5, to = 10, by = 2) ## [1] 5 7 9 c1 &lt;- seq(0,100, by = 10) c2 &lt;- seq(0,100, length.out = 10) c1 ## [1] 0 10 20 30 40 50 60 70 80 90 100 c2 ## [1] 0.00000 11.11111 22.22222 33.33333 44.44444 55.55556 66.66667 ## [8] 77.77778 88.88889 100.00000 length(c1) ## [1] 11 #### ? seq ? length ? identical (die &lt;- 1:6) ## [1] 1 2 3 4 5 6 zero_one &lt;- c(0,1) # same as 0:1 die + zero_one # c(1,2,3,4,5,6) + c(0,1). re-use ## [1] 1 3 3 5 5 7 d1 &lt;- rep(1:3,2) # repeat d1 ## [1] 1 2 3 1 2 3 die == d1 ## [1] TRUE TRUE TRUE FALSE FALSE FALSE d2 &lt;- as.character(die == d1) d2 ## [1] &quot;TRUE&quot; &quot;TRUE&quot; &quot;TRUE&quot; &quot;FALSE&quot; &quot;FALSE&quot; &quot;FALSE&quot; d3 &lt;- as.numeric(die == d1) d3 ## [1] 1 1 1 0 0 0 ### class() for class and typeof() for mode ### class of vectors: numeric, charcters, logical ### types of vectors: doubles, integers, characters, logicals (complex and raw) typeof(d1); class(d1) ## [1] &quot;integer&quot; ## [1] &quot;integer&quot; typeof(d2); class(d2) ## [1] &quot;character&quot; ## [1] &quot;character&quot; typeof(d3); class(d3) ## [1] &quot;double&quot; ## [1] &quot;numeric&quot; sqrt(2) ## [1] 1.414214 sqrt(2)^2 ## [1] 2 sqrt(2)^2 - 2 ## [1] 4.440892e-16 typeof(sqrt(2)) ## [1] &quot;double&quot; typeof(2) ## [1] &quot;double&quot; typeof(2L) ## [1] &quot;integer&quot; 5 == c(5) ## [1] TRUE length(5) ## [1] 1 ### Subsetting (A_Z &lt;- LETTERS) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot; &quot;G&quot; &quot;H&quot; &quot;I&quot; &quot;J&quot; &quot;K&quot; &quot;L&quot; &quot;M&quot; &quot;N&quot; &quot;O&quot; &quot;P&quot; &quot;Q&quot; &quot;R&quot; &quot;S&quot; ## [20] &quot;T&quot; &quot;U&quot; &quot;V&quot; &quot;W&quot; &quot;X&quot; &quot;Y&quot; &quot;Z&quot; A_F &lt;- A_Z[1:6] A_F ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot; A_F[3] ## [1] &quot;C&quot; A_F[c(3,5)] ## [1] &quot;C&quot; &quot;E&quot; large &lt;- die &gt; 3 large ## [1] FALSE FALSE FALSE TRUE TRUE TRUE even &lt;- die %in% c(2,4,6) even ## [1] FALSE TRUE FALSE TRUE FALSE TRUE A_F[large] ## [1] &quot;D&quot; &quot;E&quot; &quot;F&quot; A_F[even] ## [1] &quot;B&quot; &quot;D&quot; &quot;F&quot; A_F[die &lt; 4] ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; ### Compare df with df1 &lt;- data.frame(number = die, alphabet = A_F) df &lt;- data.frame(number = die, alphabet = A_F, stringsAsFactors = FALSE) df ## number alphabet ## 1 1 A ## 2 2 B ## 3 3 C ## 4 4 D ## 5 5 E ## 6 6 F df$number ## [1] 1 2 3 4 5 6 df$alphabet ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot; df[3,2] ## [1] &quot;C&quot; df[4,1] ## [1] 4 df[1] ## number ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## 6 6 class(df[1]) ## [1] &quot;data.frame&quot; class(df[[1]]) ## [1] &quot;integer&quot; identical(df[[1]], die) ## [1] TRUE identical(df[1],die) ## [1] FALSE #################### # The First Example #################### plot(cars) # Help ? cars # cars is in the &#39;datasets&#39; package data() # help(cars) does the same as ? cars # You can use Help tab in the right bottom pane help(plot) ## トピック &#39;plot&#39; に対するヘルプが以下のパッケージ中にありました: ## ## Package Library ## graphics /Library/Frameworks/R.framework/Versions/4.2/Resources/library ## base /Library/Frameworks/R.framework/Resources/library ## ## ## 最初にマッチしたものを使っています ... ? par head(cars) ## speed dist ## 1 4 2 ## 2 4 10 ## 3 7 4 ## 4 7 22 ## 5 8 16 ## 6 9 10 str(cars) ## &#39;data.frame&#39;: 50 obs. of 2 variables: ## $ speed: num 4 4 7 7 8 9 10 10 10 11 ... ## $ dist : num 2 10 4 22 16 10 18 26 34 17 ... summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 x &lt;- cars$speed y &lt;- cars$dist min(x) ## [1] 4 mean(x) ## [1] 15.4 quantile(x) ## 0% 25% 50% 75% 100% ## 4 12 15 19 25 plot(cars) abline(lm(cars$dist ~ cars$speed)) summary(lm(cars$dist ~ cars$speed)) ## ## Call: ## lm(formula = cars$dist ~ cars$speed) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.069 -9.525 -2.272 9.215 43.201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.5791 6.7584 -2.601 0.0123 * ## cars$speed 3.9324 0.4155 9.464 1.49e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.38 on 48 degrees of freedom ## Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 ## F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 boxplot(cars) hist(cars$speed) hist(cars$dist) hist(cars$dist, breaks = seq(0,120, 10)) 2.1.13.3 Basic Reference An Introduction to R 2.2 Practicum: Swirl and more on R Script 2.2.1 Swirl: An interactive learning environment for R and statistics {swirl} website: https://swirlstats.com JHU Data Science in coursera uses swirl for exercises. 2.2.1.1 Swirl Courses R Programming: The basics of programming in R Regression Models: The basics of regression modeling in R Statistical Inference: The basics of statistical inference in R Exploratory Data Analysis: The basics of exploring data in R You can install other swirl courses as well Swirl Courses Organized by Title Swirl Courses Organized by Author’s Name Github: swirl courses install_course(\"Course Name Here\") 2.2.2 Install and Start Swirl Courses 2.2.2.1 Three Steps to Start Swirl install.packages(&quot;swirl&quot;) # Only the first time. library(swirl) # Everytime you start swirl swirl() # Everytime you start or resume swirl 2.2.2.2 R Programming: The basics of programming in R 1: Basic Building Blocks 2: Workspace and Files 3: Sequences of Numbers 4: Vectors 5: Missing Values 6: Subsetting Vectors 7: Matrices and Data Frames 8: Logic 9: Functions 10: lapply and sapply 11: vapply and tapply 12: Looking at Data 13: Simulation 14: Dates and Times 15: Base Graphics 2.2.2.3 Recommended Sections in Order 1, 3, 4, 5, 6, 7, 12, 15, 14, 8, 9, 10, 11, 13, 2 Section 2 discusses the directories and file systems of a computer Sections 9, 10, 11 are for programming 2.2.2.4 Controling a swirl Session … &lt;– That’s your cue to press Enter to continue You can exit swirl and return to the R prompt (&gt;) at any time by pressing the Esc key. If you are already at the prompt, type bye() to exit and save your progress. When you exit properly, you’ll see a short message letting you know you’ve done so. When you are at the R prompt (&gt;): Typing skip() allows you to skip the current question. Typing play() lets you experiment with R on your own; swirl will ignore what you do… UNTIL you type nxt() which will regain swirl’s attention. Typing bye() causes swirl to exit. Your progress will be saved. Typing main() returns you to swirl’s main menu. Typing info() displays these options again. 2.2.3 The First EDA using coronavirus.R Pre-installed datasets R Script To access shortcuts, type Option + Shift + K on a Mac, or Alt + Shift + K on Linux and Windows. EDA (A diagram from R4DS by H.W. and G.G.) 2.2.4 Basics of Fundamentals of Statistics 2.2.4.1 R Commands Related to R Basics Fundamentals of Statistics: statistical measurements such as mean: mean() or mean(x, na.rm = TRUE) median: median() or median(x, na.rm = TRUE) quantile: quantile() or quantile(x, na.rm = TRUE) variance: var() or var(x, na.rm = TRUE) standard deviation: sd() covariance: cov() correlation: cor() summary() 2.2.5 Summary 2.2.5.1 Please check the following Installation of R Installation of R Studio Login to RStudio.cloud swirl: R Programming Try 1, 3, 4, 5, 6, 7, 12, 15 R Script basics.R - try similar commands coronavirus.R - try different Regions and Periods 2.2.5.2 coronavirus.R The script and its outputs. coronavirus.csv is too large # https://coronavirus.jhu.edu/map.html # JHU Covid-19 global time series data # See R pakage coronavirus at: https://github.com/RamiKrispin/coronavirus # Data taken from: https://github.com/RamiKrispin/coronavirus/tree/master/csv # Last Updated Sys.Date() ## [1] &quot;2022-11-30&quot; ## Download and read csv (comma separated value) file coronavirus &lt;- read.csv(&quot;https://github.com/RamiKrispin/coronavirus/raw/master/csv/coronavirus.csv&quot;) # write.csv(coronavirus, &quot;data/coronavirus.csv&quot;) ## Summaries and structures of the data head(coronavirus) ## date province country lat long type cases uid iso2 iso3 ## 1 2020-01-22 Alberta Canada 53.9333 -116.5765 confirmed 0 12401 CA CAN ## 2 2020-01-23 Alberta Canada 53.9333 -116.5765 confirmed 0 12401 CA CAN ## 3 2020-01-24 Alberta Canada 53.9333 -116.5765 confirmed 0 12401 CA CAN ## 4 2020-01-25 Alberta Canada 53.9333 -116.5765 confirmed 0 12401 CA CAN ## 5 2020-01-26 Alberta Canada 53.9333 -116.5765 confirmed 0 12401 CA CAN ## 6 2020-01-27 Alberta Canada 53.9333 -116.5765 confirmed 0 12401 CA CAN ## code3 combined_key population continent_name continent_code ## 1 124 Alberta, Canada 4413146 North America &lt;NA&gt; ## 2 124 Alberta, Canada 4413146 North America &lt;NA&gt; ## 3 124 Alberta, Canada 4413146 North America &lt;NA&gt; ## 4 124 Alberta, Canada 4413146 North America &lt;NA&gt; ## 5 124 Alberta, Canada 4413146 North America &lt;NA&gt; ## 6 124 Alberta, Canada 4413146 North America &lt;NA&gt; str(coronavirus) ## &#39;data.frame&#39;: 888636 obs. of 15 variables: ## $ date : chr &quot;2020-01-22&quot; &quot;2020-01-23&quot; &quot;2020-01-24&quot; &quot;2020-01-25&quot; ... ## $ province : chr &quot;Alberta&quot; &quot;Alberta&quot; &quot;Alberta&quot; &quot;Alberta&quot; ... ## $ country : chr &quot;Canada&quot; &quot;Canada&quot; &quot;Canada&quot; &quot;Canada&quot; ... ## $ lat : num 53.9 53.9 53.9 53.9 53.9 ... ## $ long : num -117 -117 -117 -117 -117 ... ## $ type : chr &quot;confirmed&quot; &quot;confirmed&quot; &quot;confirmed&quot; &quot;confirmed&quot; ... ## $ cases : int 0 0 0 0 0 0 0 0 0 0 ... ## $ uid : int 12401 12401 12401 12401 12401 12401 12401 12401 12401 12401 ... ## $ iso2 : chr &quot;CA&quot; &quot;CA&quot; &quot;CA&quot; &quot;CA&quot; ... ## $ iso3 : chr &quot;CAN&quot; &quot;CAN&quot; &quot;CAN&quot; &quot;CAN&quot; ... ## $ code3 : int 124 124 124 124 124 124 124 124 124 124 ... ## $ combined_key : chr &quot;Alberta, Canada&quot; &quot;Alberta, Canada&quot; &quot;Alberta, Canada&quot; &quot;Alberta, Canada&quot; ... ## $ population : num 4413146 4413146 4413146 4413146 4413146 ... ## $ continent_name: chr &quot;North America&quot; &quot;North America&quot; &quot;North America&quot; &quot;North America&quot; ... ## $ continent_code: chr NA NA NA NA ... coronavirus$date &lt;- as.Date(coronavirus$date) str(coronavirus) ## &#39;data.frame&#39;: 888636 obs. of 15 variables: ## $ date : Date, format: &quot;2020-01-22&quot; &quot;2020-01-23&quot; ... ## $ province : chr &quot;Alberta&quot; &quot;Alberta&quot; &quot;Alberta&quot; &quot;Alberta&quot; ... ## $ country : chr &quot;Canada&quot; &quot;Canada&quot; &quot;Canada&quot; &quot;Canada&quot; ... ## $ lat : num 53.9 53.9 53.9 53.9 53.9 ... ## $ long : num -117 -117 -117 -117 -117 ... ## $ type : chr &quot;confirmed&quot; &quot;confirmed&quot; &quot;confirmed&quot; &quot;confirmed&quot; ... ## $ cases : int 0 0 0 0 0 0 0 0 0 0 ... ## $ uid : int 12401 12401 12401 12401 12401 12401 12401 12401 12401 12401 ... ## $ iso2 : chr &quot;CA&quot; &quot;CA&quot; &quot;CA&quot; &quot;CA&quot; ... ## $ iso3 : chr &quot;CAN&quot; &quot;CAN&quot; &quot;CAN&quot; &quot;CAN&quot; ... ## $ code3 : int 124 124 124 124 124 124 124 124 124 124 ... ## $ combined_key : chr &quot;Alberta, Canada&quot; &quot;Alberta, Canada&quot; &quot;Alberta, Canada&quot; &quot;Alberta, Canada&quot; ... ## $ population : num 4413146 4413146 4413146 4413146 4413146 ... ## $ continent_name: chr &quot;North America&quot; &quot;North America&quot; &quot;North America&quot; &quot;North America&quot; ... ## $ continent_code: chr NA NA NA NA ... range(coronavirus$date) ## [1] &quot;2020-01-22&quot; &quot;2022-11-29&quot; unique(coronavirus$country) ## [1] &quot;Canada&quot; &quot;United Kingdom&quot; ## [3] &quot;China&quot; &quot;Netherlands&quot; ## [5] &quot;Australia&quot; &quot;New Zealand&quot; ## [7] &quot;Denmark&quot; &quot;France&quot; ## [9] &quot;Afghanistan&quot; &quot;Albania&quot; ## [11] &quot;Algeria&quot; &quot;Andorra&quot; ## [13] &quot;Angola&quot; &quot;Antarctica&quot; ## [15] &quot;Antigua and Barbuda&quot; &quot;Argentina&quot; ## [17] &quot;Armenia&quot; &quot;Austria&quot; ## [19] &quot;Azerbaijan&quot; &quot;Bahamas&quot; ## [21] &quot;Bahrain&quot; &quot;Bangladesh&quot; ## [23] &quot;Barbados&quot; &quot;Belarus&quot; ## [25] &quot;Belgium&quot; &quot;Belize&quot; ## [27] &quot;Benin&quot; &quot;Bhutan&quot; ## [29] &quot;Bolivia&quot; &quot;Bosnia and Herzegovina&quot; ## [31] &quot;Botswana&quot; &quot;Brazil&quot; ## [33] &quot;Brunei&quot; &quot;Bulgaria&quot; ## [35] &quot;Burkina Faso&quot; &quot;Burma&quot; ## [37] &quot;Burundi&quot; &quot;Cabo Verde&quot; ## [39] &quot;Cambodia&quot; &quot;Cameroon&quot; ## [41] &quot;Central African Republic&quot; &quot;Chad&quot; ## [43] &quot;Chile&quot; &quot;Colombia&quot; ## [45] &quot;Comoros&quot; &quot;Congo (Brazzaville)&quot; ## [47] &quot;Congo (Kinshasa)&quot; &quot;Costa Rica&quot; ## [49] &quot;Cote d&#39;Ivoire&quot; &quot;Croatia&quot; ## [51] &quot;Cuba&quot; &quot;Cyprus&quot; ## [53] &quot;Czechia&quot; &quot;Diamond Princess&quot; ## [55] &quot;Djibouti&quot; &quot;Dominica&quot; ## [57] &quot;Dominican Republic&quot; &quot;Ecuador&quot; ## [59] &quot;Egypt&quot; &quot;El Salvador&quot; ## [61] &quot;Equatorial Guinea&quot; &quot;Eritrea&quot; ## [63] &quot;Estonia&quot; &quot;Eswatini&quot; ## [65] &quot;Ethiopia&quot; &quot;Fiji&quot; ## [67] &quot;Finland&quot; &quot;Gabon&quot; ## [69] &quot;Gambia&quot; &quot;Georgia&quot; ## [71] &quot;Germany&quot; &quot;Ghana&quot; ## [73] &quot;Greece&quot; &quot;Grenada&quot; ## [75] &quot;Guatemala&quot; &quot;Guinea&quot; ## [77] &quot;Guinea-Bissau&quot; &quot;Guyana&quot; ## [79] &quot;Haiti&quot; &quot;Holy See&quot; ## [81] &quot;Honduras&quot; &quot;Hungary&quot; ## [83] &quot;Iceland&quot; &quot;India&quot; ## [85] &quot;Indonesia&quot; &quot;Iran&quot; ## [87] &quot;Iraq&quot; &quot;Ireland&quot; ## [89] &quot;Israel&quot; &quot;Italy&quot; ## [91] &quot;Jamaica&quot; &quot;Japan&quot; ## [93] &quot;Jordan&quot; &quot;Kazakhstan&quot; ## [95] &quot;Kenya&quot; &quot;Kiribati&quot; ## [97] &quot;Korea, North&quot; &quot;Korea, South&quot; ## [99] &quot;Kosovo&quot; &quot;Kuwait&quot; ## [101] &quot;Kyrgyzstan&quot; &quot;Laos&quot; ## [103] &quot;Latvia&quot; &quot;Lebanon&quot; ## [105] &quot;Lesotho&quot; &quot;Liberia&quot; ## [107] &quot;Libya&quot; &quot;Liechtenstein&quot; ## [109] &quot;Lithuania&quot; &quot;Luxembourg&quot; ## [111] &quot;Madagascar&quot; &quot;Malawi&quot; ## [113] &quot;Malaysia&quot; &quot;Maldives&quot; ## [115] &quot;Mali&quot; &quot;Malta&quot; ## [117] &quot;Marshall Islands&quot; &quot;Mauritania&quot; ## [119] &quot;Mauritius&quot; &quot;Mexico&quot; ## [121] &quot;Micronesia&quot; &quot;Moldova&quot; ## [123] &quot;Monaco&quot; &quot;Mongolia&quot; ## [125] &quot;Montenegro&quot; &quot;Morocco&quot; ## [127] &quot;Mozambique&quot; &quot;MS Zaandam&quot; ## [129] &quot;Namibia&quot; &quot;Nauru&quot; ## [131] &quot;Nepal&quot; &quot;Nicaragua&quot; ## [133] &quot;Niger&quot; &quot;Nigeria&quot; ## [135] &quot;North Macedonia&quot; &quot;Norway&quot; ## [137] &quot;Oman&quot; &quot;Pakistan&quot; ## [139] &quot;Palau&quot; &quot;Panama&quot; ## [141] &quot;Papua New Guinea&quot; &quot;Paraguay&quot; ## [143] &quot;Peru&quot; &quot;Philippines&quot; ## [145] &quot;Poland&quot; &quot;Portugal&quot; ## [147] &quot;Qatar&quot; &quot;Romania&quot; ## [149] &quot;Russia&quot; &quot;Rwanda&quot; ## [151] &quot;Saint Kitts and Nevis&quot; &quot;Saint Lucia&quot; ## [153] &quot;Saint Vincent and the Grenadines&quot; &quot;Samoa&quot; ## [155] &quot;San Marino&quot; &quot;Sao Tome and Principe&quot; ## [157] &quot;Saudi Arabia&quot; &quot;Senegal&quot; ## [159] &quot;Serbia&quot; &quot;Seychelles&quot; ## [161] &quot;Sierra Leone&quot; &quot;Singapore&quot; ## [163] &quot;Slovakia&quot; &quot;Slovenia&quot; ## [165] &quot;Solomon Islands&quot; &quot;Somalia&quot; ## [167] &quot;South Africa&quot; &quot;South Sudan&quot; ## [169] &quot;Spain&quot; &quot;Sri Lanka&quot; ## [171] &quot;Sudan&quot; &quot;Summer Olympics 2020&quot; ## [173] &quot;Suriname&quot; &quot;Sweden&quot; ## [175] &quot;Switzerland&quot; &quot;Syria&quot; ## [177] &quot;Taiwan*&quot; &quot;Tajikistan&quot; ## [179] &quot;Tanzania&quot; &quot;Thailand&quot; ## [181] &quot;Timor-Leste&quot; &quot;Togo&quot; ## [183] &quot;Tonga&quot; &quot;Trinidad and Tobago&quot; ## [185] &quot;Tunisia&quot; &quot;Turkey&quot; ## [187] &quot;Tuvalu&quot; &quot;Uganda&quot; ## [189] &quot;Ukraine&quot; &quot;United Arab Emirates&quot; ## [191] &quot;Uruguay&quot; &quot;US&quot; ## [193] &quot;Uzbekistan&quot; &quot;Vanuatu&quot; ## [195] &quot;Venezuela&quot; &quot;Vietnam&quot; ## [197] &quot;West Bank and Gaza&quot; &quot;Winter Olympics 2022&quot; ## [199] &quot;Yemen&quot; &quot;Zambia&quot; ## [201] &quot;Zimbabwe&quot; unique(coronavirus$type) ## [1] &quot;confirmed&quot; &quot;death&quot; &quot;recovery&quot; ## Set Country COUNTRY &lt;- &quot;Japan&quot; df0 &lt;- coronavirus[coronavirus$country == COUNTRY,] head(df0) ## date province country lat long type cases uid iso2 ## 183569 2020-01-22 &lt;NA&gt; Japan 36.20482 138.2529 confirmed 2 392 JP ## 183570 2020-01-23 &lt;NA&gt; Japan 36.20482 138.2529 confirmed 0 392 JP ## 183571 2020-01-24 &lt;NA&gt; Japan 36.20482 138.2529 confirmed 0 392 JP ## 183572 2020-01-25 &lt;NA&gt; Japan 36.20482 138.2529 confirmed 0 392 JP ## 183573 2020-01-26 &lt;NA&gt; Japan 36.20482 138.2529 confirmed 2 392 JP ## 183574 2020-01-27 &lt;NA&gt; Japan 36.20482 138.2529 confirmed 0 392 JP ## iso3 code3 combined_key population continent_name continent_code ## 183569 JPN 392 Japan 126476458 Asia AS ## 183570 JPN 392 Japan 126476458 Asia AS ## 183571 JPN 392 Japan 126476458 Asia AS ## 183572 JPN 392 Japan 126476458 Asia AS ## 183573 JPN 392 Japan 126476458 Asia AS ## 183574 JPN 392 Japan 126476458 Asia AS tail(df0) ## date province country lat long type cases uid iso2 ## 771815 2022-11-24 &lt;NA&gt; Japan 36.20482 138.2529 recovery 0 392 JP ## 771816 2022-11-25 &lt;NA&gt; Japan 36.20482 138.2529 recovery 0 392 JP ## 771817 2022-11-26 &lt;NA&gt; Japan 36.20482 138.2529 recovery 0 392 JP ## 771818 2022-11-27 &lt;NA&gt; Japan 36.20482 138.2529 recovery 0 392 JP ## 771819 2022-11-28 &lt;NA&gt; Japan 36.20482 138.2529 recovery 0 392 JP ## 771820 2022-11-29 &lt;NA&gt; Japan 36.20482 138.2529 recovery 0 392 JP ## iso3 code3 combined_key population continent_name continent_code ## 771815 JPN 392 Japan 126476458 Asia AS ## 771816 JPN 392 Japan 126476458 Asia AS ## 771817 JPN 392 Japan 126476458 Asia AS ## 771818 JPN 392 Japan 126476458 Asia AS ## 771819 JPN 392 Japan 126476458 Asia AS ## 771820 JPN 392 Japan 126476458 Asia AS (pop &lt;- df0$population[1]) ## [1] 126476458 df &lt;- df0[c(1,6,7,13)] str(df) ## &#39;data.frame&#39;: 3129 obs. of 4 variables: ## $ date : Date, format: &quot;2020-01-22&quot; &quot;2020-01-23&quot; ... ## $ type : chr &quot;confirmed&quot; &quot;confirmed&quot; &quot;confirmed&quot; &quot;confirmed&quot; ... ## $ cases : int 2 0 0 0 2 0 3 0 4 4 ... ## $ population: num 1.26e+08 1.26e+08 1.26e+08 1.26e+08 1.26e+08 ... head(df) ## date type cases population ## 183569 2020-01-22 confirmed 2 126476458 ## 183570 2020-01-23 confirmed 0 126476458 ## 183571 2020-01-24 confirmed 0 126476458 ## 183572 2020-01-25 confirmed 0 126476458 ## 183573 2020-01-26 confirmed 2 126476458 ## 183574 2020-01-27 confirmed 0 126476458 ### alternatively, head(df0[c(&quot;date&quot;, &quot;type&quot;, &quot;cases&quot;, &quot;population&quot;)]) ## date type cases population ## 183569 2020-01-22 confirmed 2 126476458 ## 183570 2020-01-23 confirmed 0 126476458 ## 183571 2020-01-24 confirmed 0 126476458 ## 183572 2020-01-25 confirmed 0 126476458 ## 183573 2020-01-26 confirmed 2 126476458 ## 183574 2020-01-27 confirmed 0 126476458 ### ## Set types df_confirmed &lt;- df[df$type == &quot;confirmed&quot;,] df_death &lt;- df[df$type == &quot;death&quot;,] df_recovery &lt;- df[df$data_type == &quot;recovery&quot;,] head(df_confirmed) ## date type cases population ## 183569 2020-01-22 confirmed 2 126476458 ## 183570 2020-01-23 confirmed 0 126476458 ## 183571 2020-01-24 confirmed 0 126476458 ## 183572 2020-01-25 confirmed 0 126476458 ## 183573 2020-01-26 confirmed 2 126476458 ## 183574 2020-01-27 confirmed 0 126476458 head(df_death) ## date type cases population ## 484996 2020-01-22 death 0 126476458 ## 484997 2020-01-23 death 0 126476458 ## 484998 2020-01-24 death 0 126476458 ## 484999 2020-01-25 death 0 126476458 ## 485000 2020-01-26 death 0 126476458 ## 485001 2020-01-27 death 0 126476458 head(df_recovery) ## [1] date type cases population ## &lt;0 行&gt; (または長さ 0 の row.names) ## Histogram plot(df_confirmed$date, df_confirmed$cases, type = &quot;h&quot;) plot(df_death$date, df_death$cases, type = &quot;h&quot;) # plot(df_recovered$date, df_recovered$cases, type = &quot;h&quot;) # no data for recovery ## Scatter plot and correlation plot(df_confirmed$cases, df_death$cases, type = &quot;p&quot;) cor(df_confirmed$cases, df_death$cases) ## [1] 0.716229 ## In addition set a period start_date &lt;- as.Date(&quot;2021-07-01&quot;) end_date &lt;- Sys.Date() df_date &lt;- df[df$date &gt;=start_date &amp; df$date &lt;= end_date,] ## ## Set types df_date_confirmed &lt;- df_date[df_date$type == &quot;confirmed&quot;,] df_date_death &lt;- df_date[df_date$type == &quot;death&quot;,] df_date_recovery &lt;- df_date[df_date$data_type == &quot;recovery&quot;,] head(df_date_confirmed) ## date type cases population ## 184095 2021-07-01 confirmed 1754 126476458 ## 184096 2021-07-02 confirmed 1775 126476458 ## 184097 2021-07-03 confirmed 1878 126476458 ## 184098 2021-07-04 confirmed 1485 126476458 ## 184099 2021-07-05 confirmed 1029 126476458 ## 184100 2021-07-06 confirmed 1668 126476458 head(df_date_death) ## date type cases population ## 485522 2021-07-01 death 24 126476458 ## 485523 2021-07-02 death 25 126476458 ## 485524 2021-07-03 death 9 126476458 ## 485525 2021-07-04 death 6 126476458 ## 485526 2021-07-05 death 19 126476458 ## 485527 2021-07-06 death 22 126476458 head(df_date_recovery) ## [1] date type cases population ## &lt;0 行&gt; (または長さ 0 の row.names) ## Histogram plot(df_date_confirmed$date, df_date_confirmed$cases, type = &quot;h&quot;) plot(df_date_death$date, df_date_death$cases, type = &quot;h&quot;) # plot(df_date_recovered$date, df_date_recovered$cases, type = &quot;h&quot;) # no data for recovery plot(df_date_confirmed$cases, df_date_death$cases, type = &quot;p&quot;) cor(df_date_confirmed$cases, df_date_death$cases) ## [1] 0.7289401 ### Q0. Change the values of the location and the period and see the outcomes. ### Q1. What is the correlation between df_confirmed$cases and df_death$cases? ### Q2. Do we have a larger correlation value if we shift the dates to implement the time-lag? ### Q3. Do you have any other questions to explore? #### Extra plot(df_confirmed$date, df_confirmed$cases, type = &quot;h&quot;, main = paste(&quot;Comfirmed Cases in&quot;,COUNTRY), xlab = &quot;Date&quot;, ylab = &quot;Number of Cases&quot;) 2.2.5.3 Assignment 1 and Assignment 2: Questions and a Quiz in Moodle Please complete assignments in Moodle by 2021-12-21 "],["eda2.html", "Chapter 3 Exploratory Data Analysis (EDA) 2 3.1 Part I: R Markdown for Communication 3.2 Part II: Data Visualization and Tidyverse Package 3.3 The Week Three Assignment (in Moodle) 3.4 Responses to the Week Three Assignment", " Chapter 3 Exploratory Data Analysis (EDA) 2 Course Contents 2021-12-08: Introduction: About the course - An introduction to open and public data, and data science 2021-12-15: Exploratory Data Analysis (EDA) 1 [lead by hs] - R Basics with RStudio and/or RStudio.cloud; R Script, swirl 2021-12-22: Exploratory Data Analysis (EDA) 2 [lead by hs] - R Markdown; Introduction to tidyverse; RStudio Primers 2022-01-12: Exploratory Data Analysis (EDA) 3 [lead by hs] - Introduction to tidyverse; Public Data, WDI, etc 2022-01-19: Exploratory Data Analysis (EDA) 4 [lead by hs] - Introduction to tidyverse; WDI, UN, WHO, etc 2022-01-26: Exploratory Data Analysis (EDA) 5 [lead by hs] - Introduction to tidyverse; WDI, OECD, US gov, etc 2022-02-02: Inference Statistics 1 2022-02-09: Inference Statistics 2 2022-02-16: Inference Statistics 3 2022-02-23: Project Presentation EDA and Data Visualization Communication in the EDA Cycle Data Visualization in the EDA Cycle Tidyverse Package centered at ggplot2 EDA (A diagram from R4DS by H.W. and G.G.) EDA from r4ds Contents of EDA2 Reproducible Research Literate Programming R Markdown R Notebook Formats HTML pdf MS Word Presentation and more Practicum I: R Markdown and R Notebook Introduction to tidyverse package ggplot2 ggplot(), aes(), geoms, etc. Practicum II: tidyverse, ggplot2 on R Notebook Learning Resources: RStuido Primers, etc. 3.1 Part I: R Markdown for Communication 3.1.1 Records of EDA and Communication Memo on scratch paper: R Scripts Record on a notebook: R Notebook (a type of an R Markdown format) Short paper or a digital communication: R Notebook Paper or a report: R Markdown (html, pdf, or MS Word) Presentation R Markdown with a presentation format (html, pdf, or PowerPoint) Publication of a Book BOOKDOWN: Write HTML, PDF, ePub, and Kindle books with R Markdown. Free online document is provided in pdf as well Arxive Page 3.1.2 What is R Markdown Notebook R Markdown provides an authoring framework for data science. You can use a single R Markdown file to both save and execute code generate high quality reports that can be shared with an audience R Notebooks are an implementation of Literate Programming that allows for direct interaction with R while producing a reproducible document with publication-quality output. An R Notebook is an R Markdown document with chunks that can be executed independently and interactively, with output visible immediately beneath the input. (Reference: R Markdown: The Definitive Guide, 3.2 Notebook) 3.1.3 R Studio Setup Start R Studio Create a Project Tool &gt; Install Packages rmarkdown Or on Console: install.packages(\"rmarkdown\") Tool &gt; Install Packages tinytex (for pdf generation) Let’s try! File &gt; New File &gt; R Notebook Save with a file name, say, test-notebook Preview by [Preview] button Run Code Chunk plot(cars) and then Preview again. Knit PDF, Word (and HTML) Note: R Notebooks are relatively new feature of RStudio and are only available in version 1.0 or higher of RStudio. 3.1.4 Default YAML: R Notebook, HTML, PDF, WORD --- title: &quot;The Title of This R Notebook&quot; author: &quot;Your Name&quot; date: &quot;2021-12-22&quot; output: html_notebook: default html_document: default word_document: default pdf_document: default --- Original format is output: html_document Indention matters in YAML. So it is safer to copy and paste output: to pdf_document: default R Notebook is also an HTML format, so html_notebook part may disappear after knitting in HTML. 3.1.5 An Example of R Notebooks Moodle: QALL401 2021W 2021-12-22 Examples of R Notebook Open the file R Codes with Outputs Headings, Links, Explanations, etc. [Hide] button, and [Code] button with a menu Choose: Download Rmd Save as “file_name.nb.html” in your R project directory Download and Save “jhu_covid.Rmd”. (Or, open the file in editor) Preview by [Preview] button. Knit to other formats, e.g. Word under [Preview] button N.B. If Step 4 does not work, create a new R Notebook and copy and paste [R Markdown Source File] in Moodle 3.1.6 Knit, Notebook Mode and Preview of Default.Rmd Knit to HTML Knit to PDF (require  system, install tinytex package) Knit to Word Controlling a code chunk and its output Highlight and run Run all chunks above Expand, collapse and clear output Show output in other window Modify chunk options Output Options: Notebook, HTML, PDF, Word General, Figures and Advanced 3.1.7 yaml - YAML Ain’t a Markup Language - Example --- title: &quot;File Name --Subtitle--&quot; author: &quot;My Name&quot; date: &quot;2021/12/22&quot; output: html_notebook: number_sections: yes toc: yes toc_float: yes word_document: fig_caption: yes fig_height: 5 fig_width: 6 # reference_docx: word-styles-reference-01.docx --- 3.1.8 R Markdown: Quick References (See Moodle) R Studio Help (menu bar) &gt; Markdown Quick Reference R Studio Help (menu bar) &gt; Cheat Sheet R Markdown Cheat Sheet R Markdown Reference Guide R Markdown: The Definitive Guide by Yihui Xie, J. J. Allaire, Garrett Grolemund In Textbook: R for Data Science: Communicate Markdown: R Markdown is based on the Markdown language of Pandoc Pandoc’s Markdown: Detailed Information Markdown Tutorials: Interactive Practicum DARING FIREBALL: Markdown (detailed explanation and editor as Dingus) 3.1.9 Markdown Language – or use WYSIWYG editor Headers: #, ##, ###, #### Lists: 1. 2. , * Links: linked phrase Images: ![alt text](figures/filename.jpg) Block quotes” &gt; (block)   equations: e.g. $\\frac{a}{b}$ for \\(\\frac{a}{b}\\) Horizontal rules: Three or more asterisks or dashes (*** or - - - ) Tables Footnotes Bibliographies and Citations Slide breaks Italicized text by _italic_, Bold text by **bold** Superscripts, Subscripts, Strikethrough text 3.1.10 MS Word: Happy collaboration with Rmd to docx Use R Markdown to create a Word document Save as: ``word-styles-reference-01.docx’’ Edit the Word styles Edit the styles of the file ``word-styles-reference-01.docx’’ . Save this document as your style reference docx file Format an Rmd report using the styles reference docx file --- title: &quot;Test Report&quot; author: &quot;Your Name&quot; date: &quot;January 6, 2021&quot; output: word_document: reference_docx: word-styles-reference-01.docx --- 3.1.11 Why R Markdown? 3.1.11.1 R Markdown Cheat Sheet .Rmd files: An R Markdown (.Rmd) file is a record of your research. It contains the code that a scientist needs to reproduce your work along with the narration that a reader needs to understand your work. Reproducible Research: At the click of a button, or the type of a command, you can rerun the code in an R Markdown file to Rmd reproduce your work and export the results as a finished report. Dynamic Documents: You can choose to export the finished report as a html, nb.html, pdf, MS Word, ODT, RTF, or markdown document; or as a html or pdf based slide show. 3.1.12 Literate Programming by D. Knuth Literate programming is an approach to programming introduced by Donald Knuth in which a program is given as an explanation of the program logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which a compilable source code can be generated 3.1.12.1 D. Knuth Let us change our traditional attitude to the construction of programs: Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do. 3.1.13 Reproducible Research - Quote from a Coursera Course 3.1.13.1 Reproducible Research Reproducible research is the idea that data analyses, and more generally, scientific claims, are published with their data and software code so that others may verify the findings and build upon them. The need for reproducibility is increasing dramatically as data analyses become more complex, involving larger datasets and more sophisticated computations. Reproducibility allows for people to focus on the actual content of a data analysis, rather than on superficial details reported in a written summary. In addition, reproducibility makes an analysis more useful to others because the data and code that actually conducted the analysis are available. 3.1.14 R Markdown workflow, R for Data Science R Markdown is also important because it so tightly integrates prose and code. This makes it a great analysis notebook because it lets you develop code and record your thoughts. It: Records what you did and why you did it. Regardless of how great your memory is, if you don’t record what you do, there will come a time when you have forgotten important details. Write them down so you don’t forget! Supports rigorous thinking. You are more likely to come up with a strong analysis if you record your thoughts as you go, and continue to reflect on them. This also saves you time when you eventually write up your analysis to share with others. Helps others understand your work. It is rare to do data analysis by yourself, and you’ll often be working as part of a team. A lab notebook helps you share why you did it with your colleagues or lab mates. 3.1.14.1 Examples of yaml --- title: &quot;R Notebook&quot; output: html_notebook --- 3.1.14.1.1 Default + author + date The format of date can be changed. --- title: &quot;Title of the Notebook&quot; author: &quot;Your Name&quot; date: &quot;2021-12-22&quot; output: html_notebook --- 3.1.14.1.2 Examples 3.1.14.1.3 Notebook of Coronavirus --- title: &quot;A Study of Cases of Coronavirus Pandemic&quot; author: &quot;Hiroshi Suzuki&quot; date: &quot;2021/12/22&quot; output: html_notebook: number_sections: yes toc: yes toc_float: yes --- 3.1.14.1.4 Notebook of Coronavirus + pdf + word --- title: &quot;A Study of Cases of Coronavirus Pandemic&quot; author: &quot;Hiroshi Suzuki&quot; date: &quot;2021/12/22&quot; output: html_notebook: number_sections: yes toc: yes toc_float: yes pdf_document: toc: true number_sections: true word_document: default --- 3.1.14.1.5 Edit the word file file_show.docx to create my-styles.docx, e.g., a4-my-styles.docx changed the paper size to A4 from US letter. --- title: &quot;A Study of Cases of Coronavirus Pandemic&quot; author: &quot;Hiroshi Suzuki&quot; date: &quot;2021/12/22&quot; output: html_notebook: number_sections: yes toc: yes toc_float: yes pdf_document: toc: true number_sections: true word_document: reference_docx: a4-my-styles.docx --- 3.1.15 Learning Resources, EDA2-1 Textbook: R for Data Science, Part V Communicate R Markdown: The Definitive Guide by Yihui Xie, J. J. Allaire, Garrett Grolemund [Last Revised: 2020-12-14] BOOKDOWN: Write HTML, PDF, ePub, and Kindle books with R Markdown. Free online document is provided in pdf as well Arxive Page RMarkdown for Scientists by Nicholas Tierney [Last Revised: 2020-09-09] Report Writing for Data Science in R by Roger Peng Social Science Computing Cooperative at the University of Wisconsin R for Researchers: R Markdown [Last Revised: 2015-04-16] 3.1.16 Practicum: R Markdown and R Notebook Install rmarkdown, tinytex, tidyverse (and timetk) Try RMarkdown: HTML, PDF, Word Try R Notebook: Code Chunk RStudio Help Visual Editor Download from Moodle Export and import files file name.nb.html and file name.Rmd YAML Shared link to RStudio.cloud: https://moodle3.icu.ac.jp/mod/url/view.php?id=185785 NB. Sys.setenv(LANG = \"en\") 3.2 Part II: Data Visualization and Tidyverse Package 3.2.1 Introduction to tidyverse R Packages CRAN: https://cran.r-project.org &gt; Packages (menu) Contributed Packages Currently, the CRAN package repository features 16850 available packages. RStudio: R Packages Quick list of useful R packages Tidyverse: https://www.tidyverse.org Install tidyverse install.packages(“tidyverse”) RStudio Menu: Tools &gt; Install Packages &gt; tidyverse Attaching tidyverse library(tidyverse) The following packages are attached automatically: ggplot2, tibble, tidyr, readr, purrr, dplyr, stringr, forcats 3.2.2 ggplot2 Overview ggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details. Examples ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) ggplot(data = mpg) + geom_boxplot(mapping = aes(x = class, y = hwy)) Template ggplot(data = &lt;DATA&gt;) + &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;)) 3.2.3 Practicum: ggplot2 on R Notebook library(tidyverse) cars and iris df_cars &lt;- cars, df_iris &lt;- iris geom_point, geom_line, geom_histogram, goem_boxplot aes(), color = () facet_wrap(vars()) Examples in Moodle 3.2.4 Basics of Fundamentals of Statistics 3.2.4.1 R Commands Related to R Basics Fundamentals of Statistics: statistical measurements such as mean: mean() or mean(x, na.rm = TRUE) median: median() or median(x, na.rm = TRUE) quantile: quantile() or quantile(x, na.rm = TRUE) variance: var() or var(x, na.rm = TRUE) standard deviation: sd() covariance: cov() correlation: cor() summary() –&gt; 3.2.5 RStudio Primers created by learnr 3.2.5.1 RStudio Primers https://rstudio.cloud/learn/primers The Basics – r4ds: Explore, I Programming Basics: Try this first! Visualization Basics Work with Data – r4ds: Wrangle, I Working with Tibbles Isolating Data with dplyr Deriving Information with dplyr Visualize Data – r4ds: Explore, II Tidy Your Data – r4ds: Wrangle, II Iterate – r4ds: Program Write Functions – r4ds: Program Report Reproductively – r4ds: Communicate Build Interactive Web Apps – r4ds: Communicate 3.2.6 Learning Resources, EDA2-2 https://rstudio.com/resources/webinars/a-gentle-introduction-to-tidy-statistics-in-r/ Textbook: R for Data Science, Part I Explore RStuio Primers: See References in Moodle at the bottom Stackoverflow https://stackoverflow.com For non-English system users: Set Sys.setenv(LANGUAGE = \"en\"): It is helpful for searching information on the internet when you get an error. Books: R Cookbook, 2nd Edition, James (JD) Long and Paul Teetor: https://rc2e.com Fundamentals of Data Visualization, by Claus O. Wilke: https://clauswilke.com/dataviz/index.html 3.3 The Week Three Assignment (in Moodle) Pick two data from the built-in dataset. (library(help = \"datasets\") or go to the site The R Datasets Package) One of them can be iris but do not choose cars or AirPassengers. ggplot2 examples of cars, iris and AirPassengers are given below. Create an R Notebook of a Data Analysis containing the following and submit the rendered HTML file (file name.nb.html): title, date, and author, i.e., Your Name an explanation of the data and the variables at least one code chunk containing the following: head(), str() for each dataset, at least one code chunk containing graphs using ggplot2. Please try at least two geoms: geom_hist(), geom_boxplot(), goem_col(), etc. geom_line(), geom_point(), etc. your findings and/or questions file name: ID.nb.html, e.g. 123456.nb.html option: median(), mean(), sd() of a quantitative (numeric) variable, cor() of two quantitative (numeric) variables (or a correlation table) Submit your R Notebook file to Moodle (The Third Assignment) by 2021-01-11 23:59:00 3.3.1 Note on R Notebook Please note the following. There are essentially three modes: R Scripts, R Notebook and R Markdown. R Notebook is a special type of R Markdown but please use R Notebook at least for Suzuki’s assignments. To start, choose R Notebook from New File in the File Menu. If you started with R Markdown, please switch it with the Preview button hidden under the triangle on the right of the knit button. The file we preview has the name file name.nb.html. For example if the original file name is a3_12345.Rmd, then a3_12345.nb.html is created. You can check it using Files tab. When you preview R Notebook, on the top right, you can find Code button. If you press it you also can find download Rmd, which is the source of R Notebook you edited. In this way we can share both the outputs and the source. One difficulty is that you cannot include the outputs of the code chunk in the preview. Check Preview on Save and/or save the file before preview, i.e., pressing the preview button. Select Run all under Run button. Then all outputs apear on your editor and all outputs will appear in your preview. If some of your code chunks have problems, run each code chunk from top to bottom so that all outputs appear in your editor or viewer. When you share your R Notebook, do not forget to share file name.nb.html. If you have a file name.nb.html, then find it from Files in R Studio, you can automatically create file name.Rmd to edit the source. To create a fancy document with R Notebook, see the Markdown Quick Reference under Help on top menu for the editor. If you are using Visual Editor using A bottun on top left pane, see https://rstudio.github.io/visual-markdown-editing/. R Studio introduced Visual Editor last year. It seems to be stable but it is not perfect to go back and forth from the original editor using tags. I always use the original editor and I am confident on all the functions of it but I do not have much experience on Visual Editor. 3.3.2 Set up We will use ggplot2 package which is a part of tidyverse package. # install.packages(&quot;tidyverse&quot;) # only once library(tidyverse) ## ── Attaching packages ───────────────────────────────────────────────────────────────────────────────────── tidyverse 1.3.2 ── ## ✔ ggplot2 3.4.0 ✔ purrr 0.3.5 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.4.1 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ──────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 3.3.3 Data cars We will study the data cars in an R package datasets. Name: Speed and Stopping Distances of Cars Description: The data give the speed of cars and the distances taken to stop. Note that the data were recorded in the 1920s. Source: The data give the speed of cars and the distances taken to stop. Note that the data were recorded in the 1920s. References McNeil, D. R. (1977) Interactive Data Analysis. Wiley. data(cars) # to refresh data, it is better to start with this. df_cars &lt;- cars # You can use cars as is, but it is safer to assign it to other name head(df_cars) ## speed dist ## 1 4 2 ## 2 4 10 ## 3 7 4 ## 4 7 22 ## 5 8 16 ## 6 9 10 str(df_cars) ## &#39;data.frame&#39;: 50 obs. of 2 variables: ## $ speed: num 4 4 7 7 8 9 10 10 10 11 ... ## $ dist : num 2 10 4 22 16 10 18 26 34 17 ... 3.3.3.1 Observations on Data Structure There are two numeric variables of lenght 50: speed, dist 3.3.3.2 Data Visualization by ggplot2 3.3.3.2.1 Scatter Plot by geom_point() ggplot(df_cars) + geom_point(aes(x = speed, y = dist)) + labs(x = &quot;speed (mph)&quot;, y = &quot;dist (ft)&quot;, title = &quot;Speed and Stopping Distances of Cars&quot;) Observation 1 Roughly the dist is proportional to the speed. cor(df_cars) ## speed dist ## speed 1.0000000 0.8068949 ## dist 0.8068949 1.0000000 The correlation is 0.806. So we say strongly correlated. In the following, since the mapping aes() are the same for two geoms, we can place it in ggplot(). We will study linear regression in Week 6 and on. ggplot(df_cars, aes(x = speed, y = dist)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(x = &quot;speed (mph)&quot;, y = &quot;dist (ft)&quot;, title = &quot;Speed and Stopping Distances of Cars&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; summary(lm(df_cars$dist ~ df_cars$speed)) ## ## Call: ## lm(formula = df_cars$dist ~ df_cars$speed) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.069 -9.525 -2.272 9.215 43.201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.5791 6.7584 -2.601 0.0123 * ## df_cars$speed 3.9324 0.4155 9.464 1.49e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.38 on 48 degrees of freedom ## Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 ## F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 3.3.3.2.2 Histograms by geom_histogram() ggplot(df_cars) + geom_histogram(aes(x = speed)) + labs(title = &quot;Histogram of speed&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot(df_cars) + geom_histogram(aes(x = speed), binwidth = 1) + labs(title = &quot;Histogram of speed&quot;) ggplot(df_cars) + geom_histogram(aes(x = dist)) + labs(title = &quot;Histogram of speed&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot(df_cars) + geom_histogram(aes(x = dist), binwidth = 1) + labs(title = &quot;Histogram of speed&quot;) 3.3.4 Data iris We will study the data iris in an R package datasets. Name: Edgar Anderson’s Iris Data Description: This famous (Fisher’s or Anderson’s) iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica. Source: Fisher, R. A. (1936) The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7, Part II, 179–188. The data were collected by Anderson, Edgar (1935). The irises of the Gaspe Peninsula, Bulletin of the American Iris Society, 59, 2–5. data(iris) # to refresh data, it is better to start with this. df_iris &lt;- iris # You can use iris as is, but it is safer to assign it to other name head(df_iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa str(df_iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... unique(df_iris$Species) ## [1] setosa versicolor virginica ## Levels: setosa versicolor virginica 3.3.4.1 Observations on Data Structure There are five variables: Sepal.Length, Sepal.Width, Petal.Length, Petal.Width and Species First four variables are numeric, and the fifth is a character vector in factor Species have three levels corresponding to three kinds of iris: setosa versicolor virginica 3.3.4.2 Data Analysis of Each Variable, i.e., Univariate Analysis For geom_histogram the default of stat is “bin” for continuous data. Set stat = \"count\", and adjust the bins by the number of bins, i.e. bins option with 30 for default or biwidth option that overrides bins. ggplot(df_iris) + geom_histogram(aes(x = Sepal.Length)) + labs(title = &quot;Histogram of Sepal.Length&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot(df_iris) + geom_histogram(aes(x = Sepal.Length), bins = 10) + labs(title = &quot;Histogram of Sepal.Length&quot;) ggplot(df_iris) + geom_histogram(aes(x = Sepal.Length), binwidth = 0.1) + labs(title = &quot;Histogram of Sepal.Length&quot;) The previous graph is similar to the next. ggplot(df_iris) + geom_histogram(aes(x = Sepal.Length), stat = &quot;count&quot;) + labs(title = &quot;Histogram of Sepal.Length&quot;) ## Warning in geom_histogram(aes(x = Sepal.Length), stat = &quot;count&quot;): Ignoring ## unknown parameters: `binwidth`, `bins`, and `pad` ggplot(df_iris) + geom_histogram(aes(x = Sepal.Length, fill = Species), stat = &quot;count&quot;) + labs(title = &quot;Histogram of Sepal.Length&quot;) ## Warning in geom_histogram(aes(x = Sepal.Length, fill = Species), stat = ## &quot;count&quot;): Ignoring unknown parameters: `binwidth`, `bins`, and `pad` ggplot(df_iris) + geom_histogram(aes(x = Sepal.Length, fill = Species), binwidth = 0.1) + labs(title = &quot;Histogram of Sepal.Length&quot;) ggplot(df_iris) + geom_boxplot(aes(x = Species, y = Sepal.Length)) + labs(title = &quot;Boxplot of Sepal.Lenght&quot;) ggplot(df_iris) + geom_boxplot(aes(x = Species, y = Sepal.Width)) + labs(title = &quot;Boxplot of Sepal.Width&quot;) ggplot(df_iris) + geom_boxplot(aes(x = Species, y = Petal.Length)) + labs(title = &quot;Boxplot of Petal.Length&quot;) ggplot(df_iris) + geom_boxplot(aes(x = Species, y = Petal.Width)) + labs(title = &quot;Boxplot of Petal.Width&quot;) 3.3.4.3 Data Analysis of Two Variables, i.e., Multivariate Analysis The following is a simple scatter plot. However, from the univariate analysis, it is clear that Species are key factors. ggplot(df_iris) + geom_point(aes(x = Sepal.Length, y = Sepal.Width)) ggplot(df_iris) + geom_point(aes(x = Sepal.Length, y = Sepal.Width, color = Species)) ggplot(df_iris) + geom_point(aes(x = Sepal.Length, y = Sepal.Width)) + facet_wrap(vars(Species)) Let us check the correlation matrix cor(df_iris[,1:4]) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length 1.0000000 -0.1175698 0.8717538 0.8179411 ## Sepal.Width -0.1175698 1.0000000 -0.4284401 -0.3661259 ## Petal.Length 0.8717538 -0.4284401 1.0000000 0.9628654 ## Petal.Width 0.8179411 -0.3661259 0.9628654 1.0000000 Petal.Length and Petal.Width have very strong positive correlation, and so are Sepal.Length and Petal.Length. Sepal.Width and Petal.Length have weak negative correlation, and so are Sepal.Width and Petal.Width. ggplot(df_iris) + geom_point(aes(x = Petal.Length, y = Petal.Width, color = Species)) ggplot(df_iris) + geom_point(aes(x = Petal.Length, y = Petal.Width)) + facet_wrap(vars(Species)) ggplot(df_iris) + geom_point(aes(x = Sepal.Length, y = Petal.Length, color = Species)) ggplot(df_iris) + geom_point(aes(x = Sepal.Length, y = Petal.Length)) + facet_wrap(vars(Species)) ggplot(df_iris) + geom_point(aes(x = Sepal.Width, y = Petal.Length, color = Species)) ggplot(df_iris) + geom_point(aes(x = Sepal.Width, y = Petal.Length)) + facet_wrap(vars(Species)) ggplot(df_iris) + geom_point(aes(x = Sepal.Width, y = Petal.Width, color = Species)) ggplot(df_iris) + geom_point(aes(x = Sepal.Width, y = Petal.Width)) + facet_wrap(vars(Species)) As we have seen above the situation is more complicated. Observation: Altogether Sepal.Width and Petal.Width have a negative correlation. However, if we look at the graph for each, they seem to have a positive correlation. In statistics we say that the Species is the confounder. 3.3.5 Data AirPassengers Next, we look at the data AirPassengers in an R package datasets. Name: Monthly Airline Passenger Numbers 1949-1960 Description: The classic Box &amp; Jenkins airline data. Monthly totals of international airline passengers, 1949 to 1960. Source: Box, G. E. P., Jenkins, G. M. and Reinsel, G. C. (1976) Time Series Analysis, Forecasting and Control. Third Edition. Holden-Day. Series G. data(AirPassengers) # to refresh data, it is better to start with this. df_AirPassengers &lt;- AirPassengers # You can use iris as is, but it is safer to assign it to other name head(df_AirPassengers) ## [1] 112 118 132 129 121 135 str(df_AirPassengers) ## Time-Series [1:144] from 1949 to 1961: 112 118 132 129 121 135 148 148 136 119 ... This is a time series data, a special data format of Base R. plot(df_AirPassengers) We can handle this type of data using tidyverse but now it is easier to use the following package. timetk Package https://CRAN.R-project.org/package=timetk # install.packages(&quot;timetk&quot;) # run this line or install `timetk` from Install Packages in Tool library(timetk) df_ap &lt;- tk_tbl(df_AirPassengers) df_ap ## # A tibble: 144 × 2 ## index value ## &lt;yearmon&gt; &lt;dbl&gt; ## 1 1 1949 112 ## 2 2 1949 118 ## 3 3 1949 132 ## 4 4 1949 129 ## 5 5 1949 121 ## 6 6 1949 135 ## 7 7 1949 148 ## 8 8 1949 148 ## 9 9 1949 136 ## 10 10 1949 119 ## # … with 134 more rows ggplot(df_ap) + geom_line(aes(x = index, y = value)) + labs(title = &quot;Line Graph of AirPassengers&quot;) Observation: There is a seasonal pattern. ggplot(df_ap) + geom_histogram(aes(x = value)) + labs(title = &quot;Histogram of AirPassengers&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 3.3.6 Recommended Study Plan Review the class note, slides and/or videos RStudio Primers: Programming Basics, and Visualization Basics Review this note Look at The R Datasets Package) or in Console, library(help = \"datasets\") and use Help to check each data in the built-in datasets. Try this assignment Study the chapters 1-8 under Explore in the textbook. 3.4 Responses to the Week Three Assignment 3.4.1 Setup We load two packages; datasets and ggplot2. The datasets are loaded automatically and you do not need the first line of the followiong code chunk. But it is safer to include it because some data names are used previously for different purposes. library(datasets) library(ggplot2) For explanation, we use the following population data of WDI. library(WDI) pop &lt;- WDI( country = c(&quot;NG&quot;, &quot;BD&quot;, &quot;RU&quot;, &quot;MX&quot;, &quot;JP&quot;), indicator = c(population = &quot;SP.POP.TOTL&quot;), start = 1960, end = 2020) head(pop) ## country iso2c iso3c year population ## 1 Bangladesh BD BGD 2020 164689383 ## 2 Bangladesh BD BGD 2019 163046173 ## 3 Bangladesh BD BGD 2018 161376713 ## 4 Bangladesh BD BGD 2017 159685421 ## 5 Bangladesh BD BGD 2016 157977151 ## 6 Bangladesh BD BGD 2015 156256287 3.4.2 Visualization of Two Variables iris_df &lt;- iris str(iris_df) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... head(iris_df) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa Let us look at two plots, one with geom_point() and the other with geom_line() ggplot(iris_df) + geom_point(mapping = aes(x=Petal.Length, y=Petal.Width)) ggplot(iris_df) + geom_line(mapping = aes(x=Petal.Length, y=Petal.Width)) The line graph is not appropriate. Can you see why? By help(geom_line) or Help tab with geom_line in the search window, we get the following: geom_line() connects them in order of the variable on the x axis. See https://ggplot2.tidyverse.org/reference/geom_path.html In this case lines are meaningless especially the vertical lines. ggplot(iris_df) + geom_point(mapping = aes(x=Petal.Length, y=Petal.Width, color = Species)) ggplot(iris_df) + geom_line(mapping = aes(x=Petal.Length, y=Petal.Width, color = Species)) It is good to add colors for Species in geom_point, however, geom_line is not appropriate in this case. Let us look at the population data of WDI. ggplot(pop) + geom_point(mapping = aes(x=year, y=population)) This is OK. ggplot(pop) + geom_line(mapping = aes(x=year, y=population)) We have a similar problem with geom_line(). However, ggplot(pop) + geom_line(mapping = aes(x=year, y=population, color = country)) This looks just fine. In this case geom_line() with color aestic is better than geom_point() above. If you have two numerical data, it is safer to use geom_point() first and decide to choose a better option. It is not easy to decide whether the lines between the points are meaningful or not. Please think carefully what you want to communicate. In some cases, you may want to choose the following. Note that I moved the aes() to ggplot() as it is common to both geom_line() and geom_point(). ggplot(pop, mapping = aes(x=year, y=population, color = country)) + geom_line() + geom_point() See other examples df_tg &lt;- ToothGrowth head(df_tg) ## len supp dose ## 1 4.2 VC 0.5 ## 2 11.5 VC 0.5 ## 3 7.3 VC 0.5 ## 4 5.8 VC 0.5 ## 5 6.4 VC 0.5 ## 6 10.0 VC 0.5 str(df_tg) ## &#39;data.frame&#39;: 60 obs. of 3 variables: ## $ len : num 4.2 11.5 7.3 5.8 6.4 10 11.2 11.2 5.2 7 ... ## $ supp: Factor w/ 2 levels &quot;OJ&quot;,&quot;VC&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ dose: num 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 ... ggplot(df_tg) + geom_point(aes(x = dose, y = len, color = supp)) ggplot(df_tg) + geom_boxplot(aes(x = factor(dose), y = len)) + facet_wrap(vars(supp)) 3.4.3 Visualization of One Variable df_chickwts &lt;- chickwts head(df_chickwts) ## weight feed ## 1 179 horsebean ## 2 160 horsebean ## 3 136 horsebean ## 4 227 horsebean ## 5 217 horsebean ## 6 168 horsebean ggplot(df_chickwts) + geom_point(aes(x = feed, y = weight)) ggplot(df_chickwts) + geom_boxplot(aes(x = feed, y = weight)) Since there are two variables, you can use geom_point(). However, one of them is a categorical variable and box_plot() works better, I believe. It is called a box and whiskers plot (in the style of Turkey). Please look at Help. The boxplot compactly displays the distribution of a continuous variable. It visualises five summary statistics (the median, two hinges and two whiskers), and all “outlying” points individually. See also https://ggplot2.tidyverse.org/reference/geom_boxplot.html. The other popular visualization is geom_histogram(). Please try fill and color options to see the difference. ggplot(df_chickwts) + geom_histogram(aes(x = weight, fill = feed)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot(df_chickwts) + geom_freqpoly(aes(x = weight, color = feed)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot(df_chickwts) + geom_histogram(aes(x = weight), bins = 10) + facet_wrap(vars(feed)) Which one do you like best? There are many options. Again, it depends on what you want to communicate. # `rivers` is a numeric vector, so changed into a dataframe. df_riv &lt;- data.frame(length = rivers) # it is safer to assign it to other name head(df_riv) ## length ## 1 735 ## 2 320 ## 3 325 ## 4 392 ## 5 524 ## 6 450 Since there is no meaning in ordering, geom_histogram() may be an appropriate choice. ggplot(df_riv, aes(length)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 3.4.4 More Examples 3.4.4.1 Weight versus age of chicks on different diets Description: The ChickWeight data frame has 578 rows and 4 columns from an experiment on the effect of diet on early growth of chicks. Format: An object of class c(“nfnGroupedData”, “nfGroupedData”, “groupedData”, “data.frame”) containing the following columns: weight: a numeric vector giving the body weight of the chick (gm). Time: a numeric vector giving the number of days since birth when the measurement was made. Chick: an ordered factor with levels 18 &lt; … &lt; 48 giving a unique identifier for the chick. The ordering of the levels groups chicks on the same diet together and orders them according to their final weight (lightest to heaviest) within diet. Diet: a factor with levels 1, …, 4 indicating which experimental diet the chick received. Details: The body weights of the chicks were measured at birth and every second day thereafter until day 20. They were also measured on day 21. There were four groups on chicks on different protein diets. df_chickweight &lt;- ChickWeight head(df_chickweight) ## Grouped Data: weight ~ Time | Chick ## weight Time Chick Diet ## 1 42 0 1 1 ## 2 51 2 1 1 ## 3 59 4 1 1 ## 4 64 6 1 1 ## 5 76 8 1 1 ## 6 93 10 1 1 str(df_chickweight) ## Classes &#39;nfnGroupedData&#39;, &#39;nfGroupedData&#39;, &#39;groupedData&#39; and &#39;data.frame&#39;: 578 obs. of 4 variables: ## $ weight: num 42 51 59 64 76 93 106 125 149 171 ... ## $ Time : num 0 2 4 6 8 10 12 14 16 18 ... ## $ Chick : Ord.factor w/ 50 levels &quot;18&quot;&lt;&quot;16&quot;&lt;&quot;15&quot;&lt;..: 15 15 15 15 15 15 15 15 15 15 ... ## $ Diet : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## - attr(*, &quot;formula&quot;)=Class &#39;formula&#39; language weight ~ Time | Chick ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; ## - attr(*, &quot;outer&quot;)=Class &#39;formula&#39; language ~Diet ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; ## - attr(*, &quot;labels&quot;)=List of 2 ## ..$ x: chr &quot;Time&quot; ## ..$ y: chr &quot;Body weight&quot; ## - attr(*, &quot;units&quot;)=List of 2 ## ..$ x: chr &quot;(days)&quot; ## ..$ y: chr &quot;(gm)&quot; summary(df_chickweight) ## weight Time Chick Diet ## Min. : 35.0 Min. : 0.00 13 : 12 1:220 ## 1st Qu.: 63.0 1st Qu.: 4.00 9 : 12 2:120 ## Median :103.0 Median :10.00 20 : 12 3:120 ## Mean :121.8 Mean :10.72 10 : 12 4:118 ## 3rd Qu.:163.8 3rd Qu.:16.00 17 : 12 ## Max. :373.0 Max. :21.00 19 : 12 ## (Other):506 ggplot(df_chickweight) + geom_point(aes(x = Time, y = weight, color = Diet)) If we use dplyr(), we can summarize the mean, i.e., average weight in each category. library(dplyr) df_chickweight %&gt;% group_by(Diet, Time) %&gt;% summarize(wt = mean(weight)) %&gt;% ggplot() + geom_line(aes(x = Time, y = wt, color = Diet)) ## `summarise()` has grouped output by &#39;Diet&#39;. You can override using the `.groups` ## argument. If we use geom_smooth(), we can plot the smoothed conditional means. See https://ggplot2.tidyverse.org/reference/geom_smooth.html. ggplot(df_chickweight) + geom_smooth(aes(x = Time, y = weight, color = Diet)) ## `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; 3.4.4.2 Carbon Dioxide Uptake in Grass Plants Description: The CO2 data frame has 84 rows and 5 columns of data from an experiment on the cold tolerance of the grass species Echinochloa crus-galli. Usage: CO2 Format: An object of class c(“nfnGroupedData”, “nfGroupedData”, “groupedData”, “data.frame”) containing the following columns: Plant an ordered factor with levels Qn1 &lt; Qn2 &lt; Qn3 &lt; … &lt; Mc1 giving a unique identifier for each plant. Type a factor with levels Quebec Mississippi giving the origin of the plant Treatment a factor with levels nonchilled chilled conc a numeric vector of ambient carbon dioxide concentrations (mL/L). uptake a numeric vector of carbon dioxide uptake rates (umol/m^2 sec). Details The CO2 uptake of six plants from Quebec and six plants from Mississippi was measured at several levels of ambient CO2 concentration. Half the plants of each type were chilled overnight before the experiment was conducted. df_CO2 &lt;- CO2 head(df_CO2) ## Grouped Data: uptake ~ conc | Plant ## Plant Type Treatment conc uptake ## 1 Qn1 Quebec nonchilled 95 16.0 ## 2 Qn1 Quebec nonchilled 175 30.4 ## 3 Qn1 Quebec nonchilled 250 34.8 ## 4 Qn1 Quebec nonchilled 350 37.2 ## 5 Qn1 Quebec nonchilled 500 35.3 ## 6 Qn1 Quebec nonchilled 675 39.2 str(df_CO2) ## Classes &#39;nfnGroupedData&#39;, &#39;nfGroupedData&#39;, &#39;groupedData&#39; and &#39;data.frame&#39;: 84 obs. of 5 variables: ## $ Plant : Ord.factor w/ 12 levels &quot;Qn1&quot;&lt;&quot;Qn2&quot;&lt;&quot;Qn3&quot;&lt;..: 1 1 1 1 1 1 1 2 2 2 ... ## $ Type : Factor w/ 2 levels &quot;Quebec&quot;,&quot;Mississippi&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Treatment: Factor w/ 2 levels &quot;nonchilled&quot;,&quot;chilled&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ conc : num 95 175 250 350 500 675 1000 95 175 250 ... ## $ uptake : num 16 30.4 34.8 37.2 35.3 39.2 39.7 13.6 27.3 37.1 ... ## - attr(*, &quot;formula&quot;)=Class &#39;formula&#39; language uptake ~ conc | Plant ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; ## - attr(*, &quot;outer&quot;)=Class &#39;formula&#39; language ~Treatment * Type ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; ## - attr(*, &quot;labels&quot;)=List of 2 ## ..$ x: chr &quot;Ambient carbon dioxide concentration&quot; ## ..$ y: chr &quot;CO2 uptake rate&quot; ## - attr(*, &quot;units&quot;)=List of 2 ## ..$ x: chr &quot;(uL/L)&quot; ## ..$ y: chr &quot;(umol/m^2 s)&quot; summary(df_CO2) ## Plant Type Treatment conc uptake ## Qn1 : 7 Quebec :42 nonchilled:42 Min. : 95 Min. : 7.70 ## Qn2 : 7 Mississippi:42 chilled :42 1st Qu.: 175 1st Qu.:17.90 ## Qn3 : 7 Median : 350 Median :28.30 ## Qc1 : 7 Mean : 435 Mean :27.21 ## Qc3 : 7 3rd Qu.: 675 3rd Qu.:37.12 ## Qc2 : 7 Max. :1000 Max. :45.50 ## (Other):42 ggplot(df_CO2) + geom_point(aes(x = conc, y = uptake, color = Type)) + facet_wrap(vars(Treatment)) ggplot(df_CO2) + geom_smooth(aes(x = conc, y = uptake, color = Type)) + facet_wrap(vars(Treatment)) ## `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; 3.4.4.3 Growth of Orange Trees Description: The Orange data frame has 35 rows and 3 columns of records of the growth of orange trees. Usage: Orange Format: An object of class c(“nfnGroupedData”, “nfGroupedData”, “groupedData”, “data.frame”) containing the following columns: Tree an ordered factor indicating the tree on which the measurement is made. The ordering is according to increasing maximum diameter. age a numeric vector giving the age of the tree (days since 1968/12/31) circumference a numeric vector of trunk circumferences (mm). This is probably “circumference at breast height”, a standard measurement in forestry. Details: This dataset was originally part of package nlme, and that has methods (including for [, as.data.frame, plot and print) for its grouped-data classes. df_orange &lt;- Orange head(df_orange) ## Grouped Data: circumference ~ age | Tree ## Tree age circumference ## 1 1 118 30 ## 2 1 484 58 ## 3 1 664 87 ## 4 1 1004 115 ## 5 1 1231 120 ## 6 1 1372 142 str(df_orange) ## Classes &#39;nfnGroupedData&#39;, &#39;nfGroupedData&#39;, &#39;groupedData&#39; and &#39;data.frame&#39;: 35 obs. of 3 variables: ## $ Tree : Ord.factor w/ 5 levels &quot;3&quot;&lt;&quot;1&quot;&lt;&quot;5&quot;&lt;&quot;2&quot;&lt;..: 2 2 2 2 2 2 2 4 4 4 ... ## $ age : num 118 484 664 1004 1231 ... ## $ circumference: num 30 58 87 115 120 142 145 33 69 111 ... ## - attr(*, &quot;formula&quot;)=Class &#39;formula&#39; language circumference ~ age | Tree ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; ## - attr(*, &quot;labels&quot;)=List of 2 ## ..$ x: chr &quot;Time since December 31, 1968&quot; ## ..$ y: chr &quot;Trunk circumference&quot; ## - attr(*, &quot;units&quot;)=List of 2 ## ..$ x: chr &quot;(days)&quot; ## ..$ y: chr &quot;(mm)&quot; summary(df_orange) ## Tree age circumference ## 3:7 Min. : 118.0 Min. : 30.0 ## 1:7 1st Qu.: 484.0 1st Qu.: 65.5 ## 5:7 Median :1004.0 Median :115.0 ## 2:7 Mean : 922.1 Mean :115.9 ## 4:7 3rd Qu.:1372.0 3rd Qu.:161.5 ## Max. :1582.0 Max. :214.0 ggplot(df_orange) + geom_point(aes(x = age, y = circumference, color = Tree)) ggplot(df_orange) + geom_boxplot(aes(x = factor(age), y = circumference)) ggplot(df_orange) + geom_smooth(aes(x = age, y = circumference)) ## `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; "],["eda3.html", "Chapter 4 Exploratory Data Analysis (EDA) 3 4.1 Part I: Importing Data by readr and WDI Package 4.2 Part II: Data Transforamtion with dplyr 4.3 The Fourth Assignment (in Moodle)", " Chapter 4 Exploratory Data Analysis (EDA) 3 Course Contents 2021-12-08: Introduction: About the course - An introduction to open and public data, and data science 2021-12-15: Exploratory Data Analysis (EDA) 1 [lead by hs] - R Basics with RStudio and/or RStudio.cloud; R Script, swirl 2021-12-22: Exploratory Data Analysis (EDA) 2 [lead by hs] - R Markdown; Introduction to tidyverse; RStudio Primers 2022-01-12: Exploratory Data Analysis (EDA) 3 [lead by hs] - Introduction to tidyverse; Public Data, WDI, etc 2022-01-19: Exploratory Data Analysis (EDA) 4 [lead by hs] - Introduction to tidyverse; WDI, UN, WHO, etc 2022-01-26: Exploratory Data Analysis (EDA) 5 [lead by hs] - Introduction to tidyverse; WDI, OECD, US gov, etc 2022-02-02: Inference Statistics 1 2022-02-09: Inference Statistics 2 2022-02-16: Inference Statistics 3 2022-02-23: Project Presentation Importing and Transforming Data Importing Data by readr in tidyverse and WDI Transforming Data by dplyr in tidyverse EDA (A diagram from R4DS by H.W. and G.G.) EDA from r4ds Contents of EDA3 Importing Data by readr Basics Public Data revisited Importing Data with API (Application Programming Interface) WDI of World Bank Exploratory Data Analysis (EDA) Variables Data Transformation (Wrangling, Reshaping) with tidyverse Packages dplyr: select, filter, mutate, arrange, group_by, summarize, etc. 4.1 Part I: Importing Data by readr and WDI Package 4.1.1 tidyverse Package Review CRAN: https://cran.r-project.org &gt; Packages (menu) Contributed Packages Currently, the CRAN package repository features 16850 available packages. RStudio: R Packages Quick list of useful R packages Tidyverse: https://www.tidyverse.org Install tidyverse install.packages(“tidyverse”) RStudio Menu: Tools &gt; Install Packages &gt; tidyverse Attaching tidyverse library(tidyverse) The following packages are attached automatically: ggplot2, tibble, tidyr, readr, purrr, dplyr, stringr, forcats 4.1.2 R Package: An Example - readr in tidyverse 4.1.2.1 CRAN - Package readr (r-project.org › package=readr) readr: Read Rectangular Text Data short description The goal of ‘readr’ is to provide a fast and friendly way to read rectangular data (like ‘csv’, ‘tsv’, and ‘fwf’). It is designed to flexibly parse many types of data found in the wild, while still cleanly failing when data unexpectedly changes. description Version: 2.1.1 Depends: R (\\(\\geq\\) 3.1) Imports: cli, clipr, crayon, hms (\\(\\geq\\) 0.4.1), methods, rlang, R6, tibble, utils, lifecycle URL: https://readr.tidyverse.org, https://github.com/tidyverse/readr Reference manual: readr.pdf Vignettes: usage with explanations Locales Introduction to readr 4.1.3 Reading Data Files - readr, (readxl, etc.) 4.1.3.1 readr: Overview The goal of readr is to provide a fast and friendly way to read rectangular data (like csv, tsv, and fwf). It is designed to flexibly parse many types of data. If you are new to readr, the best place to start is the data import chapter in R for data science. read_csv: comma separated (CSV) files e.g. heights &lt;- read_csv(“data/heights.csv”) read_delim: general delimited files Use [Help] to find out the detail, or go to the sites above. 4.1.3.2 readxl: Overview The readxl package makes it easy to get data out of Excel and into R. read_excel reads both xls and xlsx files and detects the format from the extension. eg. df1 &lt;- read_excel(“file_name.xlsx”, sheet = 1) 4.1.4 Basics of Importing Data, I Get the URL (uniform resource locator) - copy the link data_url &lt;- \"https://github.com/RamiKrispin/ coronavirus/raw/master/csv/coronavirus.csv\" Download the file into the destfile in data folder: download.file(url = data_url, destfile = \"data/corinavirus.csv\") Read the file: df_coronavirus &lt;- read_csv(\"data/corinavirus.csv\") Option 1 after 2: RStudio: Files &gt; Import Dataset Option 2:Skip 2 and df_coronavirus &lt;- read_csv(data_url) Write data: write_csv(df_coronavirus, \"data/coronavirus20220112.csv\") Note that the data is very big now. 4.1.5 Basics of Importing Data, II Get the URL (uniform resource locator) - copy the link e.g. Go to UN Data: https://data.un.org/ Copy the link: e.g. Population, surface area and density url_of_data &lt;- \"https://data.un.org/--long url--.csv\" Download the file into the destfile in data folder: download.file(url = url_of_data, destfile = \"data/un_pop.csv\") Or, directly download the file, and rename it. Read the file: df_un_pop &lt;- read_csv(\"data/un_pop.csv\", skip = 1) 4.1.6 tibble Overview A tibble, or tbl_df, is a modern reimagining of the data.frame, keeping what time has proven to be effective, and throwing out what is not. Tibbles are data.frames that are lazy and surly: they do less (i.e. they don’t change variable names or types, and don’t do partial matching) and complain more (e.g. when a variable does not exist). This forces you to confront problems earlier, typically leading to cleaner, more expressive code. Tibbles also have an enhanced print() method which makes them easier to use with large datasets containing complex objects. If you are new to tibbles, the best place to start is the tibbles chapter in R for data science. Examples iris_tbl &lt;- as_tibble(iris) iris_tbl glimpse(iris_tbl) 4.1.7 Variables 4.1.7.1 The First Step: Look at Various Visualiation What are varibles? How many variables? Quantitative variable? Qualitative variable? Numerical variable? Categorical variable? In R, there are six types: Double Integer Character Logical Raw Complex Study a, b, c, d carefully. 0, 1, 2, … can be double, integer, character, and logical symbols T and F can be computed as 1 and 0 4.1.8 Example I A Study of Cases of Coronavirus Pandemic, II Importing and Transforming Data with readr and dplyr in tidyverse Packages 4.1 Review 4.2 library: Loading/Attaching Packages 4.3 Importing data by readr in tidyverse 4.1.9 Open and Public Data, World Bank 4.1.9.1 Open Government Data Toolkit: Open Data Defined The term ``Open Data’’ has a very precise meaning. Data or content is open if anyone is free to use, re-use or redistribute it, subject at most to measures that preserve provenance and openness. The data must be , which means they must be placed in the public domain or under liberal terms of use with minimal restrictions. The data must be , which means they must be published in electronic formats that are machine readable and non-proprietary, so that anyone can access and use the data using common, freely available software tools. Data must also be publicly available and accessible on a public server, without password or firewall restrictions. To make Open Data easier to find, most organizations create and manage Open Data catalogs. 4.1.10 World Bank: WDI - World Development Indicaters World Bank: https://www.worldbank.org Who we are: To end extreme poverty: By reducing the share of the global population that lives in extreme poverty to 3 percent by 2030. To promote shared prosperity: By increasing the incomes of the poorest 40 percent of people in every country. World Bank Open Data: https://data.worldbank.org World Development Indicators (WDI) : the World Bank’s premier compilation of cross-country comparable data on development. Poverty and Inequality People Environment Economy States and Markets Global Links 4.1.11 R Package WDI WDI: World Development Indicators and Other World Bank Data Search and download data from over 40 databases hosted by the World Bank, including the World Development Indicators (‘WDI’), International Debt Statistics, Doing Business, Human Capital Index, and Sub-national Poverty indicators. Version: 2.7.4 Materials: README - usage NEWS - version history Published: 2021-04-06 README: https://cran.r-project.org/web/packages/WDI/readme/README.html Reference manual: WDI.pdf 4.1.12 Function WDI Usage WDI(country = &quot;all&quot;, indicator = &quot;NY.GDP.PCAP.KD&quot;, start = 1960, end = 2020, extra = FALSE, cache = NULL) Arguments country: Vector of countries (ISO-2 character codes, e.g. “BR”, “US”, “CA”, or “all”) indicator: If you supply a named vector, the indicators will be automatically renamed: c('women_private_sector' = 'BI.PWK.PRVS.FE.ZS') 4.1.13 Function WDIsearch library(WDI) WDIsearch(string = &quot;NY.GDP.PCAP.KD&quot;, field = &quot;indicator&quot;, cache = NULL) ## indicator name ## 11431 NY.GDP.PCAP.KD GDP per capita (constant 2015 US$) ## 11432 NY.GDP.PCAP.KD.ZG GDP per capita growth (annual %) WDIsearch(string = &quot;NY.GDP.PCAP.KD&quot;, field = &quot;indicator&quot;, short = FALSE, cache = NULL) WDIsearch(string = &quot;gdp&quot;, field = &quot;name&quot;, short = TRUE, cache = NULL) 4.1.14 Example II Introduction to WDI World Bank 1.1 About 1.2 Open Data Defined 1.3 WDI - World Development Indicaters R Package WDI 2.1 Function WDI: World Development Indicators (World Bank) 2.2 Function WDIsearch The First Example 3.1 Setup 3.2 GDP Per Capita 4.2 Part II: Data Transforamtion with dplyr 4.2.1 dplyr Overview dplyr is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges: select() picks variables based on their names. filter() picks cases based on their values. mutate() adds new variables that are functions of existing variables summarise() reduces multiple values down to a single summary. arrange() changes the ordering of the rows. group_by() takes an existing tbl and converts it into a grouped tbl. You can learn more about them in vignette(“dplyr”). As well as these single-table verbs, dplyr also provides a variety of two-table verbs, which you can learn about in vignette(“two-table”). If you are new to dplyr, the best place to start is the data transformation chapter in R for data science. 4.2.2 select: Subset columns using their names and types Helper Function Use Example - Columns except select(babynames, -prop) : Columns between (inclusive) select(babynames, year:n) contains() Columns that contains a string select(babynames, contains(“n”)) ends_with() Columns that ends with a string select(babynames, ends_with(“n”)) matches() Columns that matches a regex select(babynames, matches(“n”)) num_range() Columns with a numerical suffix in the range Not applicable with babynames one_of() Columns whose name appear in the given set select(babynames, one_of(c(“sex”, “gender”))) starts_with() Columns that starts with a string select(babynames, starts_with(“n”)) 4.2.3 filter: Subset rows using column values Logical operator tests Example &gt; Is x greater than y? x &gt; y &gt;= Is x greater than or equal to y? x &gt;= y &lt; Is x less than y? x &lt; y &lt;= Is x less than or equal to y? x &lt;= y == Is x equal to y? x == y != Is x not equal to y? x != y is.na() Is x an NA? is.na(x) !is.na() Is x not an NA? !is.na(x) 4.2.4 arrange and Pipe %&gt;% arrange() orders the rows of a data frame by the values of selected columns. Unlike other dplyr verbs, arrange() largely ignores grouping; you need to explicitly mention grouping variables (`or use .by_group = TRUE) in order to group by them, and functions of variables are evaluated once per data frame, not once per group. pipes in R for Data Science. Examples arrange(&lt;data&gt;, &lt;varible&gt;) arrange(&lt;data&gt;, desc(&lt;variable&gt;)) &lt;data&gt; %&gt;% ggplot() + geom_point(aes(x = &lt;&gt;, y = &lt;&gt;)) 4.2.5 Example III and Practicum A Study of Cases of Coronavirus Pandemic, II Importing and Transforming Data with readr and dplyr in tidyverse Packages 4.4 Transforming data by dplyr in tidyverse 4.4.1 slice(): Subset rows using their positions 4.4.2 select() Subset columns using their names and types 4.4.3 filter() Subset rows using column values 4.4.4 mutate(): Create, modify, and delete columns 4.4.5 ggplot(): Plotting 4.4.6 Summary 4.4.7 Pipes Introduction to WDI More Examples 4.2.6 Learning Resources, III Textbook: R for Data Science, Part II Explore 4.2.6.1 RStudio Primers: See References in Moodle at the bottom The Basics – r4ds: Explore, I Visualization Basics Programming Basics Work with Data – r4ds: Wrangle, I Working with Tibbles Isolating Data with dplyr Deriving Information with dplyr Visualize Data – r4ds: Explore, II Tidy Your Data – r4ds: Wrangle, II Iterate – r4ds: Program Write Functions – r4ds: Program 4.3 The Fourth Assignment (in Moodle) Choose one or more World Development Indicator(s), WDI Use the WDI function and analyse the (combined) data. Create an R Notebook (file name.nb.html) of an EDA containing: title, date, and author, i.e., Your Name your motivation and/or objectives to analyse the data, and your questions an explanation of the data and the variables (WDIsearch?) chunks containing the following: library(tidyverse), library(WDI) ‘WDI(country = , indicator = , …)’ visualize the data with ggplot() your findings and/or questions file name: a4_ID.nb.html, e.g. a4_123456.nb.html Submit your R Notebook file to Moodle (The Fourth Assignment) by 2022-01-25 23:59:00 4.3.1 Setup and YAML As YAML, R Notebook setting at the top, I used the following. --- title: &quot;Responses to the Fourth Assignment&quot; author: &quot;p000117x Hiroshi Suzuki&quot; date: &#39;2021-01-29&#39; output: html_notebook: number_sections: yes toc: yes toc_float: yes --- Section numbers will be automatically generated and the table of contents appear on left top. If you do not want to include the section number or skip a section, add {-} after the title name. If you do not need to include section numbers at all, use the following. --- title: &quot;Responses to the Fourth Assignment&quot; author: &quot;p000117x Hiroshi Suzuki&quot; date: &#39;2021-01-29&#39; output: html_notebook --- or --- title: &quot;Responses to the Fourth Assignment&quot; author: &quot;p000117x Hiroshi Suzuki&quot; date: &#39;2021-01-29&#39; output: html_notebook --- See examples of YAML in the Week 3 section of the Moodle page. For JIS keyboard users, the back-tick is typed by SHIFT+@. We load two packages; tidyverse and WDI. You need to load these once in the R Noebook file, and you do not need to add library(ggplot2) or library(dplyr) as these are automatically loaded when tidyverse package is loaded. library(tidyverse) library(WDI) library(DT) Run all or run code chunks you want to include in the output file filename.nb.html before you preview. It is safer to check Preview on Save on the top of the editor. The file name of this ends with .Rmd, the notebook file with .nb.html at the end is created by pressing Preview button. Please check it under the Files tab in the right below pane. This is the HTML file I want you to submit. Don’t knit HTML hiding behind Preview. If you knit HTML, the YAML will be changed. To see the examples, open R notebook files I created and from top right button, download Rmd file and open it in R Studio. In R notebook, you can and should include comments of each step for communication. It will benefit you as well as a record. 4.3.2 Examples and Comments In the following I include my feedback keeping anonymity. 4.3.2.1 Search string I answered a question that you can add two search words in vector format. However, this does not work. Sorry. Only the first element will be used in the following. WDIsearch(string = c(&quot;female&quot;, &quot;gdp&quot;), field = &quot;name&quot;) 4.3.2.2 Avoiding a long search result to include in R Notebook It is OK to get a very long search result, but to include it in R Notebook, it is better to use as_tibble(). WDIsearch(string = &quot;savings&quot;, field = &quot;name&quot;, cache = NULL) %&gt;% datatable() 4.3.2.3 Summary by countries WDIsearch(string = &quot;SE.XPD.TOTL.GD.ZS&quot;, field = &quot;indicator&quot;, cache = NULL) ## indicator name ## 15908 SE.XPD.TOTL.GD.ZS Government expenditure on education, total (% of GDP) df_e &lt;- WDI( country = c(&quot;CN&quot;, &quot;IN&quot;, &quot;US&quot;, &quot;ID&quot;,&quot;PK&quot;, &quot;BR&quot;, &quot;NG&quot;, &quot;BD&quot;, &quot;RU&quot;, &quot;MX&quot;, &quot;JP&quot;,&quot;LKA&quot; ), indicator = c(education = &quot;SE.XPD.TOTL.GD.ZS&quot;), start = 2000, end = 2020) df_e %&gt;% datatable() ggplot(df_e) + geom_line(aes(x = year, y = education, color = country)) + labs(title = &quot;Government expenditure on education&quot;) ## Warning: Removed 61 rows containing missing values (`geom_line()`). summary(df_e)%&gt;% datatable() A summary of the government expenditure on education by country. df_e %&gt;% group_by(country) %&gt;% summarize( min = min(education, na.rm = TRUE), median = median(education, na.rm = TRUE), max = max(education, na.rm = TRUE), mean = mean(education, na.rm = TRUE)) %&gt;% arrange(desc(mean)) %&gt;% datatable() ## Warning in min(education, na.rm = TRUE): min の引数に有限な値がありません: Inf ## を返します ## Warning in max(education, na.rm = TRUE): max の引数に有限な値がありません: -Inf ## を返します 4.3.2.4 Comparison of two variables, I WDIsearch(string = &quot;NY.GDS.TOTL.CD&quot;, field = &quot;indicator&quot;) ## indicator name ## 11446 NY.GDS.TOTL.CD Gross domestic savings (current US$) df_pk &lt;- WDI( country = &quot;PK&quot;, indicator = c(gdpcap = &quot;NY.GDP.MKTP.CD&quot;, saving = &quot;NY.GDS.TOTL.CD&quot;), start = 1976, end = 2020) df_pk %&gt;% datatable() df_pk %&gt;% ggplot(aes(x = gdpcap, y = saving)) + geom_point() + geom_line() cor(df_pk$gdpcap, df_pk$saving) ## [1] 0.901141 df_pk %&gt;% summarize(correlation = cor(gdpcap, saving)) ## correlation ## 1 0.901141 4.3.2.5 Comparison of two variables, II WDIsearch(string = &quot;FP.CPI.TOTL&quot;, field = &quot;indicator&quot;) ## indicator name ## 7441 FP.CPI.TOTL Consumer price index (2010 = 100) ## 7442 FP.CPI.TOTL.ZG Inflation, consumer prices (annual %) WDIsearch(string = &quot;NE.CON.TOTL.KD.ZG&quot;, field = &quot;indicator&quot;) ## indicator name ## 11057 NE.CON.TOTL.KD.ZG Final consumption expenditure (annual % growth) df_cp_con &lt;- WDI( country = c(&quot;CN&quot;, &quot;VN&quot;, &quot;US&quot;, &quot;TH&quot;, &quot;GB&quot;, &quot;FR&quot;, &quot;RU&quot;, &quot;DE&quot;, &quot;JP&quot;), indicator = c(cp = &quot;FP.CPI.TOTL&quot;, con =&quot;NE.CON.TOTL.KD.ZG&quot;), start = 1990, end = 2020 ) df_cp_con %&gt;% datatable() Let us have a try. df_cp_con %&gt;% filter(year == 2020) %&gt;% ggplot() + geom_point(aes(x = cp, y = con)) ## Warning: Removed 2 rows containing missing values (`geom_point()`). It may be interesting to plot all countries in this format. df_cp_con %&gt;% ggplot(aes(x = cp, y = con, color = country)) + geom_point() + geom_line() ## Warning: Removed 69 rows containing missing values (`geom_point()`). ## Warning: Removed 69 rows containing missing values (`geom_line()`). df_cp_con %&gt;% ggplot(aes(x = cp, y = con)) + geom_point() + geom_line() + facet_wrap(vars(country)) ## Warning: Removed 69 rows containing missing values (`geom_point()`). ## Warning: Removed 37 rows containing missing values (`geom_line()`). 4.3.2.6 Comparison of two variables in different scales WDIsearch(string = &quot;SP.DYN.TFRT.IN&quot;, field = &quot;indicator&quot;, short = FALSE) ## indicator name ## 17316 SP.DYN.TFRT.IN Fertility rate, total (births per woman) ## description ## 17316 Total fertility rate represents the number of children that would be born to a woman if she were to live to the end of her childbearing years and bear children in accordance with age-specific fertility rates of the specified year. ## sourceDatabase ## 17316 World Development Indicators ## sourceOrganization ## 17316 (1) United Nations Population Division. World Population Prospects: 2019 Revision. (2) Census reports and other statistical publications from national statistical offices, (3) Eurostat: Demographic Statistics, (4) United Nations Statistical Division. Population and Vital Statistics Reprot (various years), (5) U.S. Census Bureau: International Database, and (6) Secretariat of the Pacific Community: Statistics and Demography Programme. df_jp &lt;- WDI( country = &quot;JP&quot;, indicator = c(total = &quot;SP.POP.TOTL&quot;, fertility = &quot;SP.DYN.TFRT.IN&quot;), start = 1960, end = 2020) df_jp %&gt;% datatable() df_jp %&gt;% ggplot() + geom_line(aes(x = year, y = total/1000000), color = &quot;blue&quot;) + geom_line(aes(x = year, y = fertility*60), color = &quot;red&quot;) + scale_y_continuous(&quot;Population (in million)&quot;, sec.axis = sec_axis(~ ./60, name = &quot;Fertility&quot;)) + labs(title = &quot;Total Population and Fertility Rate in Japan&quot;) 4.3.2.7 The data with many missing data. WDIsearch(string = &quot;NY.GNP.PCAP.PP.CD&quot;, field = &quot;indicator&quot;, cache = NULL) ## indicator name ## 11487 NY.GNP.PCAP.PP.CD GNI per capita, PPP (current international $) df_gni &lt;- WDI( country = c(&quot;CN&quot;, &quot;IN&quot;, &quot;US&quot;, &quot;ID&quot;,&quot;PK&quot;, &quot;BR&quot;, &quot;NG&quot;, &quot;BD&quot;, &quot;VN&quot;, &quot;MX&quot;, &quot;JP&quot;), indicator = c(gni =&quot;NY.GNP.PCAP.PP.CD&quot;), start = 1960, end = 2020, ) df_gni %&gt;% datatable() ggplot(df_gni) + geom_line(aes(x = year, y = gni, color = country)) + labs(title = &quot;GNI per capita, PPP (current international $)&quot;) ## Warning: Removed 330 rows containing missing values (`geom_line()`). The following code is counting the year there is at least one value, i.e., non NA value. is.na(x) gives a value, 1 for TRUE and 0 for FALSE, and !is.na(x) is the opposite. By taking a sum, you can find years with values. The following tells you that after 1990 11 countries have values while no country has value before 1989. df_gni %&gt;% group_by(year) %&gt;% summarize(noNA = sum(!is.na(gni))) %&gt;% datatable() df_recent &lt;- filter(df_gni, (year &gt;= 1990)) ggplot(df_recent) + geom_line(aes(x = year, y = gni, color = country)) + scale_y_continuous(trans=&#39;log10&#39;) + labs(title = &quot;GNI per capita, PPP (current international $)&quot;, subtitle = &quot;Log10 Scale&quot;) 4.3.2.8 The data in two columns in one chart. WDIsearch(string =&quot;SE.SEC.ENRR&quot;, field = &quot;indicator&quot;) ## indicator name ## 15786 SE.SEC.ENRR School enrollment, secondary (% gross) ## 15787 SE.SEC.ENRR.FE School enrollment, secondary, female (% gross) ## 15788 SE.SEC.ENRR.LO Gross enrolment ratio, lower secondary, both sexes (%) ## 15789 SE.SEC.ENRR.LO.FE Gross enrolment ratio, lower secondary, female (%) ## 15790 SE.SEC.ENRR.LO.MA Gross enrolment ratio, lower secondary, male (%) ## 15791 SE.SEC.ENRR.MA School enrollment, secondary, male (% gross) ## 15792 SE.SEC.ENRR.MF School Enroll. Ratio, secondary (%) ## 15793 SE.SEC.ENRR.UP Gross enrolment ratio, upper secondary, both sexes (%) ## 15794 SE.SEC.ENRR.UP.FE Gross enrolment ratio, upper secondary, female (%) ## 15795 SE.SEC.ENRR.UP.MA Gross enrolment ratio, upper secondary, male (%) The school enrollment of male and female for the lower secondary in ASIAN countries. df_enrr &lt;- WDI( country = c(&quot;BN&quot;, &quot;KH&quot;, &quot;TL&quot;, &quot;ID&quot;,&quot;LA&quot;, &quot;MY&quot;, &quot;MM&quot;, &quot;PH&quot;, &quot;SG&quot;, &quot;TH&quot;, &quot;VN&quot;), indicator = c(all = &quot;SE.SEC.ENRR&quot;, female = &quot;SE.SEC.ENRR.LO.FE&quot;, male = &quot;SE.SEC.ENRR.LO.MA&quot;), start = 2016, end = 2018 ) df_enrr %&gt;% datatable() The first solution. ggplot(df_enrr, aes(x = year, color = country)) + geom_line(aes(y = female, linetype = &quot;dashed&quot;)) + geom_line(aes(y = male, linetype = &quot;twodashed&quot;)) + labs(x= &quot;Year&quot;, y= &quot;Enrollment Ratio &quot;, title = &quot;Ratio of School enrollment\\nLower secondary (% gross) in ASEAN&quot;, subtitle = &quot;female - dashed lines, male - two dashed lines&quot;) The second solution would be: df_enrr %&gt;% pivot_longer(cols = c(female, male), names_to = &quot;gender&quot;, values_to = &quot;percent&quot;) %&gt;% ggplot(aes(x = year, y = percent, color = country, linetype = gender)) + geom_line() + labs(x= &quot;Year&quot;, y= &quot;Enrollment Ratio &quot;, title = &quot;Ratio of School enrollment\\nLower secondary (% gross) in ASEAN&quot;, subtitle = &quot;female - dashed lines, male - two dashed lines&quot;) The third solution could be: df_enrr %&gt;% pivot_longer(cols = c(female, male), names_to = &quot;gender&quot;, values_to = &quot;percent&quot;) %&gt;% ggplot(aes(x = year, y = percent, color = country)) + geom_line() + geom_point() + facet_grid(cols = vars(gender)) + labs(x= &quot;Year&quot;, y= &quot;Enrollment Ratio &quot;, title = &quot;Ratio of School enrollment\\nLower secondary (% gross) in ASEAN&quot;) 4.3.3 theme() for ggplot2() I did not introduce themes for ggplot2. If you are interested in it, please visit the following sites: https://ggplot2-book.org/polishing.html https://ggplot2.tidyverse.org/reference/theme.html, https://ggplot2.tidyverse.org/reference/ggtheme.html. "],["eda4.html", "Chapter 5 Exploratory Data Analysis (EDA) 4 5.1 Part I: More on Transforming by dplyr and Tidying by tidyr 5.2 Part II: Various Databases 5.3 Responses to the Fifth Assignment 5.4 CLASS.xlsx 5.5 Examples and Comments", " Chapter 5 Exploratory Data Analysis (EDA) 4 Course Contents 2021-12-08: Introduction: About the course - An introduction to open and public data, and data science 2021-12-15: Exploratory Data Analysis (EDA) 1 [lead by hs] - R Basics with RStudio and/or RStudio.cloud; R Script, swirl 2021-12-22: Exploratory Data Analysis (EDA) 2 [lead by hs] - R Markdown; Introduction to tidyverse; RStudio Primers 2022-01-12: Exploratory Data Analysis (EDA) 3 [lead by hs] - Introduction to tidyverse; Public Data, WDI, etc 2022-01-19: Exploratory Data Analysis (EDA) 4 [lead by hs] - Introduction to tidyverse; WDI, UN, , etc 2022-01-26: Exploratory Data Analysis (EDA) 5 [lead by hs] - Introduction to tidyverse; WDI, WHO, OECD, US gov, etc 2022-02-02: Inference Statistics 1 2022-02-09: Inference Statistics 2 2022-02-16: Inference Statistics 3 2022-02-23: Project Presentation Transforming and Tidying Data Transforming Data by dplyr for EDA Tidying Data by tidyr EDA (A diagram from R4DS by H.W. and G.G.) EDA from r4ds Contents of EDA4 More on Transforming Data by dplyr select, filter, mutate, arrange, group_by, summarize, etc. Tidying Data by tidyr pivot_longer, pivot_wider, drop_na, etc Example: A Study of Cases of Coronavirus Pandemic, III Importing Various Public Data and Examples WDI, World Bank, read_xl, i.e., read_excel, etc., for WDI Class Data UN data etc. 5.1 Part I: More on Transforming by dplyr and Tidying by tidyr 5.1.1 Data Transforamtion by dplyr Overview – r4ds. dplyr is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges: select() picks variables based on their names. filter() picks cases based on their values. mutate() adds new variables that are functions of existing variables arrange() changes the ordering of the rows. summarise() reduces multiple values down to a single summary. summarise() and summarize() are synonyms. group_by() converts into a grouped tbl where operations are performed “by group”. ungroup() removes grouping. slice() lets you index rows by their (integer) locations. It allows you to select, remove, and duplicate rows. 5.1.2 select: Subset columns using their names and types Helper Use Example - Columns except select(babynames, -prop) : Columns between select(babynames, year:n) contains() Columns that contains a string select(babynames, contains(“n”)) ends_with() Columns that ends with a string select(babynames, ends_with(“n”)) matches() Columns that matches a regex select(babynames, matches(“n”)) num_range() Columns with a numerical suffix in the range Not applicable with babynames one_of() Columns whose name appear in the given set select(babynames, one_of(c(“sex”, “gender”))) starts_with() Columns that starts with a string select(babynames, starts_with(“n”)) 5.1.3 iris ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa colnames(iris) &lt;- c(&quot;Sepal.L&quot;, &quot;Sepal.W&quot;, &quot;Petal.L&quot;, &quot;Petal.W&quot;, &quot;Species&quot;) slice(iris, 1:10) ## Sepal.L Sepal.W Petal.L Petal.W Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5.0 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa 5.1.4 select: Example slice(select(iris, sl = Sepal.L, sw = Sepal.W, species = Species), 1:2) ## sl sw species ## 1 5.1 3.5 setosa ## 2 4.9 3.0 setosa slice(select(iris, Petal.L:Species),1:2) ## Petal.L Petal.W Species ## 1 1.4 0.2 setosa ## 2 1.4 0.2 setosa 5.1.5 filter: Subset rows using column values Logical operator tests Example &gt; Is x greater than y? x &gt; y &gt;= Is x greater than or equal to y? x &gt;= y &lt; Is x less than y? x &lt; y &lt;= Is x less than or equal to y? x &lt;= y == Is x equal to y? x == y != Is x not equal to y? x != y is.na() Is x an NA? is.na(x) !is.na() Is x not an NA? !is.na(x) filter(iris, (Species == &quot;versicolor&quot;) &amp; Sepal.L %in% c(6.7, 7.0)) ## Sepal.L Sepal.W Petal.L Petal.W Species ## 1 7.0 3.2 4.7 1.4 versicolor ## 2 6.7 3.1 4.4 1.4 versicolor ## 3 6.7 3.0 5.0 1.7 versicolor ## 4 6.7 3.1 4.7 1.5 versicolor filter(iris, (Species == &quot;versicolor&quot;) &amp; (Sepal.L &gt;= 6.7 &amp; Sepal.L &lt;= 7.0)) ## Sepal.L Sepal.W Petal.L Petal.W Species ## 1 7.0 3.2 4.7 1.4 versicolor ## 2 6.9 3.1 4.9 1.5 versicolor ## 3 6.7 3.1 4.4 1.4 versicolor ## 4 6.8 2.8 4.8 1.4 versicolor ## 5 6.7 3.0 5.0 1.7 versicolor ## 6 6.7 3.1 4.7 1.5 versicolor 5.1.6 mutate Create, modify, and delete columns Useful mutate functions +, -, log(), etc., for their usual mathematical meanings lead(), lag() dense_rank(), min_rank(), percent_rank(), row_number(), cume_dist(), ntile() cumsum(), cummean(), cummin(), cummax(), cumany(), cumall() na_if(), coalesce() if_else(), recode(), case_when() 5.1.7 arrange and Pipe %&gt;% arrange() orders the rows of a data frame by the values of selected columns. Unlike other dplyr verbs, arrange() largely ignores grouping; you need to explicitly mention grouping variables (`or use .by_group = TRUE) in order to group by them, and functions of variables are evaluated once per data frame, not once per group. pipes in R for Data Science. Examples arrange(&lt;data&gt;, &lt;varible&gt;) arrange(&lt;data&gt;, desc(&lt;variable&gt;)) &lt;data&gt; %&gt;% ggplot() + geom_point(aes(x = &lt;&gt;, y = &lt;&gt;)) 5.1.8 group_by() and summarise() group_by summarise or summarize iris %&gt;% group_by(Species) %&gt;% summarize(sl = mean(Sepal.L), sw = mean(Sepal.W), pl = mean(Petal.L), pw = mean(Petal.W)) ## # A tibble: 3 × 5 ## Species sl sw pl pw ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 3.43 1.46 0.246 ## 2 versicolor 5.94 2.77 4.26 1.33 ## 3 virginica 6.59 2.97 5.55 2.03 5.1.9 Tidy Data “Happy families are all alike; every unhappy family is unhappy in its own way.” Leo Tolstoy “Tidy datasets are all alike, but every messy dataset is messy in its own way.” Hadley Wickham 5.1.9.1 Three interrelated rules of a tidy dataset Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. 5.1.10 Tidy Data and Untidy Data, I The following examples are taken from r4ds, 12. Tidy data. table1 ## # A tibble: 6 × 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 5.1.11 Tidy Data and Untidy Data, II table2 ## # A tibble: 12 × 4 ## country year type count ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 cases 745 ## 2 Afghanistan 1999 population 19987071 ## 3 Afghanistan 2000 cases 2666 ## 4 Afghanistan 2000 population 20595360 ## 5 Brazil 1999 cases 37737 ## 6 Brazil 1999 population 172006362 ## 7 Brazil 2000 cases 80488 ## 8 Brazil 2000 population 174504898 ## 9 China 1999 cases 212258 ## 10 China 1999 population 1272915272 ## 11 China 2000 cases 213766 ## 12 China 2000 population 1280428583 5.1.12 Tidy Data and Untidy Data, III table3 ## # A tibble: 6 × 3 ## country year rate ## * &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745/19987071 ## 2 Afghanistan 2000 2666/20595360 ## 3 Brazil 1999 37737/172006362 ## 4 Brazil 2000 80488/174504898 ## 5 China 1999 212258/1272915272 ## 6 China 2000 213766/1280428583 5.1.13 Tidy Data and Untidy Data, IV table4a; table4b ## # A tibble: 3 × 3 ## country `1999` `2000` ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 745 2666 ## 2 Brazil 37737 80488 ## 3 China 212258 213766 ## # A tibble: 3 × 3 ## country `1999` `2000` ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 19987071 20595360 ## 2 Brazil 172006362 174504898 ## 3 China 1272915272 1280428583 5.1.14 Tidy Data and Untidy Data, V table5 ## # A tibble: 6 × 4 ## country century year rate ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Afghanistan 19 99 745/19987071 ## 2 Afghanistan 20 00 2666/20595360 ## 3 Brazil 19 99 37737/172006362 ## 4 Brazil 20 00 80488/174504898 ## 5 China 19 99 212258/1272915272 ## 6 China 20 00 213766/1280428583 5.1.15 Tidying Data with pivot_longer, and pivot_wider 5.1.15.1 tidyr: Pivoting pivot_longer():Pivot data from wide to long, [old: gather()] pivot_longer(data = &lt;data&gt;, cols = &lt;column&gt;, names_to = &lt;new column&gt;, values_to = &lt;value column&gt;) pivot_longer(data = table4a, cols = c(1999,2000), names_to = &quot;year&quot;, values_to = &quot;population&quot; ) pivot_wider():Pivot data from long to wide, [old: spread()] pivot_wider(data = &lt;data&gt;, names_from = &lt;specified column&gt;, values_from = &lt;value column&gt;) pivot_wider(data = table2, names_from = type, values_from = count) 5.1.16 Example 1: pivot_longer table4a ## # A tibble: 3 × 3 ## country `1999` `2000` ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 745 2666 ## 2 Brazil 37737 80488 ## 3 China 212258 213766 pivot_longer(data = table4a, cols = c(&quot;1999&quot;,&quot;2000&quot;), names_to = &quot;year&quot;, values_to = &quot;cases&quot;, names_transform = list(&quot;year&quot; = as.integer)) 5.1.17 Example 1: pivot_longer pivot_longer(data = table4a, cols = c(&quot;1999&quot;,&quot;2000&quot;), names_to = &quot;year&quot;, values_to = &quot;cases&quot;, names_transform = list(&quot;year&quot; = as.integer)) ## # A tibble: 6 × 3 ## country year cases ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 ## 2 Afghanistan 2000 2666 ## 3 Brazil 1999 37737 ## 4 Brazil 2000 80488 ## 5 China 1999 212258 ## 6 China 2000 213766 5.1.18 Example 2: pivot_wider table2 %&gt;% slice(1:4) ## # A tibble: 4 × 4 ## country year type count ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 cases 745 ## 2 Afghanistan 1999 population 19987071 ## 3 Afghanistan 2000 cases 2666 ## 4 Afghanistan 2000 population 20595360 pivot_wider(data = table2, names_from = type, values_from = count) 5.1.19 Example 2: pivot_wider pivot_wider(data = table2, names_from = type, values_from = count) ## # A tibble: 6 × 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 5.1.20 Example 3: separate Separate a character column into multiple columns with a regular expression or numeric locations table3 %&gt;% slice(1:4) ## # A tibble: 4 × 3 ## country year rate ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745/19987071 ## 2 Afghanistan 2000 2666/20595360 ## 3 Brazil 1999 37737/172006362 ## 4 Brazil 2000 80488/174504898 separate(table3, rate, c(&quot;cases&quot;, &quot;population&quot;), sep = &quot;/&quot;, convert = TRUE) 5.1.21 Example 3: separate Separate a character column into multiple columns with a regular expression or numeric locations separate(table3, rate, c(&quot;cases&quot;, &quot;population&quot;), sep = &quot;/&quot;, convert = TRUE) ## # A tibble: 6 × 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 5.1.22 Example 3: unite Unite multiple columns into one by pasting strings together table5 %&gt;% slice(1:4) ## # A tibble: 4 × 4 ## country century year rate ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Afghanistan 19 99 745/19987071 ## 2 Afghanistan 20 00 2666/20595360 ## 3 Brazil 19 99 37737/172006362 ## 4 Brazil 20 00 80488/174504898 table5 %&gt;% unite(col = &quot;year&quot;, century, year, sep = &quot;&quot;) %&gt;% separate(rate, c(&quot;cases&quot;, &quot;population&quot;), sep = &quot;/&quot;, convert = TRUE) %&gt;% mutate(year = as.integer(year), rate = cases / population) 5.1.23 Example 3: unite Unite multiple columns into one by pasting strings together table5 %&gt;% unite(col = &quot;year&quot;, century, year, sep = &quot;&quot;) %&gt;% separate(rate, c(&quot;cases&quot;, &quot;population&quot;), sep = &quot;/&quot;, convert = TRUE) %&gt;% mutate(year = as.integer(year), rate = cases / population) ## # A tibble: 6 × 5 ## country year cases population rate ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan 1999 745 19987071 0.0000373 ## 2 Afghanistan 2000 2666 20595360 0.000129 ## 3 Brazil 1999 37737 172006362 0.000219 ## 4 Brazil 2000 80488 174504898 0.000461 ## 5 China 1999 212258 1272915272 0.000167 ## 6 China 2000 213766 1280428583 0.000167 5.1.24 Example 5: bind_rows table4a; table4b ## # A tibble: 3 × 3 ## country `1999` `2000` ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 745 2666 ## 2 Brazil 37737 80488 ## 3 China 212258 213766 ## # A tibble: 3 × 3 ## country `1999` `2000` ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 19987071 20595360 ## 2 Brazil 172006362 174504898 ## 3 China 1272915272 1280428583 5.1.25 Example 5: bind_rows - 1st Step tables &lt;- list(cases = table4a, population = table4b) tables %&gt;% bind_rows(.id = &quot;type&quot;) ## # A tibble: 6 × 4 ## type country `1999` `2000` ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 cases Afghanistan 745 2666 ## 2 cases Brazil 37737 80488 ## 3 cases China 212258 213766 ## 4 population Afghanistan 19987071 20595360 ## 5 population Brazil 172006362 174504898 ## 6 population China 1272915272 1280428583 5.1.26 Example 5: bind_rows tables &lt;- list(cases = table4a, population = table4b) tables %&gt;% bind_rows(.id = &quot;type&quot;) %&gt;% pivot_longer(cols = c(&quot;1999&quot;, &quot;2000&quot;), names_to = &quot;year&quot;) %&gt;% pivot_wider(names_from = &quot;type&quot;, values_from = &quot;value&quot;) %&gt;% mutate(year = as.integer(year)) %&gt;% arrange(country) ## # A tibble: 6 × 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 5.1.27 Example I A Study of Cases of Coronavirus Pandemic, III Data of Johns Hopkins Universiy and World Bank 5.1 Importing Raw Data 5.2 Tidying and Combining: To create country level and global combined data 5.3 Aggregated by Countries 5.4 Population of 2019 5.5 Analysis Suggested by Rami Krispin 5.2 Part II: Various Databases 5.2.1 Example II Introduction to Public Data World Bank 2.6 World Bank Country and Lending Groups and an Option extra = TRUE 2.6.1 Review Basics: World Development Indicators: ?WDI 2.6.2 World Bank Country and Lending Groups 2.6.3 Importing Excel Files 2.6.4 Filtering Join 2.6.5 Join Tables 2.6.6 Join Tables: Quick References United Nations 3.1 Importing Data 3.2 Example 5.2.2 World Development Indicators: ?WDI Basic Usage WDI(country = &quot;all&quot;, indicator = &quot;NY.GDP.PCAP.KD&quot;) Vector Notation WDI(country = c(&quot;US&quot;, &quot;CN&quot;, JP&quot;), # ISO-2 codes indicator = c(&quot;gdp_pcap&quot; = &quot;NY.GDP.PCAP.KD&quot;, &quot;life_exp&quot; = &quot;SP.DYN.LE00.IN&quot;)) Use Extra WDI(country = &quot;all&quot;, indicator = c(&quot;gdp_pcap&quot; = NY.GDP.PCAP.KD&quot;, &quot;life_exp&quot; = &quot;SP.DYN.LE00.IN&quot;), extra = TRUE) extra: TRUE returns extra variables such as region, iso3c code, and incomeLevel 5.2.3 World Bank Country and Lending Groups URL World Bank Country and Lending Groups The current classification by income in XLS format 5.2.3.1 About CLASS.xlsx This table classifies all World Bank member countries (189), and all other economies with populations of more than 30,000. For operational and analytical purposes, economies are divided among income groups according to 2019 gross national income (GNI) per capita, calculated using the World Bank Atlas method. The groups are: low income, $1,035 or less; lower middle income, $1,036 - 4,045; upper middle income, $4,046 - 12,535; and high income, $12,536 or more. The effective operational cutoff for IDA eligibility is $1,185 or less. 5.2.3.2 Geographic classifications IDA countries are those that lack the financial ability to borrow from IBRD. IDA credits are deeply concessional—interest-free loans and grants for programs aimed at boosting economic growth and improving living conditions. IBRD loans are noncessional. Blend countries are eligible for IDA credits because of their low per capita incomes but are also eligible for IBRD because they are financially creditworthy. 5.2.3.3 Note The term country, used interchangeably with economy, does not imply political independence but refers to any territory for which authorities report separate social or economic statistics. Income classifications set on 1 July 2020 remain in effect until 1 July 2021. Argentina, which was temporarily unclassified in July 2016 pending release of revised national accounts statistics, was classified as upper middle income for FY17 as of 29 September 2016 based on alternative conversion factors. Also effective 29 September 2016, Syrian Arab Republic is reclassified from IBRD lending category to IDA-only. On 29 March 2017, new country codes were introduced to align World Bank 3-letter codes with ISO 3-letter codes: Andorra (AND), Dem. Rep. Congo (COD), Isle of Man (IMN), Kosovo (XKX), Romania (ROU), Timor-Leste (TLS), and West Bank and Gaza (PSE). 5.2.4 Importing Excel Files, Part I CLASS.xlsx: - copy the following link The current classification by income in XLS format readxl: https://readxl.tidyverse.org Help: read_excel, read_xls, read_xlsx url_class &lt;- &quot;http://databank.worldbank.org/ data/download/site-content/CLASS.xlsx&quot; download.file(url = url_class, destfile = &quot;data/CLASS.xlsx&quot;) library(readxl) wb_countries_tmp &lt;- read_excel(&quot;data/CLASS.xlsx&quot;, sheet = 1, skip = 4, n_max =219) %&gt;% slice(-1) wb_countries &lt;- wb_countries_tmp %&gt;% select(Economy, Code, Region, `Income group`, Code, Region:Other) wb_countries 5.2.5 Importing Excel Files, Part II readxl: https://readxl.tidyverse.org Help: read_excel, read_xls, read_xlsx wb_regions_tmp &lt;- read_excel(&quot;data/CLASS.xlsx&quot;, sheet = 1, skip = 4, n_max =272) %&gt;% slice(-(1:221)) wb_regions &lt;- wb_regions_tmp %&gt;% select(Economy:Code) %&gt;% drop_na() wb_regions wb_groups_tmp &lt;- read_excel(&quot;data/CLASS.xlsx&quot;, sheet = &quot;Groups&quot;) # sheet = 3 wb_groups_tmp Please study ‘About Assignment Five’ in Moodle 5.2.6 Filtering Joins gdp_pcap &lt;- WDI(country = &quot;all&quot;, indicator = &quot;NY.GDP.PCAP.KD&quot;) gdp_pcap_extra &lt;- WDI(country = &quot;all&quot;, indicator = &quot;NY.GDP.PCAP.KD&quot;, extra = TRUE) Compare the following: gdp_pcap %&gt;% semi_join(country_list) gdp_pcap %&gt;% filter(region != &quot;Aggregates&quot;) gdp_pcap %&gt;% anti_join(country_list) Please study ‘About Assignment Five’ in Moodle 5.2.7 Join Tables There are three types of joining tables. Commands are from tidyverse packages though there is a way to do the same by Base R with appropriate arguments. Bind rows: bind_rows(), intersect(), setdiff(), union() Bind columns: bind_cols(), left_join(), right_join(), inner_join(), full_join() Filtering join: semi_join(), anti_join() 5.2.8 Join Tables: Quick References https://r4ds.had.co.nz/relational-data.html#relational-data Cheatsheet: Data Transformation, pages 2 and 3. You can download it from RStudio &gt; Help. Tidyverse Homepage: Efficiently bind multiple data frames by row and column: bind_rows(), bind_cols() Set operations: intersect(), setdiff(), union() Mutating joins: left_join(), right_join(), inner_join(), full_join() Filtering joins: semi_join(), anti_join() R Studio Primers: Tidy Your Data – r4ds: Wrangle, II Reshape Data, Separate and Unite Columns, Join Data Sets 5.2.9 Public Data: UN Data UN Data: https://data.un.org 5.2.9.1 Importing Data Get the URL (uniform resource locator) - copy the link url_of_data &lt;- \"https://data.un.org/--long url--.csv\" Download the file into the destfile in data folder: download.file(url = url_of_data, destfile = \"data/un_pop.csv\") Read the file: df_un_pop &lt;- read_csv(\"data/un_pop.csv\") 5.2.10 Learning Resources, IV R for Data Science, Part III Wrangle R for Data Science, Part III Wrangle - Tidy and Relational Tidy Data: https://vita.had.co.nz/papers/tidy-data.pdf https://github.com/hadley/tidy-data 5.2.10.1 RStudio Primers: See References in Moodle at the bottom The Basics – r4ds: Explore, I Work with Data – r4ds: Wrangle, I Visualize Data – r4ds: Explore, II Exploratory Data Analysis, Bar Charts, Histograms Boxplots and Counts, Scatterplots, Line Plots Overplotting and Big Data, Customize Your Plots Tidy Your Data – r4ds: Wrangle, II Reshape Data, Separate and Unite Columns, Join Data Sets 5.2.11 The Fifth Assignment (in Moodle) Choose a World Bank or UN data. Clearly state how to obtain the data. Even if you are able to give the URL to download the data, explain the steps you reached and obtained the data. If you choose WDI, use CLASS.xlsx, World Bank Country and Lending Groups information. Create an R Notebook (file name.nb.html) of an EDA containing: title, date, and author, i.e., Your Name your motivation and/or objectives to analyse the data, and your questions an explanation of the data and the variables chunks containing the following: visualize the data with ggplot() your findings and/or questions file name: a5_ID.nb.html, e.g. a5_123456.nb.html Submit your R Notebook file to Moodle (The Fifth Assignment) by 2022-01-25 23:59:00 5.3 Responses to the Fifth Assignment 5.3.1 Setup and YAML library(tidyverse) library(WDI) 5.4 CLASS.xlsx 5.4.1 Introduction There are several reasons we study CLASS.xlsx WDI data contains not only the data of each country but the aggregated data of regions, income level, etc. We need to know the definitions of these groups and countries in them. CLASS.xlsx provides the information. Why are groups important? When we study a trend of data of a country, we need to look at the following. The trends of the aggregated data of the World. The comparison of the trends of the aggregated data of groups. The comparison of the trends of the data of the countries within the group the country of your interest belongs to. We first download CLASS.xlsx from world bank and obtain the data so that we can use it with other data. See Introduction to Public Data in Moodle, Week 5. 5.4.1.1 Importing Excel Files CLASS.xlsx: - copy the following link The current classification by income in XLS format readxl: https://readxl.tidyverse.org Help: read_excel, read_xls, read_xlsx url_class &lt;- &quot;https://databankfiles.worldbank.org/data/download/site-content/CLASS.xlsx&quot; download.file(url = url_class, destfile = &quot;data/CLASS.xlsx&quot;) 5.4.1.1.1 Countries Let us look at the first sheet. The column names are in the 5th row. The country data starts from the 7th row. Zimbabue is at the last row. library(readxl) wb_countries_tmp &lt;- read_excel(&quot;data/CLASS.xlsx&quot;, sheet = 1, skip = 0, n_max =219) wb_countries &lt;- wb_countries_tmp %&gt;% select(country = Economy, iso3c = Code, region = Region, income = `Income group`, lending = &quot;Lending category&quot;, other = &quot;Other (EMU or HIPC)&quot;) DT::datatable(wb_countries) 5.4.1.1.2 Regions readxl: https://readxl.tidyverse.org Help: read_excel, read_xls, read_xlsx Regions start from the 221th row. Regions end at the 266th row. wb_regions_tmp &lt;- read_excel(&quot;data/CLASS.xlsx&quot;, sheet = 1, skip = 0, n_max =266) %&gt;% slice(-(1:220)) wb_regions &lt;- wb_regions_tmp %&gt;% select(region = Economy, iso3c = Code) %&gt;% drop_na() DT::datatable(wb_regions) Let us look at the second sheet. wb_groups_tmp &lt;- read_excel(&quot;data/CLASS.xlsx&quot;, sheet = &quot;Groups&quot;) # sheet = 3 wb_groups &lt;- wb_groups_tmp %&gt;% select(gcode = GroupCode, group = GroupName, iso3c = CountryCode, country = CountryName) Let us find out how many countries are in each group. wb_groups %&gt;% group_by(group) %&gt;% summarize(number_of_countries = n_distinct(country)) %&gt;% DT::datatable() wb_groups %&gt;% group_by(group) %&gt;% summarize(number_of_countries = n_distinct(country)) %&gt;% ggplot(aes(y = group, x = number_of_countries)) + geom_col() Let us find out how many groups each country belongs to. wb_groups %&gt;% group_by(country) %&gt;% summarize(number_of_groups = n_distinct(group)) %&gt;% arrange(desc(number_of_groups)) %&gt;% DT::datatable() From this we find that each country is classified into several groups. 5.4.1.2 Just for Fun We use an intermediate level skill of defining a function in this subsection. Let us make a list of iso3c codes each country belongs to. As we have seen above, Comoros belongs to 17 groups. wb_groups %&gt;% filter(country == &quot;Comoros&quot;) %&gt;% select(gcode, group) %&gt;% DT::datatable() wb_groups %&gt;% filter(country == &quot;Comoros&quot;) %&gt;% pull(gcode) %&gt;% paste(collapse = &quot;, &quot;) ## [1] &quot;ARB, HPC, IBT, IDA, IDX, LDC, OSS, PRE, SSF, SST, TSS, WLD, AFE, FCS, LMC, LMY, MIC, SSA&quot; cd &lt;- function(x){wb_groups %&gt;% filter(country == x) %&gt;% pull(gcode) %&gt;% paste(collapse = &quot;, &quot;)} cd(&quot;Japan&quot;) ## [1] &quot;EAS, OED, PST, WLD, OED, HIC&quot; cd(&quot;Comoros&quot;) ## [1] &quot;ARB, HPC, IBT, IDA, IDX, LDC, OSS, PRE, SSF, SST, TSS, WLD, AFE, FCS, LMC, LMY, MIC, SSA&quot; cd(&quot;Zimbabwe&quot;) ## [1] &quot;EAR, IBT, IDA, IDB, SSF, TSS, WLD, AFE, FCS, LMC, LMY, MIC, SSA&quot; cd(&quot;Pakistan&quot;) ## [1] &quot;EAR, IBT, IDA, IDB, SAS, TSA, WLD, LMC, LMY, MIC&quot; wb_groups %&gt;% group_by(country) %&gt;% summarize(group_codes = cd(country)) %&gt;% DT::datatable() On the right of the table, you can find a list of group codes of each country. 5.5 Examples and Comments In the following I include my feedback keeping anonymity. 5.5.1 Information Obtained by extra = TRUE The first way is to use the option extra = TRUE when you import the data using WDI. 5.5.1.1 Population of Zimbabwe and Similar Countries df_population &lt;- as_tibble(WDI( country = &quot;all&quot;, indicator = c(population = &quot;SP.POP.TOTL&quot;), start = 1960, end = 2020, extra = TRUE )) DT::datatable(df_population) ## Warning in instance$preRenderHook(instance): It seems your data is too big ## for client-side DataTables. You may consider server-side processing: https:// ## rstudio.github.io/DT/server.html Find out extra information on Zimbabwe, i.e., which group Zimbabwe is in for region, income and lending. df_population %&gt;% filter(country == &quot;Zimbabwe&quot;) %&gt;% select(region, income, lending) %&gt;% distinct() ## # A tibble: 1 × 3 ## region income lending ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Sub-Saharan Africa Lower middle income Blend Find out all countries in the same groups as Zimbabwe. df_population %&gt;% filter(region == &quot;Sub-Saharan Africa&quot;, income == &quot;Lower middle income&quot;, lending == &quot;Blend&quot;) %&gt;% distinct(country) ## # A tibble: 6 × 1 ## country ## &lt;chr&gt; ## 1 Cabo Verde ## 2 Cameroon ## 3 Congo, Rep. ## 4 Kenya ## 5 Nigeria ## 6 Zimbabwe Since there are only 6 countries belonging to the same group as Zimbabwe, let us widen our search. df_population %&gt;% filter(region == &quot;Sub-Saharan Africa&quot;, income == &quot;Lower middle income&quot;) %&gt;% distinct(country) ## # A tibble: 17 × 1 ## country ## &lt;chr&gt; ## 1 Angola ## 2 Benin ## 3 Cabo Verde ## 4 Cameroon ## 5 Comoros ## 6 Congo, Rep. ## 7 Cote d&#39;Ivoire ## 8 Eswatini ## 9 Ghana ## 10 Kenya ## 11 Lesotho ## 12 Mauritania ## 13 Nigeria ## 14 Sao Tome and Principe ## 15 Senegal ## 16 Tanzania ## 17 Zimbabwe The following is the first chart. df_population %&gt;% filter(region == &quot;Sub-Saharan Africa&quot;, income == &quot;Lower middle income&quot;, lending == &quot;Blend&quot;) %&gt;% ggplot() + geom_line(aes(x = year, y = population, color = country)) + scale_y_log10() The second chart. df_population %&gt;% filter(region == &quot;Sub-Saharan Africa&quot;, income == &quot;Lower middle income&quot;) %&gt;% ggplot() + geom_line(aes(x = year, y = population, color = country)) + scale_y_log10() Fixing the income level and compare the regions. df_population %&gt;% filter(income == &quot;Lower middle income&quot;) %&gt;% group_by(region, year) %&gt;% summarize(average_in_region = mean(population, na.rm = TRUE)) %&gt;% ggplot() + geom_line(aes(x = year, y = average_in_region, color = region)) ## `summarise()` has grouped output by &#39;region&#39;. You can override using the ## `.groups` argument. A chart in log 10 scale. df_population %&gt;% filter(income == &quot;Lower middle income&quot;) %&gt;% group_by(region, year) %&gt;% summarize(average_in_region = mean(population, na.rm = TRUE)) %&gt;% ggplot() + geom_line(aes(x = year, y = average_in_region, color = region)) + scale_y_log10() ## `summarise()` has grouped output by &#39;region&#39;. You can override using the ## `.groups` argument. 5.5.1.2 Fertility Rate of Japan and Similar Countries df_fertility &lt;- as_tibble(WDI( country = &quot;all&quot;, indicator = c(fertility = &quot;SP.DYN.TFRT.IN&quot;), start = 1960, end = 2020, extra = TRUE)) DT::datatable(df_fertility) ## Warning in instance$preRenderHook(instance): It seems your data is too big ## for client-side DataTables. You may consider server-side processing: https:// ## rstudio.github.io/DT/server.html df_fertility %&gt;% filter(country == &quot;Japan&quot;) %&gt;% select(region, income, lending) %&gt;% distinct() ## # A tibble: 1 × 3 ## region income lending ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 East Asia &amp; Pacific High income Not classified Let us compare Japan’s fertility rate with similar countries, i.e., high income countries in East Asia &amp; Pacific df_fertility %&gt;% filter(region == &quot;East Asia &amp; Pacific&quot;, income == &quot;High income&quot;) %&gt;% distinct(country) ## # A tibble: 13 × 1 ## country ## &lt;chr&gt; ## 1 Australia ## 2 Brunei Darussalam ## 3 French Polynesia ## 4 Guam ## 5 Hong Kong SAR, China ## 6 Japan ## 7 Korea, Rep. ## 8 Macao SAR, China ## 9 Nauru ## 10 New Caledonia ## 11 New Zealand ## 12 Northern Mariana Islands ## 13 Singapore df_fertility %&gt;% filter(region == &quot;East Asia &amp; Pacific&quot;, income == &quot;High income&quot;) %&gt;% ggplot() + geom_line(aes(x = year, y = fertility, color = country)) + labs(title = &quot;Fertility Rate of High Income Countries in East Asia &amp; Pacific&quot;) ## Warning: Removed 122 rows containing missing values (`geom_line()`). Let us check the number of countries in each region. df_fertility %&gt;% filter(income == &quot;High income&quot;) %&gt;% select(country, region) %&gt;% distinct() %&gt;% group_by(region) %&gt;% summarize(number_of_countries = n_distinct(country)) ## # A tibble: 6 × 2 ## region number_of_countries ## &lt;chr&gt; &lt;int&gt; ## 1 East Asia &amp; Pacific 13 ## 2 Europe &amp; Central Asia 37 ## 3 Latin America &amp; Caribbean 17 ## 4 Middle East &amp; North Africa 8 ## 5 North America 3 ## 6 Sub-Saharan Africa 1 df_fertility %&gt;% filter(income == &quot;High income&quot;) %&gt;% group_by(region, year) %&gt;% summarize(average_in_region = mean(fertility, na.rm = TRUE)) %&gt;% ggplot() + geom_line(aes(x = year, y = average_in_region, color = region)) ## `summarise()` has grouped output by &#39;region&#39;. You can override using the ## `.groups` argument. ## Warning: Removed 16 rows containing missing values (`geom_line()`). 5.5.2 Other Groups So far we used only three classifications of groups, region, income and lending. However, CLASS.xls contains more information. For example if we compare data within a group, say Arab World (ARB), we need the information taken from CLASS.xlsx. If you need the information of the aggregated data of the group, it is included. We know that there are 22 countries. wb_arb &lt;- wb_groups %&gt;% filter(gcode == &quot;ARB&quot;) %&gt;% select(iso3c, country) wb_arb_vec &lt;- wb_arb %&gt;% pull(iso3c) wb_arb ## # A tibble: 22 × 2 ## iso3c country ## &lt;chr&gt; &lt;chr&gt; ## 1 ARE United Arab Emirates ## 2 BHR Bahrain ## 3 COM Comoros ## 4 DJI Djibouti ## 5 DZA Algeria ## 6 EGY Egypt, Arab Rep. ## 7 IRQ Iraq ## 8 JOR Jordan ## 9 KWT Kuwait ## 10 LBN Lebanon ## # … with 12 more rows wb_arb_vec ## [1] &quot;ARE&quot; &quot;BHR&quot; &quot;COM&quot; &quot;DJI&quot; &quot;DZA&quot; &quot;EGY&quot; &quot;IRQ&quot; &quot;JOR&quot; &quot;KWT&quot; &quot;LBN&quot; &quot;LBY&quot; &quot;MAR&quot; ## [13] &quot;MRT&quot; &quot;OMN&quot; &quot;PSE&quot; &quot;QAT&quot; &quot;SAU&quot; &quot;SDN&quot; &quot;SOM&quot; &quot;SYR&quot; &quot;TUN&quot; &quot;YEM&quot; I created the data frame containing Arab World countries and their iso3c codes together with a vector containing iso3c only. 5.5.2.1 iso3c vector: wb_arb_vec df_fertility %&gt;% filter(iso3c %in% wb_arb_vec) %&gt;% ggplot(aes(x = year, y = fertility, color = country)) + geom_line() ## Warning: Removed 30 rows containing missing values (`geom_line()`). #### Data Frame: wb_arb For example, if the original data does not have an iso3c code, you can obtain a part of data refering to the column they share in common. In the following I used by = \"country\" in the second code chunk. df_fertility %&gt;% right_join(wb_arb) %&gt;% ggplot(aes(x = year, y = fertility, color = country)) + geom_line() + labs(title = &quot;right_join without using by&quot;) ## Joining, by = c(&quot;country&quot;, &quot;iso3c&quot;) ## Warning: Removed 30 rows containing missing values (`geom_line()`). df_fertility %&gt;% right_join(wb_arb, by = &quot;country&quot;) %&gt;% ggplot(aes(x = year, y = fertility, color = country)) + geom_line() + labs(title = &quot;`right_join` with `by`&quot;) ## Warning: Removed 30 rows containing missing values (`geom_line()`). 5.5.3 Wangling: Transform and Tidy Data url_class &lt;- &quot;https://api.worldbank.org/v2/en/indicator/EN.ATM.CO2E.PC?downloadformat=excel&quot; download.file(url = url_class, mode = &quot;wb&quot;, destfile = &quot;data/API_EN.ATM.CO2E.PC_DS2_en_excel_v2_3469464.xls&quot;) 5.5.3.1 Tidying data country_tmp &lt;- read_excel(&quot;data/API_EN.ATM.CO2E.PC_DS2_en_excel_v2_3469464.xls&quot;, sheet = 1, skip = 3, n_max =271) %&gt;% slice(-1) DT::datatable(country_tmp) The following is an original code chunk. Co2_country &lt;- country_tmp %&gt;% select(`Country Name`, `Country Code`, `Indicator Name`, `Indicator Code`,`1960`:`2018`) Co2_country &lt;- pivot_longer(Co2_country,cols = 5:63, names_to = &quot;year&quot;, values_to = &quot;Co2&quot;, values_drop_na = TRUE) %&gt;% select(ID = `Country Code`, Country = `Country Name`, year, Co2) DT::datatable(Co2_country) Co2_country %&gt;% filter(ID == &quot;WLD&quot;) %&gt;% ggplot(aes(x = year, y = Co2)) + geom_line() ## `geom_line()`: Each group consists of only one observation. ## ℹ Do you need to adjust the group aesthetic? Empty chart. The following is much better but we cannot see year and a line graph may be preferable to see changes. Please notice that year is in character not in integer. Co2_country %&gt;% filter(ID == &quot;WLD&quot;) %&gt;% ggplot(aes(x = year, y = Co2)) + geom_point() In the following I used Base R command. Co2_country_rev$year denotes the column year of the data frame Co2_country_rev. Co2_country_rev &lt;- Co2_country Co2_country_rev$year &lt;- as.integer(Co2_country_rev$year) Co2_country_rev %&gt;% filter(ID == &quot;WLD&quot;) %&gt;% ggplot(aes(x = year, y = Co2)) + geom_line() Using dplyr, the following is an alternative. Co2_country_rev2 &lt;- Co2_country %&gt;% mutate(year = as.integer(year)) Co2_country_rev2 %&gt;% filter(ID == &quot;WLD&quot;) %&gt;% ggplot(aes(x = year, y= Co2)) + geom_line() It is possible to set year to be an integer variable when pivot_longer is applied by adding names_transform = list(year = as.integer). co2_country &lt;- country_tmp %&gt;% select(&#39;Country Name&#39;, &#39;Country Code&#39;, &#39;Indicator Name&#39;, &#39;Indicator Code&#39;,&#39;1960&#39;:&#39;2018&#39;) co2_country &lt;- pivot_longer(co2_country,cols = 5:63, names_to = &quot;year&quot;, names_transform = list(year = as.integer), values_to = &quot;Co2&quot;, values_drop_na = TRUE) %&gt;% select(ID = &#39;Country Code&#39;, Country = &#39;Country Name&#39;, year, Co2) DT::datatable(co2_country) co2_country &lt;- country_tmp %&gt;% pivot_longer(cols = 5:63, names_to = &quot;year&quot;, names_transform = list(year = as.integer), values_to = &quot;co2&quot;, values_drop_na = TRUE) %&gt;% select(id = &#39;Country Code&#39;, country = &#39;Country Name&#39;, year, co2) DT::datatable(co2_country) co2_country %&gt;% filter(id %in% c(&quot;WLD&quot;, &quot;USA&quot;, &quot;CHN&quot;, &quot;JPN&quot;)) %&gt;% ggplot(aes(x = year, y= co2, color = id)) + geom_line() COUNTRIES &lt;- c(&quot;China&quot;,&quot;Japan&quot;,&quot;United States&quot;,&quot;Great Britain&quot;,&quot;India&quot;,&quot;South Africa&quot;,&quot;Malaysia&quot;,&quot;Russia&quot;,&quot;Australia&quot;, &quot;Canada&quot;, &quot;Vietnam&quot;) co2_country %&gt;% filter(country %in% COUNTRIES) %&gt;% ggplot(aes(x = year, y= co2, color = country)) + geom_line() + geom_point() Compare the data above and the those in WDI. WID Data URL: https://data.worldbank.org/indicator/EN.ATM.CO2E.KT Description: WDIsearch(string = &quot;co2&quot;, field = &quot;name&quot;) %&gt;% DT::datatable() 5.5.4 Population Analysis The original analysis uses Base R commands a lot, I do a similar analysis using tidyverse as an example. The following population data has seven variables. url &lt;- &quot;https://data.un.org/_Docs/SYB/CSV/SYB64_246_202110_Population%20Growth,%20Fertility%20and%20Mortality%20Indicators.csv&quot; df_un_pop &lt;- read_csv(url, skip = 1) ## New names: ## • `` -&gt; `...2` ## Warning: One or more parsing issues, call `problems()` on your data frame for details, e.g.: ## dat &lt;- vroom(...) ## problems(dat) ## Rows: 4899 Columns: 7 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): ...2, Series, Footnotes, Source ## dbl (3): Region/Country/Area, Year, Value ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. df_un_pop ## # A tibble: 4,899 × 7 ## `Region/Country/Area` ...2 Year Series Value Footn…¹ Source ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Total, all countries… 2010 Popul… 1.2 Data r… &quot;Unit… ## 2 1 Total, all countries… 2010 Total… 2.6 Data r… &quot;Unit… ## 3 1 Total, all countries… 2010 Infan… 41 Data r… &quot;Unit… ## 4 1 Total, all countries… 2010 Mater… 248 &lt;NA&gt; &quot;Worl… ## 5 1 Total, all countries… 2010 Life … 68.9 Data r… &quot;Unit… ## 6 1 Total, all countries… 2010 Life … 66.7 Data r… &quot;Unit… ## 7 1 Total, all countries… 2010 Life … 71.3 Data r… &quot;Unit… ## 8 1 Total, all countries… 2015 Popul… 1.2 Data r… &quot;Unit… ## 9 1 Total, all countries… 2015 Total… 2.5 Data r… &quot;Unit… ## 10 1 Total, all countries… 2015 Infan… 33.9 Data r… &quot;Unit… ## # … with 4,889 more rows, and abbreviated variable name ¹​Footnotes Check the regions of interst. df_un_pop %&gt;% filter(`Region/Country/Area` %in% c(2,9,21,419,142,150)) %&gt;% distinct(`...2`) ## # A tibble: 6 × 1 ## ...2 ## &lt;chr&gt; ## 1 Africa ## 2 Northern America ## 3 Latin America &amp; the Caribbean ## 4 Asia ## 5 Europe ## 6 Oceania df_un_pop_rev &lt;- df_un_pop %&gt;% select(rn = `Region/Country/Area`, region = `...2`, year = Year, series = Series, value = Value) %&gt;% filter(rn %in% c(2,9,21,419,142,150)) DT::datatable(df_un_pop_rev) df_un_pop_rev %&gt;% distinct(series) ## # A tibble: 7 × 1 ## series ## &lt;chr&gt; ## 1 Population annual rate of increase (percent) ## 2 Total fertility rate (children per women) ## 3 Infant mortality for both sexes (per 1,000 live births) ## 4 Life expectancy at birth for both sexes (years) ## 5 Life expectancy at birth for males (years) ## 6 Life expectancy at birth for females (years) ## 7 Maternal mortality ratio (deaths per 100,000 population) df_un_pop_rev %&gt;% filter(series == &quot;Population annual rate of increase (percent)&quot;) %&gt;% ggplot(aes(x = year, y = value, color = region)) + geom_line() + labs(title = &quot;Population annual rate of increase (percent)&quot;) df_un_pop_rev %&gt;% filter(series == &quot;Total fertility rate (children per women)&quot;) %&gt;% ggplot(aes(x = year, y = value, color = region)) + geom_line() + labs(title = &quot;Total fertility rate (children per women)&quot;) df_un_pop_rev %&gt;% filter(series == &quot;Infant mortality for both sexes (per 1,000 live births)&quot;) %&gt;% ggplot(aes(x = year, y = value, color = region)) + geom_line() + labs(title = &quot;Infant mortality for both sexes (per 1,000 live births)&quot;) 5.5.5 Literacy rate, youth total (% of people ages 15-24) I will introduce this because it uses semi_join to choose countries’ data deleting aggrigated data. WDIsearch(string = &quot;SE.ADT.1524.LT.ZS&quot;, field = &quot;indicator&quot;, cache = NULL) ## indicator name ## 15166 SE.ADT.1524.LT.ZS Literacy rate, youth total (% of people ages 15-24) literacy_rate_youth &lt;- WDI(country = &quot;all&quot;, indicator = &quot;SE.ADT.1524.LT.ZS&quot;) DT::datatable(literacy_rate_youth) ## Warning in instance$preRenderHook(instance): It seems your data is too big ## for client-side DataTables. You may consider server-side processing: https:// ## rstudio.github.io/DT/server.html 5.5.5.1 Filtering joins Description Filtering joins filter rows from x based on the presence or absence of matches in y: semi_join() return all rows from x with a match in y. anti_join() return all rows from x without a match in y. Combine the literacy rate data with the country data semi_joint: list contains countries only listed in wb_countries deleted some of the aggregated data literacy_rate_youth_country &lt;- literacy_rate_youth %&gt;% semi_join(wb_countries, by = &quot;country&quot;) DT::datatable(literacy_rate_youth_country) Compare with the following. literacy_rate_youth_aggregated &lt;- literacy_rate_youth %&gt;% anti_join(wb_countries, by = &quot;country&quot;) DT::datatable(literacy_rate_youth_aggregated) 5.5.6 Visualization of the data 5.5.6.1 Gender: Ratio of girls to boys in primary, secondary and tertiary levels Went to http://data.un.org/ (UNdata website) Scrolled down to “Gender” within “Popular statistical tables, country (area) and regional profiles” Copied CSV download link for “Ratio of girls to boys in primary, secondary and tertiary levels” Pasted the link below⇩ url_of_data &lt;- &quot;http://data.un.org/_Docs/SYB/CSV/SYB64_319_202110_Ratio%20of%20girls%20to%20boys%20in%20education.csv&quot; gender_education &lt;- read_csv(&quot;http://data.un.org/_Docs/SYB/CSV/SYB64_319_202110_Ratio%20of%20girls%20to%20boys%20in%20education.csv&quot;, skip = 1) ## New names: ## Rows: 2881 Columns: 7 ## ── Column specification ## ────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; chr (4): ...2, Series, Footnotes, Source dbl (3): Region/Country/Area, Year, Value ## ℹ Use `spec()` to retrieve the full column specification for this data. ℹ Specify the column types or set `show_col_types = ## FALSE` to quiet this message. ## • `` -&gt; `...2` gender_education ## # A tibble: 2,881 × 7 ## `Region/Country/Area` ...2 Year Series Value Footn…¹ Source ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Total, all countries… 1995 Ratio… 0.91 &lt;NA&gt; Unite… ## 2 1 Total, all countries… 2005 Ratio… 0.95 &lt;NA&gt; Unite… ## 3 1 Total, all countries… 2010 Ratio… 0.97 &lt;NA&gt; Unite… ## 4 1 Total, all countries… 2015 Ratio… 1 &lt;NA&gt; Unite… ## 5 1 Total, all countries… 2017 Ratio… 1 &lt;NA&gt; Unite… ## 6 1 Total, all countries… 2018 Ratio… 0.98 &lt;NA&gt; Unite… ## 7 1 Total, all countries… 2019 Ratio… 0.98 Estima… Unite… ## 8 1 Total, all countries… 1995 Ratio… 0.88 &lt;NA&gt; Unite… ## 9 1 Total, all countries… 2005 Ratio… 0.95 &lt;NA&gt; Unite… ## 10 1 Total, all countries… 2010 Ratio… 0.97 &lt;NA&gt; Unite… ## # … with 2,871 more rows, and abbreviated variable name ¹​Footnotes colnames(gender_education) ## [1] &quot;Region/Country/Area&quot; &quot;...2&quot; &quot;Year&quot; ## [4] &quot;Series&quot; &quot;Value&quot; &quot;Footnotes&quot; ## [7] &quot;Source&quot; It is a good try to use pivot_wider, and it is easy to see the data. However for visualization using tidyverse it is not necessary if you use filter and/or grouping. gender_education_tbl &lt;- gender_education %&gt;% select(num = &quot;Region/Country/Area&quot;, region = &quot;...2&quot;, year = &quot;Year&quot;, series = &quot;Series&quot;, value = &quot;Value&quot;) %&gt;% pivot_wider(names_from = series, values_from = value) gender_education_tbl ## # A tibble: 1,173 × 6 ## num region year Ratio of girls to…¹ Ratio…² Ratio…³ ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Total, all countries or areas 1995 0.91 0.88 0.95 ## 2 1 Total, all countries or areas 2005 0.95 0.95 1.05 ## 3 1 Total, all countries or areas 2010 0.97 0.97 1.07 ## 4 1 Total, all countries or areas 2015 1 0.99 1.1 ## 5 1 Total, all countries or areas 2017 1 0.99 1.12 ## 6 1 Total, all countries or areas 2018 0.98 0.99 1.12 ## 7 1 Total, all countries or areas 2019 0.98 0.99 1.13 ## 8 15 Northern Africa 1995 0.86 0.86 0.76 ## 9 15 Northern Africa 2005 0.93 0.99 0.96 ## 10 15 Northern Africa 2010 0.95 0.98 1.07 ## # … with 1,163 more rows, and abbreviated variable names ## # ¹​`Ratio of girls to boys in primary education`, ## # ²​`Ratio of girls to boys in secondary education`, ## # ³​`Ratio of girls to boys in tertiary education` 5.5.6.1.1 Visualization The student asks: I do not know how to visualize this data using ggplot… First select one region and make a scatter plot and see the data. gender_education %&gt;% filter(`Region/Country/Area` == 1) %&gt;% ggplot() + geom_point(aes(x = Year, y = Value)) 2. Since there are a few data in each year, use color and geom_line as well. gender_education %&gt;% filter(`Region/Country/Area` == 1) %&gt;% ggplot(aes(x = Year, y = Value, color = Series)) + geom_point() + geom_line() 3. Now we choose several countries and start analyzing what you want. Probably it is easier to see in separate charts. I wanted to introduce facet-grid. gender_education %&gt;% select(`Region/Country/Area`, region = `...2`, year = Year, value = Value, series = Series) %&gt;% mutate(year = as.integer(year)) %&gt;% filter(`Region/Country/Area` %in% c(1, 15, 202, 21, 419, 143, 30, 35, 34, 145, 150, 9)) %&gt;% ggplot(aes(x = year, y = value, color = region)) + geom_point() + geom_line() + facet_grid(cols = vars(series)) 5.5.6.2 Proportion of seats held by women in national parliament. url_of_df &lt;- &quot;http://data.un.org/_Docs/SYB/CSV/SYB64_317_202110_Seats%20held%20by%20women%20in%20Parliament.csv&quot; download.file(url = url_of_df, destfile = &quot;data/UN_WO.csv&quot;) df_UN_WO &lt;- read_csv(&quot;data/UN_WO.csv&quot;, skip = 1) ## New names: ## Rows: 1958 Columns: 9 ## ── Column specification ## ────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; chr (5): ...2, Series, Last Election Date, Footnotes, Source dbl (3): Region/Country/Area, Year, Value lgl ## (1): Last Election Date footnote ## ℹ Use `spec()` to retrieve the full column specification for this data. ℹ Specify the column types or set `show_col_types = ## FALSE` to quiet this message. ## • `` -&gt; `...2` df_UN_WO ## # A tibble: 1,958 × 9 ## `Region/Country/Area` ...2 Year Series Last …¹ Last …² Value Footn…³ Source ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Tota… 2000 Seats… &lt;NA&gt; NA 13.3 &lt;NA&gt; &quot;Inte… ## 2 1 Tota… 2005 Seats… &lt;NA&gt; NA 15.9 &lt;NA&gt; &quot;Inte… ## 3 1 Tota… 2010 Seats… &lt;NA&gt; NA 19 &lt;NA&gt; &quot;Inte… ## 4 1 Tota… 2015 Seats… &lt;NA&gt; NA 22.3 &lt;NA&gt; &quot;Inte… ## 5 1 Tota… 2017 Seats… &lt;NA&gt; NA 23.4 &lt;NA&gt; &quot;Inte… ## 6 1 Tota… 2018 Seats… &lt;NA&gt; NA 23.4 &lt;NA&gt; &quot;Inte… ## 7 1 Tota… 2019 Seats… &lt;NA&gt; NA 24.3 &lt;NA&gt; &quot;Inte… ## 8 1 Tota… 2020 Seats… &lt;NA&gt; NA 24.9 Data a… &quot;Inte… ## 9 1 Tota… 2021 Seats… &lt;NA&gt; NA 25.6 Data a… &quot;Inte… ## 10 15 Nort… 2000 Seats… &lt;NA&gt; NA 5.4 &lt;NA&gt; &quot;Inte… ## # … with 1,948 more rows, and abbreviated variable names ¹​`Last Election Date`, ## # ²​`Last Election Date footnote`, ³​Footnotes colnames(df_UN_WO) ## [1] &quot;Region/Country/Area&quot; &quot;...2&quot; ## [3] &quot;Year&quot; &quot;Series&quot; ## [5] &quot;Last Election Date&quot; &quot;Last Election Date footnote&quot; ## [7] &quot;Value&quot; &quot;Footnotes&quot; ## [9] &quot;Source&quot; UN_WO_tbl &lt;- df_UN_WO %&gt;% select(num = &quot;Region/Country/Area&quot;, region = &quot;...2&quot;, year = &quot;Year&quot;, series = &quot;Series&quot;, LED = &quot;Last Election Date&quot;, value = &quot;Value&quot;)%&gt;% pivot_wider (names_from = series, values_from = value) UN_WO_tbl ## # A tibble: 1,958 × 5 ## num region year LED Seats held by women in nati…¹ ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Total, all countries or areas 2000 &lt;NA&gt; 13.3 ## 2 1 Total, all countries or areas 2005 &lt;NA&gt; 15.9 ## 3 1 Total, all countries or areas 2010 &lt;NA&gt; 19 ## 4 1 Total, all countries or areas 2015 &lt;NA&gt; 22.3 ## 5 1 Total, all countries or areas 2017 &lt;NA&gt; 23.4 ## 6 1 Total, all countries or areas 2018 &lt;NA&gt; 23.4 ## 7 1 Total, all countries or areas 2019 &lt;NA&gt; 24.3 ## 8 1 Total, all countries or areas 2020 &lt;NA&gt; 24.9 ## 9 1 Total, all countries or areas 2021 &lt;NA&gt; 25.6 ## 10 15 Northern Africa 2000 &lt;NA&gt; 5.4 ## # … with 1,948 more rows, and abbreviated variable name ## # ¹​`Seats held by women in national parliament, as of February (%)` colnames(UN_WO_tbl) &lt;- c(&quot;num&quot;, &quot;region&quot;, &quot;year&quot;, &quot;LED&quot;, &quot;seats&quot;) UN_WO_tbl ## # A tibble: 1,958 × 5 ## num region year LED seats ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Total, all countries or areas 2000 &lt;NA&gt; 13.3 ## 2 1 Total, all countries or areas 2005 &lt;NA&gt; 15.9 ## 3 1 Total, all countries or areas 2010 &lt;NA&gt; 19 ## 4 1 Total, all countries or areas 2015 &lt;NA&gt; 22.3 ## 5 1 Total, all countries or areas 2017 &lt;NA&gt; 23.4 ## 6 1 Total, all countries or areas 2018 &lt;NA&gt; 23.4 ## 7 1 Total, all countries or areas 2019 &lt;NA&gt; 24.3 ## 8 1 Total, all countries or areas 2020 &lt;NA&gt; 24.9 ## 9 1 Total, all countries or areas 2021 &lt;NA&gt; 25.6 ## 10 15 Northern Africa 2000 &lt;NA&gt; 5.4 ## # … with 1,948 more rows UN_WO_tbl_short &lt;- UN_WO_tbl %&gt;% select(region, year, seats) UN_WO_tbl_short ## # A tibble: 1,958 × 3 ## region year seats ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Total, all countries or areas 2000 13.3 ## 2 Total, all countries or areas 2005 15.9 ## 3 Total, all countries or areas 2010 19 ## 4 Total, all countries or areas 2015 22.3 ## 5 Total, all countries or areas 2017 23.4 ## 6 Total, all countries or areas 2018 23.4 ## 7 Total, all countries or areas 2019 24.3 ## 8 Total, all countries or areas 2020 24.9 ## 9 Total, all countries or areas 2021 25.6 ## 10 Northern Africa 2000 5.4 ## # … with 1,948 more rows UN_WO_tbl_short$year &lt;- as.integer(UN_WO_tbl_short$year) UN_WO_tbl_short ## # A tibble: 1,958 × 3 ## region year seats ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Total, all countries or areas 2000 13.3 ## 2 Total, all countries or areas 2005 15.9 ## 3 Total, all countries or areas 2010 19 ## 4 Total, all countries or areas 2015 22.3 ## 5 Total, all countries or areas 2017 23.4 ## 6 Total, all countries or areas 2018 23.4 ## 7 Total, all countries or areas 2019 24.3 ## 8 Total, all countries or areas 2020 24.9 ## 9 Total, all countries or areas 2021 25.6 ## 10 Northern Africa 2000 5.4 ## # … with 1,948 more rows 5.5.6.2.1 Visualization UN_WO_tbl_short %&gt;% filter(year %in% c(2000, 2005, 2010, 2015, 2020)) %&gt;% mutate(year = as.factor(year)) %&gt;% ggplot() + geom_freqpoly(aes(x = seats, color = year)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. UN_WO_tbl_short %&gt;% filter(year %in% c(2000, 2005, 2010, 2015, 2020)) %&gt;% mutate(year = as.factor(year)) %&gt;% group_by(year) %&gt;% count(cut_width(seats, 5)) ## # A tibble: 57 × 3 ## # Groups: year [5] ## year `cut_width(seats, 5)` n ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 2000 [-2.5,2.5] 24 ## 2 2000 (2.5,7.5] 50 ## 3 2000 (7.5,12.5] 65 ## 4 2000 (12.5,17.5] 25 ## 5 2000 (17.5,22.5] 21 ## 6 2000 (22.5,27.5] 9 ## 7 2000 (27.5,32.5] 6 ## 8 2000 (32.5,37.5] 5 ## 9 2000 (42.5,47.5] 1 ## 10 2005 [-2.5,2.5] 17 ## # … with 47 more rows UN_WO_tbl_short ## # A tibble: 1,958 × 3 ## region year seats ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Total, all countries or areas 2000 13.3 ## 2 Total, all countries or areas 2005 15.9 ## 3 Total, all countries or areas 2010 19 ## 4 Total, all countries or areas 2015 22.3 ## 5 Total, all countries or areas 2017 23.4 ## 6 Total, all countries or areas 2018 23.4 ## 7 Total, all countries or areas 2019 24.3 ## 8 Total, all countries or areas 2020 24.9 ## 9 Total, all countries or areas 2021 25.6 ## 10 Northern Africa 2000 5.4 ## # … with 1,948 more rows wb_regions ## # A tibble: 45 × 2 ## region iso3c ## &lt;chr&gt; &lt;chr&gt; ## 1 Caribbean small states CSS ## 2 Central Europe and the Baltics CEB ## 3 Early-demographic dividend EAR ## 4 East Asia &amp; Pacific EAS ## 5 East Asia &amp; Pacific (excluding high income) EAP ## 6 East Asia &amp; Pacific (IDA &amp; IBRD) TEA ## 7 Euro area EMU ## 8 Europe &amp; Central Asia ECS ## 9 Europe &amp; Central Asia (excluding high income) ECA ## 10 Europe &amp; Central Asia (IDA &amp; IBRD) TEC ## # … with 35 more rows UN_WO_tbl_short %&gt;% filter(region %in% c(&quot;Total, all countries or areas&quot;, &quot;Northern Africa&quot;, &quot;Sub-Saharan Africa&quot;, &quot;Eastern Africa&quot;, &quot;Middle Africa&quot;, &quot;Southern Africa&quot;, &quot;Western Africa&quot;)) %&gt;% ggplot() + geom_line(aes(x= year, y= seats, color= region)) + labs(title = &quot;Seats of Women in Parliament&quot;) Good Luck! If you need help, please drop me a line. I would set up a Zoom Office Hour. HS "],["eda5.html", "Chapter 6 Exploratory Data Analysis (EDA) 5 6.1 Part I: Exploratory Data Analysis and Data Modeling 6.2 Part II: Practicum - Model Analysis Warnings 6.3 The Sixth Assignment (in Moodle)", " Chapter 6 Exploratory Data Analysis (EDA) 5 Course Contents 2021-12-08: Introduction: About the course - An introduction to open and public data, and data science 2021-12-15: Exploratory Data Analysis (EDA) 1 [lead by hs] - R Basics with RStudio and/or RStudio.cloud; R Script, swirl 2021-12-22: Exploratory Data Analysis (EDA) 2 [lead by hs] - R Markdown; Introduction to tidyverse; RStudio Primers 2022-01-12: Exploratory Data Analysis (EDA) 3 [lead by hs] - Introduction to tidyverse; Public Data, WDI, etc 2022-01-19: Exploratory Data Analysis (EDA) 4 [lead by hs] - Introduction to tidyverse; WDI, UN, WHO, etc 2022-01-26: Exploratory Data Analysis (EDA) 5 [lead by hs] - Introduction to tidyverse; UN, WHO,OECD, US gov, etc 2022-02-02: Inference Statistics 1 2022-02-09: Inference Statistics 2 2022-02-16: Inference Statistics 3 2022-02-23: Project Presentation Data Modeling and EDA Modeling using tidyverse EDA (A diagram from R4DS by H.W. and G.G.) EDA from r4ds Contents of EDA5 Data Science is empirical!? empirical: based on, concerned with, or verifiable by observation or experience rather than theory or pure logic Part I: Data Modeling Introduction to Modeling and EDA, in Moodle Exploration in Visualization Modeling Scientific: Why? Prediction! - Do you support this? Evidence based! - What does it mean? What is regression, and why regression?  Linear Regression, ggplot2 Predictions and Residues Part II: Examples using Public Data and Roundup 6.1 Part I: Exploratory Data Analysis and Data Modeling 6.1.1 Exploration in Visualization, I – r4ds:EDA EDA is an iterative cycle. You: Generate questions about your data. Search for answers by visualising, transforming, and modelling your data. Use what you learn to refine your questions and/or generate new questions. “There are no routine statistical questions, only questionable statistical routines.” — Sir David Cox (1924-2022) “Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.” — John Tukey (1915-2000) 6.1.2 Exploration in Visualization, II – r4ds:EDA The very basic questions: What type of variation occurs within my variables? What type of covariation occurs between my variables? Typical values Which values are the most common? Why? Which values are rare? Why? Does that match your expectations? Can you see any unusual patterns? What might explain them? 6.1.3 Exploration in Visualization, III – r4ds:EDA Clusters and Groups How are the observations within each cluster similar to each other? How are the observations in separate clusters different from each other? How can you explain or describe the clusters? Why might the appearance of clusters be misleading? Outliers and Unusual Values Sometimes outliers are data entry errors; other times outliers suggest important new science. 6.1.4 Exploration in Visualization, IV – r4ds:EDA Patterns and models Could this pattern be due to coincidence (i.e. random chance)? How can you describe the relationship implied by the pattern? How strong is the relationship implied by the pattern? What other variables might affect the relationship? Does the relationship change if you look at individual subgroups of the data? 6.1.5 EDA and Data Modeling: Simple Summary of Data Goal: A simple low-dimensional summary of a dataset. Ideally, the model will capture true “signals” (i.e. patterns generated by the phenomenon of interest), and ignore “noise” (i.e. random variation that you’re not interested in). “predictive” models: supervised “data discovery” models: unsupervised 6.1.5.1 EDA (A diagram from R4DS by H.W. and G.G.) EDA from r4ds 6.1.6 Hypothesis generation vs. hypothesis confirmation Each observation can either be used for exploration or confirmation, not both. You can use an observation as many times as you like for exploration, but you can only use it once for confirmation. As soon as you use an observation twice, you’ve switched from confirmation to exploration. If you are serious about doing an confirmatory analysis, one approach is to split your data into three pieces before you begin the analysis: 60% of your data goes into a training (or exploration) set. You’re allowed to do anything you like with this data: visualise it and fit tons of models to it. 20% goes into a query set. You can use this data to compare models or visualisations by hand, but you’re not allowed to use it as part of an automated process. 20% is held back for a test set. You can only use this data ONCE, to test your final model. 6.1.7 R4DS: Model Basics There are two parts to a model: First, you define a family of models that express a precise, but generic, pattern that you want to capture. For example, the pattern might be a straight line, or a quadratic curve. You will express the model family as an equation like \\(y = a_1 * x + a_2\\) or \\(y = a_1 * x ^ {a_2}\\). Here, \\(x\\) and \\(y\\) are known variables from your data, and \\(a_1\\) and \\(a_2\\) are parameters that can vary to capture different patterns. Next, you generate a fitted model by finding the model from the family that is the closest to your data. This takes the generic model family and makes it specific, like \\(y = 3 * x + 7\\) or \\(y = 9 * x ^ 2\\). It’s important to understand that a fitted model is just the closest model from a family of models. That implies that you have the “best” model (according to some criteria); it doesn’t imply that you have a good model and it certainly doesn’t imply that the model is “true”. 6.1.8 All models are wrong, but some are useful. - George E.P. Box (1919-2013) Now it would be very remarkable if any system existing in the real world could be exactly represented by any simple model. However, cunningly chosen parsimonious models often do provide remarkably useful approximations. For example, the law PV = RT relating pressure P, volume V and temperature T of an “ideal” gas via a constant R is not exactly true for any real gas, but it frequently provides a useful approximation and furthermore its structure is informative since it springs from a physical view of the behavior of gas molecules. For such a model there is no need to ask the question “Is the model true?”. If “truth” is to be the “whole truth” the answer must be “No”. The only question of interest is “Is the model illuminating and useful?”. The goal of a model is not to uncover truth, but to discover a simple approximation that is still useful. 6.1.9 Regression Analysis (wikipedia) In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (‘outcome variable’) and one or more independent variables (‘predictors’, ‘covariates’, or ‘features’). The most common form of regression analysis is linear regression, in which a researcher finds the line (or a more complex linear combination) that most closely fits the data according to a specific mathematical criterion. Two purposes of regression analysis: First, regression analysis is widely used for prediction and forecasting. Second, regression analysis can be used to infer causal relationships between the independent and dependent variables. Importantly, regressions by themselves only reveal relationships between a dependent variable and a collection of independent variables in a fixed dataset. To use regressions for prediction or to infer causal relationships, respectively, a researcher must carefully justify why existing relationships have predictive power for a new context or why a relationship between two variables has a causal interpretation. 6.1.10 History of Regression Analysis (wikipedia) The earliest form of regression was the method of least squares, which was published by Legendre in 1805, and by Gauss in 1809. Legendre and Gauss both applied the method to the problem of determining, from astronomical observations, the orbits of bodies about the Sun (mostly comets, but also later the then newly discovered minor planets). Gauss published a further development of the theory of least squares in 1821, including a version of the Gauss–Markov theorem. The term “regression” was coined by Francis Galton in the nineteenth century to describe a biological phenomenon. The phenomenon was that the heights of descendants of tall ancestors tend to regress down towards a normal average (a phenomenon also known as regression toward the mean). For Galton, regression had only this biological meaning, but his work was later extended by Udny Yule and Karl Pearson to a more general statistical context. 6.1.11 The First Example plot(cars) plot(cars) # cars: Speed and Stopping Distances of Cars abline(lm(cars$dist~cars$speed)) summary(lm(cars$dist~cars$speed)) ## ## Call: ## lm(formula = cars$dist ~ cars$speed) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.069 -9.525 -2.272 9.215 43.201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.5791 6.7584 -2.601 0.0123 * ## cars$speed 3.9324 0.4155 9.464 1.49e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.38 on 48 degrees of freedom ## Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 ## F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 6.1.12 Model Analysis with tidyverse: modelr Tidymodels: https://www.tidymodels.org modelr: https://modelr.tidyverse.org Compute model quality for a given dataset library(tidyverse) library(modelr) t_cars &lt;- as_tibble(cars) (mod &lt;- lm(dist ~ speed, data = t_cars)) ## ## Call: ## lm(formula = dist ~ speed, data = t_cars) ## ## Coefficients: ## (Intercept) speed ## -17.579 3.932 ## ## Call: ## lm(formula = dist ~ speed, data = t_cars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.069 -9.525 -2.272 9.215 43.201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.5791 6.7584 -2.601 0.0123 * ## speed 3.9324 0.4155 9.464 1.49e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.38 on 48 degrees of freedom ## Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 ## F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 ggplot(t_cars, aes(speed, dist)) + geom_point() + geom_abline(slope = coef(mod)[[2]], intercept = coef(mod)[[1]]) ggplot(t_cars, aes(speed, dist)) + geom_jitter() + geom_smooth(method = &quot;lm&quot;, se = FALSE) 6.1.13 Predictions \\(\\hat{y} = a_1 + a_2x\\) and Residuals \\(y - \\hat{y}\\) (mod_table &lt;- t_cars %&gt;% add_predictions(mod) %&gt;% add_residuals(mod, var = &quot;resid&quot;)) ## # A tibble: 50 × 4 ## speed dist pred resid ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 2 -1.85 3.85 ## 2 4 10 -1.85 11.8 ## 3 7 4 9.95 -5.95 ## 4 7 22 9.95 12.1 ## 5 8 16 13.9 2.12 ## 6 9 10 17.8 -7.81 ## 7 10 18 21.7 -3.74 ## 8 10 26 21.7 4.26 ## 9 10 34 21.7 12.3 ## 10 11 17 25.7 -8.68 ## # … with 40 more rows mod_table %&gt;% ggplot() + geom_jitter(aes(speed, resid)) + geom_hline(yintercept = 0) ### geom_jitter() t_cars %&gt;% group_by(speed, dist) %&gt;% summarize(count = n()) %&gt;% arrange(desc(count)) ## `summarise()` has grouped output by &#39;speed&#39;. You can override using the ## `.groups` argument. ## # A tibble: 49 × 3 ## # Groups: speed [19] ## speed dist count ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 13 34 2 ## 2 4 2 1 ## 3 4 10 1 ## 4 7 4 1 ## 5 7 22 1 ## 6 8 16 1 ## 7 9 10 1 ## 8 10 18 1 ## 9 10 26 1 ## 10 10 34 1 ## # … with 39 more rows 6.1.14 iris (t_iris &lt;- as_tibble(datasets::iris)) ## # A tibble: 150 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # … with 140 more rows 6.1.15 Linear Model: Sepal.W ~ Sepal.L colnames(t_iris) &lt;- c(&quot;Sepal.L&quot;, &quot;Sepal.W&quot;, &quot;Petal.L&quot;, &quot;Petal.W&quot;, &quot;Species&quot;) lm(Sepal.W ~ Sepal.L, data = t_iris) ## ## Call: ## lm(formula = Sepal.W ~ Sepal.L, data = t_iris) ## ## Coefficients: ## (Intercept) Sepal.L ## 3.41895 -0.06188 6.1.16 Linear Model: Petal.W ~ Petal.L lm(Petal.W ~ Petal.L, data = t_iris) ## ## Call: ## lm(formula = Petal.W ~ Petal.L, data = t_iris) ## ## Coefficients: ## (Intercept) Petal.L ## -0.3631 0.4158 6.1.17 Correlation cor(t_iris[1:4]) ## Sepal.L Sepal.W Petal.L Petal.W ## Sepal.L 1.0000000 -0.1175698 0.8717538 0.8179411 ## Sepal.W -0.1175698 1.0000000 -0.4284401 -0.3661259 ## Petal.L 0.8717538 -0.4284401 1.0000000 0.9628654 ## Petal.W 0.8179411 -0.3661259 0.9628654 1.0000000 6.1.18 filter(Species == \"setosa\") t_iris %&gt;% filter(Species == &quot;setosa&quot;) %&gt;% ggplot(aes(x = Petal.L, y = Petal.W)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) 6.1.19 aes(Sepal.L, Sepal.W, color = Species) 6.1.20 Linear Regression Quick Reference DataCamp: https://www.datacamp.com/community/tutorials/linear-regression-R What is a linear regression? Creating a linear regression in R. Learn the concepts of coefficients and residuals. How to test if your linear model has a good fit? Detecting influential points. DataCamp Top: https://www.datacamp.com - to be introduced r-statistics.co by Selva Prabhakaran: http://r-statistics.co/Linear-Regression.html The aim of linear regression is to model a continuous variable Y as a mathematical function of one or more X variable(s), so that we can use this regression model to predict the Y when only the X is known. This mathematical equation can be generalized as follows: \\(Y = \\beta_1 + \\beta_2 X + \\varepsilon\\) r-statistics.co Top: http://r-statistics.co An educational resource for those seeking knowledge related to machine learning and statistical computing in R. 6.2 Part II: Practicum - Model Analysis Warnings 6.2.1 Linear Regression: aes(Sepal.L, Sepal.W, color = Species) 6.2.2 aes(Sepal.L, Sepal.W, color = Species) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. 6.2.3 Correlation and Covariance cor(t_iris[1:4]) ## Sepal.L Sepal.W Petal.L Petal.W ## Sepal.L 1.0000000 -0.1175698 0.8717538 0.8179411 ## Sepal.W -0.1175698 1.0000000 -0.4284401 -0.3661259 ## Petal.L 0.8717538 -0.4284401 1.0000000 0.9628654 ## Petal.W 0.8179411 -0.3661259 0.9628654 1.0000000 cov(t_iris[1:4]) ## Sepal.L Sepal.W Petal.L Petal.W ## Sepal.L 0.6856935 -0.0424340 1.2743154 0.5162707 ## Sepal.W -0.0424340 0.1899794 -0.3296564 -0.1216394 ## Petal.L 1.2743154 -0.3296564 3.1162779 1.2956094 ## Petal.W 0.5162707 -0.1216394 1.2956094 0.5810063 6.2.4 Useful Mathematical Formula Let \\(x = c(x_1, x_2, \\ldots, x_n)\\) be the independent variable, i.e., Sepal.L Let \\(y = c(y_1, y_2, \\ldots, y_n)\\) be the dependent variable, i.e., Sepal.W Let \\(\\mbox{pred} = c(\\hat{y}_1, \\hat{y}_2, \\ldots, \\hat{y}_n)\\) be the predicted values by linear regression. \\[ \\begin{aligned} \\mbox{slope of the regression line} &amp;= \\frac{cov(x,y)}{var(x)} = \\frac{cor(x,y)\\sqrt{var(y)}}{\\sqrt{var(x)}}\\\\ \\mbox{total sum of squares} &amp;= SS_{tot} = sum((y-mean(y))^2)\\\\ \\mbox{residual sum of squares} &amp;= SS_{res} = sum((y-\\mbox{pred})^2)\\\\ \\mbox{R squared} = R^2 &amp; = 1 - \\frac{SS_{res}}{SS_{tot}} = cor(x,y)^2 \\end{aligned} \\] 6.2.5 p-value 6.2.6 t-statistic 6.2.7 Mathematical Formulas, I Let \\(x = c(x_1, x_2, \\ldots, x_n)\\) be the independent variable, e.g., the height of fathers Let \\(y = c(y_1, y_2, \\ldots, y_n)\\) be the dependent variable, i.e., height of children Let \\(\\mbox{pred} = c(\\hat{y}_1, \\hat{y}_2, \\ldots, \\hat{y}_n)\\) be the predicted values by linear regression. 6.2.8 Mathematical Formulas, II \\[ \\begin{aligned} \\mbox{unbiased covariance: } cov(x,y) &amp;= \\frac{\\sum_{i=1}^n(x_i - mean(x))(y_i - mean(y))}{n-1}\\\\ \\mbox{unviased variance: } var(x) &amp;= cov(x,x) \\\\ \\mbox{correlation: } cor(x.y) &amp;= \\frac{cov(x,y)}{\\sqrt{var(x)var(y)}}\\\\ \\mbox{slope of the regression line} &amp;= \\frac{cov(x,y)}{var(x)} = \\frac{cor(x,y)\\sqrt{var(y)}}{\\sqrt{var(x)}}\\\\ \\mbox{total sum of squares} &amp;= SS_{tot} = \\sum_{i=1}^n((y_i-mean(y))^2)\\\\ \\mbox{residual sum of squares} &amp;= SS_{res} = \\sum_{i=1}^n((y_i-\\hat{y}_i)^2)\\\\ \\mbox{R squared} = R^2 &amp; = 1 - \\frac{SS_{res}}{SS_{tot}} = cor(x,y)^2 \\end{aligned} \\] 6.2.9 Regression Analyais: Summary R-Squared: Higher the better (&gt; 0.70) How well the prediction fit to the data. \\(0 \\leq R2 \\leq 1\\). For linear regression between two variables x and y, R-squared is the square of cor(x,y). R-Squared can be measured on any prediction model. t-statistic: The absolute value should be greater 1.96, and for p-value be less than 0.05 The model and the choice of variable(s) are suitable or not. p-value appears in Hypothesis testing (A/B test, sensitivity - specificity, etc.) 6.2.10 The ASA Statement on p-Values: Context, Process, and Purpose P-values can indicate how incompatible the data are with a specified statistical model. P-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone. Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold. Proper inference requires full reporting and transparency. A p-value, or statistical significance, does not measure the size of an effect or the importance of a result. By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis. AMERICAN STATISTICAL ASSOCIATION RELEASES STATEMENT ON STATISTICAL SIGNIFICANCE AND P-VALUES 6.2.11 Public Data: UN Data and US Census UN Data: https://data.un.org Uited States Census Bureau: https://www.census.gov 6.2.11.1 Importing Data Get the URL (uniform resource locator) - copy the link url_of_data &lt;- \"https://data.un.org/--long url--.csv\" Download the file into the destfile in data folder: download.file(url = url_of_data, destfile = \"data/un_pop.csv\") Read the file: df_un_pop &lt;- read_csv(\"data/un_pop.csv\") 6.2.12 An Example: US Census Bureau International Data Base (IDB) idbzip.zip is a large archive with three files; idb5yr.all, idbsingleyear.all and Readme.txt. The data of idbsingleyear.all is separated by | and contains over 3 million rows. Execute the following code only once. Then commnet these lines out with #. 6.2.12.1 Importance of Population Data of Each Country Population: Special meaning in statistics In statistics, a population is a set of similar items or events which is of interest for some question or experiment. Basis of analysis Public Opinion Poll 6.2.13 OECD: About Who we are The Organisation for Economic Co-operation and Development (OECD) is an international organisation that works to build better policies for better lives. Our goal is to shape policies that foster prosperity, equality, opportunity and well-being for all. 6.2.13.1 Data http://www.oecd.org Data: https://data.oecd.org OECD.Stat: https://stats.oecd.org OECD.Stat Web Browser User Guide, May 2013 Search Box &gt; Filter by type, Filter by topic, Sort results Browse by Topic: Agriculture, Development, Economy, Education, Energy, Finance, Govetnment, Health, Innovation and Technology, Job, Society Browse by Country: choose from 43 countries Catalogue of OECD databases Featured Charts, Latest News, Statistical Resources 6.2.14 Learning Resources, V R for Data Science, Part IV Model 6.2.14.1 RStudio Primers: See References in Moodle at the bottom The Basics – r4ds: Explore, I Work with Data – r4ds: Wrangle, I Visualize Data – r4ds: Explore, II Exploratory Data Analysis, Bar Charts, Histograms Boxplots and Counts, Scatterplots, Line Plots Overplotting and Big Data, Customize Your Plots Tidy Your Data – r4ds: Wrangle, II Reshape Data, Separate and Unite Columns, Join Data Sets 6.2.15 Learning Resources – Summary Books Textbook: R for Data Science Books Online: BOOKDOWN Exercise Tools {swirl} Learn R in R R Studio Cloud Primers Others R Studio Education Cheat Sheet Data Science is an empirical study! Develop your skills and knowledge by experiences. Data Science is for Everyone! 6.3 The Sixth Assignment (in Moodle) Choose a public data. Clearly state how you obtained the data. Even if you are able to give the URL to download the data, explain the steps you reached and obtained the data. Create an R Notebook (file name.nb.html, e.g. a6_12345.nb.html) of an EDA containing: title, date, and author, i.e., Your Name your motivation and/or objectives to analyze the data, and your questions an explanation of the data and the variables chunks containing the following: library(tidyverse, modelr), visualization of the data with ggplot() both a linear model (summary(lm(y~x))) and a chart created by ggplot() with fitted line. explanation of the values given by the model and describe a prediction or a finding using the model. your findings and/or questions Submit your R Notebook file to Moodle (The Sixth Assignment) by 2021-02-01 23:59:00 6.3.1 Setup and YAML This R Notebook document uses the following YAML: --- title: &quot;Responses to the Sixth Assignment on Modeling and EDA&quot; author: &quot;p000117x Hiroshi Suzuki&quot; date: &#39;2022-02-07&#39; output: html_notebook: number_sections: yes toc: yes toc_float: yes --- 6.3.2 Packages to load https://tidyverse.tidyverse.org/ library(tidyverse) will load the core tidyverse packages: ggplot2, for data visualisation. dplyr, for data manipulation. tidyr, for data tidying. readr, for data import. purrr, for functional programming. tibble, for tibbles, a modern re-imagining of data frames. stringr, for strings. forcats, for factors. Hence the following code chunk loads every core package above, and modelr. Although modelr is a tidyverse package but it is not a core tidyverse package, you need to add library(modelr). library(tidyverse) library(modelr) We will use WDI package for examples. library(WDI) 6.3.3 Modeling and EDA Summary Reference: * r4ds:EDA * R4DS: Model Basics Importing Glimpsing Transforming Visualising Modeling 6.3.3.1 Exploration in Visualization EDA is an iterative cycle. You: Generate questions about your data. Search for answers by visualising, transforming, and modelling your data. - Methods in Data Science Use what you learn to refine your questions and/or generate new questions. “There are no routine statistical questions, only questionable statistical routines.” — Sir David Cox (1924.7.15-2022.1.18) “Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.” — John Tukey (1915.6.16-2000.7.26) Exploratory Data Analysis cannot be done by statistical routines or a list of routine questions. 6.3.3.2 Quotation Marks Reference: https://stat.ethz.ch/R-manual/R-devel/library/base/html/Quotes.html Three types of quotes are part of the syntax of R: single and double quotation marks and the backtick (or back quote, ‘⁠`⁠’). Backslash is used to start an escape sequence inside character constants. Single and double quotes delimit character constants. They can be used interchangeably but double quotes are preferred (and character constants are printed using double quotes), so single quotes are normally only used to delimit character constants containing double quotes. ‘single quotes can be used more-or-less interchangeably’ “with double quotes to create character vectors” identical(&#39;&quot;It\\&#39;s alive!&quot;, he screamed.&#39;, &quot;\\&quot;It&#39;s alive!\\&quot;, he screamed.&quot;) # same ## [1] TRUE Backticks are used for non-standard variable names. (See make.names and ?Reserved for what counts as non-standard.) When you have a space, or other special characters, in the variable name, use backticks. `x y` &lt;- 1:5 # variable `x y` # variable ## [1] 1 2 3 4 5 &quot;x y&quot; &lt;- 1:5 # variable `x y` # variable ## [1] 1 2 3 4 5 &quot;x y&quot; # character vector ## [1] &quot;x y&quot; d &lt;- data.frame(`1st column` = rchisq(5, 2), check.names = FALSE) d$`1st column` ## [1] 1.51940165 0.91352143 0.17061561 0.01771576 1.31073623 6.3.4 Examples and Comments 6.3.4.1 Preparations 6.3.4.1.1 Extra Packages In the following we use modelsummary which Prof Kaizoji introduced at his Week 7 lecture. You need to install it if you have not by the followiin in you console or from Tools menu on top. See: https://CRAN.R-project.org/package=modelsummary install.packages(&quot;modelsummary&quot;) library(modelsummary) # Tool for writting tables of the regression results 6.3.5 CLASS.xlsx See Responses to Assignment Five. 6.3.5.1 Importing Excel Files CLASS.xlsx: - copy the following link The current classification by income in XLS format readxl: https://readxl.tidyverse.org Help: read_excel, read_xls, read_xlsx url_class &lt;- &quot;https://databankfiles.worldbank.org/data/download/site-content/CLASS.xlsx&quot; download.file(url = url_class, destfile = &quot;data/CLASS.xlsx&quot;) 6.3.5.1.1 Countries Let us look at the first sheet. The column names are in the 5th row. The country data starts from the 7th row. Zimbabue is at the last row. library(readxl) wb_countries_tmp &lt;- read_excel(&quot;data/CLASS.xlsx&quot;, sheet = 1, skip = 0, n_max =219) wb_countries &lt;- wb_countries_tmp %&gt;% select(country = Economy, iso3c = Code, region = Region, income = `Income group`, lending = &quot;Lending category&quot;, other = &quot;Other (EMU or HIPC)&quot;) wb_countries ## # A tibble: 218 × 6 ## country iso3c region income lending other ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Aruba ABW Latin America &amp; Caribbean High inc… &lt;NA&gt; &lt;NA&gt; ## 2 Afghanistan AFG South Asia Low inco… IDA HIPC ## 3 Angola AGO Sub-Saharan Africa Lower mi… IBRD &lt;NA&gt; ## 4 Albania ALB Europe &amp; Central Asia Upper mi… IBRD &lt;NA&gt; ## 5 Andorra AND Europe &amp; Central Asia High inc… &lt;NA&gt; &lt;NA&gt; ## 6 United Arab Emirates ARE Middle East &amp; North Africa High inc… &lt;NA&gt; &lt;NA&gt; ## 7 Argentina ARG Latin America &amp; Caribbean Upper mi… IBRD &lt;NA&gt; ## 8 Armenia ARM Europe &amp; Central Asia Upper mi… IBRD &lt;NA&gt; ## 9 American Samoa ASM East Asia &amp; Pacific Upper mi… &lt;NA&gt; &lt;NA&gt; ## 10 Antigua and Barbuda ATG Latin America &amp; Caribbean High inc… IBRD &lt;NA&gt; ## # … with 208 more rows 6.3.5.1.2 Regions readxl: https://readxl.tidyverse.org Help: read_excel, read_xls, read_xlsx Regions start from the 221th row. Regions end at the 266th row. wb_regions_tmp &lt;- read_excel(&quot;data/CLASS.xlsx&quot;, sheet = 1, skip = 0, n_max =266) %&gt;% slice(-(1:220)) wb_regions &lt;- wb_regions_tmp %&gt;% select(region = Economy, iso3c = Code) %&gt;% drop_na() wb_regions ## # A tibble: 45 × 2 ## region iso3c ## &lt;chr&gt; &lt;chr&gt; ## 1 Caribbean small states CSS ## 2 Central Europe and the Baltics CEB ## 3 Early-demographic dividend EAR ## 4 East Asia &amp; Pacific EAS ## 5 East Asia &amp; Pacific (excluding high income) EAP ## 6 East Asia &amp; Pacific (IDA &amp; IBRD) TEA ## 7 Euro area EMU ## 8 Europe &amp; Central Asia ECS ## 9 Europe &amp; Central Asia (excluding high income) ECA ## 10 Europe &amp; Central Asia (IDA &amp; IBRD) TEC ## # … with 35 more rows Let us look at the second sheet. wb_groups_tmp &lt;- read_excel(&quot;data/CLASS.xlsx&quot;, sheet = &quot;Groups&quot;) # sheet = 3 wb_groups &lt;- wb_groups_tmp %&gt;% select(gcode = GroupCode, group = GroupName, iso3c = CountryCode, country = CountryName) 6.3.6 Population and GDP per Capita 6.3.6.1 Importing WDIsearch(string = &quot;SP.POP.TOTL&quot;, field = &quot;indicator&quot;) ## indicator name ## 17674 SP.POP.TOTL Population, total ## 17675 SP.POP.TOTL.FE.IN Population, female ## 17676 SP.POP.TOTL.FE.ZS Population, female (% of total population) ## 17677 SP.POP.TOTL.ICP SP.POP.TOTL.ICP:Population ## 17678 SP.POP.TOTL.ICP.ZS SP.POP.TOTL.ICP.ZS:Population shares (World=100) ## 17679 SP.POP.TOTL.MA.IN Population, male ## 17680 SP.POP.TOTL.MA.ZS Population, male (% of total population) ## 17681 SP.POP.TOTL.ZS Population (% of total) df_pop_gdpcap &lt;- WDI( country = &quot;all&quot;, indicator = c(pop =&quot;SP.POP.TOTL&quot;, gdpcap = &quot;NY.GDP.PCAP.KD&quot;), start = 1961, end = 2020 ) head(df_pop_gdpcap) ## country iso2c iso3c year pop gdpcap ## 1 Afghanistan AF AFG 1961 9169406 NA ## 2 Afghanistan AF AFG 1962 9351442 NA ## 3 Afghanistan AF AFG 1963 9543200 NA ## 4 Afghanistan AF AFG 1964 9744772 NA ## 5 Afghanistan AF AFG 1965 9956318 NA ## 6 Afghanistan AF AFG 1966 10174840 NA 6.3.6.2 Visializing and Modeling df_pop_gdpcap %&gt;% filter(country == &quot;World&quot;) %&gt;% ggplot(aes(x = pop, y = gdpcap)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; df_pop_gdpcap %&gt;% filter(country == &quot;World&quot;) -&gt; df_pk df_pk %&gt;% lm(gdpcap ~ pop, .) %&gt;% summary() ## ## Call: ## lm(formula = gdpcap ~ pop, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -516.54 -172.78 45.57 223.68 606.04 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6.184e+02 1.447e+02 -4.272 7.28e-05 *** ## pop 1.436e-06 2.622e-08 54.759 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 284.8 on 58 degrees of freedom ## Multiple R-squared: 0.981, Adjusted R-squared: 0.9807 ## F-statistic: 2999 on 1 and 58 DF, p-value: &lt; 2.2e-16 6.3.7 Internet 6.3.7.1 Importing df_un_internet &lt;- read_csv(&quot;https://data.un.org/_Docs/SYB/CSV/SYB64_314_202110_Internet%20Usage.csv&quot;, skip = 1) ## New names: ## Rows: 1357 Columns: 7 ## ── Column specification ## ────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; chr (4): ...2, Series, Footnotes, Source dbl (3): Region/Country/Area, Year, Value ## ℹ Use `spec()` to retrieve the full column specification for this data. ℹ Specify the column types or set `show_col_types = ## FALSE` to quiet this message. ## • `` -&gt; `...2` 6.3.7.2 Glimpsing We first overview the structure of the data briefly head(df_un_internet) ## # A tibble: 6 × 7 ## `Region/Country/Area` ...2 Year Series Value Footn…¹ Source ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Total, all countries … 2000 Perce… 6.5 &lt;NA&gt; Inter… ## 2 1 Total, all countries … 2005 Perce… 16.8 &lt;NA&gt; Inter… ## 3 1 Total, all countries … 2010 Perce… 29.3 &lt;NA&gt; Inter… ## 4 1 Total, all countries … 2015 Perce… 41.1 &lt;NA&gt; Inter… ## 5 1 Total, all countries … 2017 Perce… 46.3 &lt;NA&gt; Inter… ## 6 1 Total, all countries … 2018 Perce… 49 &lt;NA&gt; Inter… ## # … with abbreviated variable name ¹​Footnotes glimpse(df_un_internet) ## Rows: 1,357 ## Columns: 7 ## $ `Region/Country/Area` &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 15, 15, 15, 15, 15, 15, 15,… ## $ ...2 &lt;chr&gt; &quot;Total, all countries or areas&quot;, &quot;Total, all cou… ## $ Year &lt;dbl&gt; 2000, 2005, 2010, 2015, 2017, 2018, 2019, 2000, … ## $ Series &lt;chr&gt; &quot;Percentage of individuals using the internet&quot;, … ## $ Value &lt;dbl&gt; 6.5, 16.8, 29.3, 41.1, 46.3, 49.0, 51.4, 0.6, 9.… ## $ Footnotes &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ Source &lt;chr&gt; &quot;International Telecommunication Union (ITU), Ge… Note that the first column name is surrounded by backticks. df_un_internet %&gt;% summary() ## Region/Country/Area ...2 Year Series ## Min. : 1 Length:1357 Min. :2000 Length:1357 ## 1st Qu.:156 Class :character 1st Qu.:2005 Class :character ## Median :388 Mode :character Median :2010 Mode :character ## Mean :393 Mean :2011 ## 3rd Qu.:620 3rd Qu.:2017 ## Max. :894 Max. :2019 ## Value Footnotes Source ## Min. : 0.00 Length:1357 Length:1357 ## 1st Qu.: 8.00 Class :character Class :character ## Median :34.10 Mode :character Mode :character ## Mean :39.01 ## 3rd Qu.:68.10 ## Max. :99.70 df_un_internet %&gt;% distinct(Source) ## # A tibble: 1 × 1 ## Source ## &lt;chr&gt; ## 1 International Telecommunication Union (ITU), Geneva, the ITU database, last a… All sources are ‘International Telecommunication Union (ITU), Geneva, the ITU database, last accessed January 2021’. df_un_internet %&gt;% distinct(Footnotes) ## # A tibble: 71 × 1 ## Footnotes ## &lt;chr&gt; ## 1 &lt;NA&gt; ## 2 Estimate. ## 3 Population aged 15 years and over. ## 4 Population aged 15 years and over.;Accessed the internet for personal use in… ## 5 Population aged 14 years and over. ## 6 Population aged 16 to 74 years. ## 7 Population aged 16 to 74 years.;Users in the last 3 months. ## 8 Population aged 7 years and over. ## 9 Population aged 16 years and over. ## 10 Users in the last 12 months. ## # … with 61 more rows df_un_internet %&gt;% distinct(Series) ## # A tibble: 1 × 1 ## Series ## &lt;chr&gt; ## 1 Percentage of individuals using the internet There is only one series. ‘Percentage of individuals using the internet’ can be for the title. df_un_internet %&gt;% group_by(Footnotes) %&gt;% summarize(number_of_regions = n_distinct(`...2`)) %&gt;% arrange(desc(number_of_regions)) ## # A tibble: 71 × 2 ## Footnotes number_of_regions ## &lt;chr&gt; &lt;int&gt; ## 1 &lt;NA&gt; 236 ## 2 Estimate. 151 ## 3 Population aged 16 to 74 years. 31 ## 4 Users in the last 3 months. 16 ## 5 Population aged 5 years and over. 9 ## 6 Population aged 6 years and over. 9 ## 7 Estimate.;Population aged 18 years and over. 8 ## 8 Population aged 15 years and over. 6 ## 9 Population aged 16 to 74 years.;Users in the last 3 months. 6 ## 10 Population aged 10 years and over. 5 ## # … with 61 more rows We need to see Footnotes carefully but in the beginning we do not examine. df_un_internet %&gt;% distinct(Year) ## # A tibble: 7 × 1 ## Year ## &lt;dbl&gt; ## 1 2000 ## 2 2005 ## 3 2010 ## 4 2015 ## 5 2017 ## 6 2018 ## 7 2019 df_un_internet %&gt;% group_by(Year) %&gt;% summarize(number_of_regions = n_distinct(...2)) %&gt;% arrange(desc(number_of_regions)) ## # A tibble: 7 × 2 ## Year number_of_regions ## &lt;dbl&gt; &lt;int&gt; ## 1 2010 233 ## 2 2000 232 ## 3 2005 232 ## 4 2015 227 ## 5 2017 222 ## 6 2018 110 ## 7 2019 101 df_un_internet %&gt;% group_by(...2) %&gt;% summarize(number_of_years = n_distinct(Year)) %&gt;% arrange(number_of_years) ## # A tibble: 246 × 2 ## ...2 number_of_years ## &lt;chr&gt; &lt;int&gt; ## 1 &quot;Cura\\xe7ao&quot; 1 ## 2 &quot;Mayotte&quot; 1 ## 3 &quot;Montserrat&quot; 1 ## 4 &quot;Nauru&quot; 1 ## 5 &quot;Ascension&quot; 2 ## 6 &quot;British Virgin Islands&quot; 2 ## 7 &quot;French Guiana&quot; 2 ## 8 &quot;Guadeloupe&quot; 2 ## 9 &quot;Guernsey&quot; 2 ## 10 &quot;Jersey&quot; 2 ## # … with 236 more rows df_un_internet %&gt;% group_by(`Region/Country/Area`) %&gt;% summarize(number_of_years = n_distinct(Year)) %&gt;% arrange(number_of_years) ## # A tibble: 246 × 2 ## `Region/Country/Area` number_of_years ## &lt;dbl&gt; &lt;int&gt; ## 1 175 1 ## 2 500 1 ## 3 520 1 ## 4 531 1 ## 5 92 2 ## 6 254 2 ## 7 312 2 ## 8 412 2 ## 9 474 2 ## 10 638 2 ## # … with 236 more rows df_un_internet %&gt;% distinct(`Region/Country/Area`, ...2) ## # A tibble: 246 × 2 ## `Region/Country/Area` ...2 ## &lt;dbl&gt; &lt;chr&gt; ## 1 1 Total, all countries or areas ## 2 15 Northern Africa ## 3 202 Sub-Saharan Africa ## 4 14 Eastern Africa ## 5 17 Middle Africa ## 6 18 Southern Africa ## 7 11 Western Africa ## 8 21 Northern America ## 9 419 Latin America &amp; the Caribbean ## 10 29 Caribbean ## # … with 236 more rows Region/Country/Area and names in …2 are in one to one correspondence. It seems that the first 21 of them are areas and data are aggregated and the rest of them are countries or regions. Since these 21 regions are overlapping, we need to study the definition. Let us choose area_num_tmp &lt;- c(15, 202, 21, 29, 420, 142, 150, 9) 6.3.7.3 Transforming df_internet &lt;- df_un_internet %&gt;% select(num = `Region/Country/Area`, region = ...2, year = Year, value = Value) %&gt;% mutate(year = as.integer(year)) head(df_internet) ## # A tibble: 6 × 4 ## num region year value ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 Total, all countries or areas 2000 6.5 ## 2 1 Total, all countries or areas 2005 16.8 ## 3 1 Total, all countries or areas 2010 29.3 ## 4 1 Total, all countries or areas 2015 41.1 ## 5 1 Total, all countries or areas 2017 46.3 ## 6 1 Total, all countries or areas 2018 49 area_vector &lt;- df_internet %&gt;% distinct(region) %&gt;% slice(1:21) %&gt;% pull() area_vector ## [1] &quot;Total, all countries or areas&quot; &quot;Northern Africa&quot; ## [3] &quot;Sub-Saharan Africa&quot; &quot;Eastern Africa&quot; ## [5] &quot;Middle Africa&quot; &quot;Southern Africa&quot; ## [7] &quot;Western Africa&quot; &quot;Northern America&quot; ## [9] &quot;Latin America &amp; the Caribbean&quot; &quot;Caribbean&quot; ## [11] &quot;Latin America&quot; &quot;Asia&quot; ## [13] &quot;Central Asia&quot; &quot;Eastern Asia&quot; ## [15] &quot;South-central Asia&quot; &quot;South-eastern Asia&quot; ## [17] &quot;Southern Asia&quot; &quot;Western Asia&quot; ## [19] &quot;Europe&quot; &quot;Oceania&quot; ## [21] &quot;Australia and New Zealand&quot; 6.3.7.4 Visualising df_internet %&gt;% filter(region == &quot;Total, all countries or areas&quot;) %&gt;% ggplot(aes(x = year, y = value)) + geom_point() + geom_line() The value, ‘Percentage of individuals using the internet’ is increasing and almost on a straight line. We will study linear regression later. df_internet %&gt;% filter(num %in% area_num_tmp) %&gt;% ggplot(aes(x = year, y = value, color = region)) + geom_line() + geom_point() df_internet %&gt;% filter(region %in% area_vector) %&gt;% ggplot(aes(x = year, y = value, color = region)) + geom_line() df_internet %&gt;% filter(!(region %in% area_vector)) %&gt;% ggplot(aes(x = factor(year), y = value)) + geom_boxplot() + labs(title = &#39;Percentage of individuals using the internet&#39;, x = &#39;year&#39;, y = &#39;percentage&#39;) df_internet %&gt;% filter(!(region %in% area_vector)) %&gt;% filter(year %in% c(2000, 2005, 2010, 2015, 2019)) %&gt;% ggplot(aes(x = value, color = factor(year))) + geom_freqpoly() + labs(title = &#39;Percentage of individuals using the internet&#39;, x = &#39;percentage&#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. df_internet %&gt;% filter(!(region %in% area_vector)) %&gt;% filter(year == 2019) %&gt;% ggplot(aes(x = value)) + geom_histogram() + labs(title = &#39;Percentage of individuals using the internet&#39;, x = &#39;percentage&#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Only countries not in area_vector are used. Medians are increasing. There are lot of upper outliers in 2000 and 2005, and lower outliers in 2018 and 2020. 6.3.7.5 Modeling df_internet %&gt;% filter(region == &quot;Total, all countries or areas&quot;) %&gt;% ggplot(aes(x = year, y = value)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; internet_all &lt;- df_internet %&gt;% filter(region == &quot;Total, all countries or areas&quot;) %&gt;% lm(value ~ year, .) internet_all %&gt;% summary() ## ## Call: ## lm(formula = value ~ year, data = .) ## ## Residuals: ## 1 2 3 4 5 6 7 ## 0.80524 -0.83146 -0.26817 -0.40488 0.02043 0.33309 0.34575 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.769e+03 6.803e+01 -70.1 1.12e-08 *** ## year 2.387e+00 3.381e-02 70.6 1.08e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6011 on 5 degrees of freedom ## Multiple R-squared: 0.999, Adjusted R-squared: 0.9988 ## F-statistic: 4985 on 1 and 5 DF, p-value: 1.08e-08 Each year internet use is increasing by about 2.4 percents. df_internet %&gt;% filter(num %in% area_num_tmp) %&gt;% ggplot(aes(x = year, y = value)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; internet_area &lt;- df_internet %&gt;% filter(num %in% area_num_tmp) %&gt;% lm(value ~ year, .) internet_area %&gt;% summary() ## ## Call: ## lm(formula = value ~ year, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -43.255 -13.282 -4.344 19.570 42.655 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -5045.5456 827.6849 -6.096 1.20e-07 *** ## year 2.5293 0.4114 6.148 9.85e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 20.68 on 54 degrees of freedom ## Multiple R-squared: 0.4118, Adjusted R-squared: 0.4009 ## F-statistic: 37.8 on 1 and 54 DF, p-value: 9.851e-08 df_internet %&gt;% filter(!(region %in% area_vector)) %&gt;% ggplot(aes(x = year, y = value)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; internet_country &lt;- df_internet %&gt;% filter(!(region %in% area_vector)) %&gt;% lm(value ~ year, .) internet_country %&gt;% summary() ## ## Call: ## lm(formula = value ~ year, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -57.950 -18.042 -1.026 18.268 64.755 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6160.7035 214.9829 -28.66 &lt;2e-16 *** ## year 3.0838 0.1069 28.84 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 24.65 on 1208 degrees of freedom ## Multiple R-squared: 0.4078, Adjusted R-squared: 0.4073 ## F-statistic: 831.8 on 1 and 1208 DF, p-value: &lt; 2.2e-16 internet_summary &lt;- list(ALL = internet_all, By_Area = internet_area, By_Country = internet_country) msummary(internet_summary, statistic = &#39;p.value&#39;) ALL By_Area By_Country (Intercept) -4768.989 -5045.546 -6160.704 (&lt;0.001) (&lt;0.001) (&lt;0.001) year 2.387 2.529 3.084 (&lt;0.001) (&lt;0.001) (&lt;0.001) Num.Obs. 7 56 1210 R2 0.999 0.412 0.408 R2 Adj. 0.999 0.401 0.407 AIC 16.4 502.2 11193.3 BIC 16.2 508.2 11208.5 Log.Lik. -5.192 -248.085 -5593.626 RMSE 0.51 20.31 24.63 What can you see? What are the difference of these three models? Which one is what you wanted? All aggreagated growth, growth by area, or growth by country of the internet use? 6.3.8 Bond Yields 6.3.8.1 Importing Bonds and debentures are long-term securities that give the holders the unconditional right to one or both of: (a) a fixed or contractually determined variable money income in the form of coupon payments, i.e. payment of interest is not dependent on earnings of the debtors, (b) a stated fixed sum as a repayment of principal on a specified date or dates when the security is redeemed. Data at United Nations: https://data.un.org/Data.aspx?q=bond+yeild&amp;d=IFS&amp;f=SeriesCode%3a61. Reach the site by search string “bond yields” Downloaded Data file in CSV: UNdata_Export_20220205_134850951.csv df_by &lt;- read_csv(&quot;data/UNdata_Export_20220205_134850951.csv&quot;) ## Rows: 2462 Columns: 6 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): OID, Country or Area, Description, Magnitude ## dbl (2): Year, Value ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. 6.3.8.2 Glimpsing df_by ## # A tibble: 2,462 × 6 ## OID `Country or Area` Year Description Magnit…¹ Value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 91161...ZF... ARMENIA 2007 GOVERNMENT BOND YIELD PERCENT… 6.56 ## 2 91161...ZF... ARMENIA 2006 GOVERNMENT BOND YIELD PERCENT… 5.86 ## 3 91161...ZF... ARMENIA 2005 GOVERNMENT BOND YIELD PERCENT… 5.18 ## 4 91161...ZF... ARMENIA 2004 GOVERNMENT BOND YIELD PERCENT… 8.21 ## 5 91161...ZF... ARMENIA 2003 GOVERNMENT BOND YIELD PERCENT… 15.7 ## 6 91161...ZF... ARMENIA 2002 GOVERNMENT BOND YIELD PERCENT… 17.4 ## 7 91161...ZF... ARMENIA 2001 GOVERNMENT BOND YIELD PERCENT… 23.2 ## 8 91161...ZF... ARMENIA 2000 GOVERNMENT BOND YIELD PERCENT… 25.5 ## 9 19361A..ZF... AUSTRALIA 2009 TREASURY BONDS: 3 YEARS PERCENT… 4.26 ## 10 19361...ZF... AUSTRALIA 2009 TREASURY BONDS: 15 YEARS PERCENT… 5.09 ## # … with 2,452 more rows, and abbreviated variable name ¹​Magnitude df_by %&gt;% glimpse() ## Rows: 2,462 ## Columns: 6 ## $ OID &lt;chr&gt; &quot;91161...ZF...&quot;, &quot;91161...ZF...&quot;, &quot;91161...ZF...&quot;, &quot;… ## $ `Country or Area` &lt;chr&gt; &quot;ARMENIA&quot;, &quot;ARMENIA&quot;, &quot;ARMENIA&quot;, &quot;ARMENIA&quot;, &quot;ARMENIA… ## $ Year &lt;dbl&gt; 2007, 2006, 2005, 2004, 2003, 2002, 2001, 2000, 2009… ## $ Description &lt;chr&gt; &quot;GOVERNMENT BOND YIELD&quot;, &quot;GOVERNMENT BOND YIELD&quot;, &quot;G… ## $ Magnitude &lt;chr&gt; &quot;PERCENT PER ANNU&quot;, &quot;PERCENT PER ANNU&quot;, &quot;PERCENT PER… ## $ Value &lt;dbl&gt; 6.56222, 5.85693, 5.18325, 8.20862, 15.71420, 17.442… df_by %&gt;% distinct(Description) ## # A tibble: 33 × 1 ## Description ## &lt;chr&gt; ## 1 GOVERNMENT BOND YIELD ## 2 TREASURY BONDS: 3 YEARS ## 3 TREASURY BONDS: 15 YEARS ## 4 GOV. BONDS: 10 YEARS ## 5 GOVERNMENT BOND YEILD ## 6 GOVT BOND YIELD 3-5 YEARS ## 7 GOVERNMENT BOND YIELD &gt; 10 YRS. ## 8 GOVT. BOND YIELD ## 9 MORTGAGE BOND YIELD ## 10 LONG-TERM LOAN RATE ## # … with 23 more rows df_by %&gt;% group_by(Description) %&gt;% summarize(num_of_regions = n_distinct(`Country or Area`)) %&gt;% arrange(desc(num_of_regions)) ## # A tibble: 33 × 2 ## Description num_of_regions ## &lt;chr&gt; &lt;int&gt; ## 1 GOVERNMENT BOND YIELD 44 ## 2 15 YEAR TREASURY BOND YIELD 1 ## 3 5 YEAR TREASURY BOND YIELD 1 ## 4 AVG SHT TRM GOVT BONDS RATE 1 ## 5 COUPON RATE ON L.T.DEVELPT BONDS 1 ## 6 GOV. BONDS: 10 YEARS 1 ## 7 GOVENMENT BOND YIELD 1 ## 8 GOVERNMENT BOND RATE 1 ## 9 GOVERNMENT BOND YEILD 1 ## 10 GOVERNMENT BOND YIELD - 10 YEAR IND. 1 ## # … with 23 more rows df_by %&gt;% filter(Description == &quot;GOVERNMENT BOND YIELD&quot;) %&gt;% distinct(OID, `Country or Area`, Description) ## # A tibble: 44 × 3 ## OID `Country or Area` Description ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 91161...ZF... ARMENIA GOVERNMENT BOND YIELD ## 2 12261...ZF... AUSTRIA GOVERNMENT BOND YIELD ## 3 12461...ZF... BELGIUM GOVERNMENT BOND YIELD ## 4 93561...ZF... CZECH REPUBLIC GOVERNMENT BOND YIELD ## 5 12861...ZF... DENMARK GOVERNMENT BOND YIELD ## 6 64461...ZF... ETHIOPIA GOVERNMENT BOND YIELD ## 7 16361...ZF... EURO AREA GOVERNMENT BOND YIELD ## 8 17261...ZF... FINLAND GOVERNMENT BOND YIELD ## 9 13261...ZF... FRANCE GOVERNMENT BOND YIELD ## 10 13461...ZF... GERMANY GOVERNMENT BOND YIELD ## # … with 34 more rows df_by %&gt;% filter(Description != &quot;GOVERNMENT BOND YIELD&quot;) %&gt;% distinct(OID, `Country or Area`, Description) ## # A tibble: 32 × 3 ## OID `Country or Area` Description ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 19361A..ZF... AUSTRALIA TREASURY BONDS: 3 YEARS ## 2 19361...ZF... AUSTRALIA TREASURY BONDS: 15 YEARS ## 3 61661...ZF... BOTSWANA GOV. BONDS: 10 YEARS ## 4 91861...ZF... BULGARIA GOVERNMENT BOND YEILD ## 5 15661A..ZF... CANADA GOVT BOND YIELD 3-5 YEARS ## 6 15661...ZF... CANADA GOVERNMENT BOND YIELD &gt; 10 YRS. ## 7 42361...ZF... CYPRUS GOVT. BOND YIELD ## 8 12861A..ZF... DENMARK MORTGAGE BOND YIELD ## 9 93961...ZF... ESTONIA LONG-TERM LOAN RATE ## 10 81961...ZF... FIJI AVG SHT TRM GOVT BONDS RATE ## # … with 22 more rows df_by %&gt;% distinct(Magnitude) ## # A tibble: 1 × 1 ## Magnitude ## &lt;chr&gt; ## 1 PERCENT PER ANNU df_by %&gt;% distinct(Year) %&gt;% arrange(desc(Year)) ## # A tibble: 62 × 1 ## Year ## &lt;dbl&gt; ## 1 2009 ## 2 2008 ## 3 2007 ## 4 2006 ## 5 2005 ## 6 2004 ## 7 2003 ## 8 2002 ## 9 2001 ## 10 2000 ## # … with 52 more rows df_by %&gt;% group_by(Year) %&gt;% summarize(number_of_countries = n_distinct(`Country or Area`)) %&gt;% arrange(desc(number_of_countries)) ## # A tibble: 62 × 2 ## Year number_of_countries ## &lt;dbl&gt; &lt;int&gt; ## 1 2006 61 ## 2 2005 60 ## 3 2007 59 ## 4 2004 58 ## 5 2003 57 ## 6 2002 54 ## 7 2001 53 ## 8 2008 53 ## 9 1995 47 ## 10 2000 47 ## # … with 52 more rows df_by %&gt;% distinct(OID) ## # A tibble: 76 × 1 ## OID ## &lt;chr&gt; ## 1 91161...ZF... ## 2 19361A..ZF... ## 3 19361...ZF... ## 4 12261...ZF... ## 5 12461...ZF... ## 6 61661...ZF... ## 7 91861...ZF... ## 8 15661A..ZF... ## 9 15661...ZF... ## 10 42361...ZF... ## # … with 66 more rows df_by %&gt;% distinct(`Country or Area`) ## # A tibble: 69 × 1 ## `Country or Area` ## &lt;chr&gt; ## 1 ARMENIA ## 2 AUSTRALIA ## 3 AUSTRIA ## 4 BELGIUM ## 5 BOTSWANA ## 6 BULGARIA ## 7 CANADA ## 8 CYPRUS ## 9 CZECH REPUBLIC ## 10 DENMARK ## # … with 59 more rows df_by %&gt;% distinct(OID, `Country or Area`) ## # A tibble: 76 × 2 ## OID `Country or Area` ## &lt;chr&gt; &lt;chr&gt; ## 1 91161...ZF... ARMENIA ## 2 19361A..ZF... AUSTRALIA ## 3 19361...ZF... AUSTRALIA ## 4 12261...ZF... AUSTRIA ## 5 12461...ZF... BELGIUM ## 6 61661...ZF... BOTSWANA ## 7 91861...ZF... BULGARIA ## 8 15661A..ZF... CANADA ## 9 15661...ZF... CANADA ## 10 42361...ZF... CYPRUS ## # … with 66 more rows df_by %&gt;% group_by(`Country or Area`) %&gt;% summarize(num_of_oid = n_distinct(OID)) %&gt;% arrange(desc(num_of_oid)) ## # A tibble: 69 × 2 ## `Country or Area` num_of_oid ## &lt;chr&gt; &lt;int&gt; ## 1 AUSTRALIA 2 ## 2 CANADA 2 ## 3 DENMARK 2 ## 4 ITALY 2 ## 5 MOROCCO 2 ## 6 UNITED KINGDOM 2 ## 7 UNITED STATES 2 ## 8 ARMENIA 1 ## 9 AUSTRIA 1 ## 10 BELGIUM 1 ## # … with 59 more rows Seven countries have two OID and others have one OID. 6.3.8.3 Visualising df_by %&gt;% filter(`Country or Area` == &#39;JAPAN&#39;) %&gt;% ggplot(aes(x = Year, y = Value)) + geom_line() df_by %&gt;% filter(`Country or Area` %in% c(&#39;FRANCE&#39;, &#39;GERMANY&#39;, &#39;GREECE&#39;, &#39;CHINA&#39;, &#39;INDIA&#39;, &#39;JAPAN&#39;, &#39;PAKISTAN&#39;)) %&gt;% ggplot(aes(x = Year, y = Value, color = `Country or Area`)) + geom_line() 6.3.8.4 Modeling df_by %&gt;% filter(`Country or Area` %in% c(&#39;JAPAN&#39;)) %&gt;% ggplot(aes(x = Year, y = Value)) + geom_line() + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; df_by %&gt;% filter(`Country or Area` %in% c(&#39;JAPAN&#39;)) %&gt;% lm(Value ~ Year, .) %&gt;% summary() ## ## Call: ## lm(formula = Value ~ Year, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.2402 -1.0199 -0.3071 0.9237 2.8517 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 385.05294 32.41293 11.88 5.15e-15 *** ## Year -0.19123 0.01631 -11.73 7.86e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.374 on 42 degrees of freedom ## Multiple R-squared: 0.766, Adjusted R-squared: 0.7604 ## F-statistic: 137.5 on 1 and 42 DF, p-value: 7.864e-15 df_by %&gt;% filter(Description %in% c(&#39;GOVERNMENT BOND YIELD&#39;)) %&gt;% ggplot(aes(x = Year, y = Value)) + scale_y_log10() + geom_point() + geom_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; df_by %&gt;% filter(Description %in% c(&#39;GOVERNMENT BOND YIELD&#39;)) %&gt;% filter(Year &gt;=1980) %&gt;% ggplot(aes(x = Year, y = Value)) + scale_y_log10() + geom_point() + geom_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; df_by %&gt;% filter(Description %in% c(&#39;GOVERNMENT BOND YIELD&#39;)) %&gt;% filter(Year &gt;=1980) %&gt;% lm(Value ~ Year, .) %&gt;% summary() ## ## Call: ## lm(formula = Value ~ Year, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.041 -3.619 -2.251 0.882 258.725 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 599.57505 93.60857 6.405 2.43e-10 *** ## Year -0.29534 0.04691 -6.296 4.79e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.07 on 888 degrees of freedom ## Multiple R-squared: 0.04273, Adjusted R-squared: 0.04165 ## F-statistic: 39.64 on 1 and 888 DF, p-value: 4.793e-10 Very small R squared value, however we can see a downward trend as the coefficient of Year is about -0.3. 6.3.9 Elderly Population 6.3.9.1 Importing OECD website: https://data.oecd.org/ Choose from “topic”, “Society”, and then “Elderly population” URL: https://data.oecd.org/pop/elderly-population.htm We can copy the URL of the download data The elderly population is defined as people aged 65 and over. Use of CLASS.xls url_oecd_elderly &lt;- &quot;https://stats.oecd.org/sdmx-json/data/DP_LIVE/.ELDLYPOP.TOT.PC_POP.A/OECD?contentType=csv&amp;detail=code&amp;separator=comma&amp;csv-lang=en&amp;startPeriod=1970&quot; elderly_population &lt;- read_csv(url_oecd_elderly) ## Rows: 2909 Columns: 8 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (5): LOCATION, INDICATOR, SUBJECT, MEASURE, FREQUENCY ## dbl (2): TIME, Value ## lgl (1): Flag Codes ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. 6.3.9.2 Glimpsing elderly_population ## # A tibble: 2,909 × 8 ## LOCATION INDICATOR SUBJECT MEASURE FREQUENCY TIME Value `Flag Codes` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 AUS ELDLYPOP TOT PC_POP A 1970 8.35 NA ## 2 AUS ELDLYPOP TOT PC_POP A 1971 8.34 NA ## 3 AUS ELDLYPOP TOT PC_POP A 1972 8.41 NA ## 4 AUS ELDLYPOP TOT PC_POP A 1973 8.51 NA ## 5 AUS ELDLYPOP TOT PC_POP A 1974 8.61 NA ## 6 AUS ELDLYPOP TOT PC_POP A 1975 8.72 NA ## 7 AUS ELDLYPOP TOT PC_POP A 1976 8.93 NA ## 8 AUS ELDLYPOP TOT PC_POP A 1977 9.07 NA ## 9 AUS ELDLYPOP TOT PC_POP A 1978 9.24 NA ## 10 AUS ELDLYPOP TOT PC_POP A 1979 9.44 NA ## # … with 2,899 more rows elderly_population %&gt;% distinct(INDICATOR) ## # A tibble: 1 × 1 ## INDICATOR ## &lt;chr&gt; ## 1 ELDLYPOP elderly_population %&gt;% distinct(SUBJECT) ## # A tibble: 1 × 1 ## SUBJECT ## &lt;chr&gt; ## 1 TOT elderly_population %&gt;% distinct(MEASURE) ## # A tibble: 1 × 1 ## MEASURE ## &lt;chr&gt; ## 1 PC_POP elderly_population %&gt;% distinct(FREQUENCY) ## # A tibble: 1 × 1 ## FREQUENCY ## &lt;chr&gt; ## 1 A elderly_population %&gt;% distinct(`Flag Codes`) ## # A tibble: 1 × 1 ## `Flag Codes` ## &lt;lgl&gt; ## 1 NA 6.3.9.3 Transforming elderly_pop &lt;- elderly_population %&gt;% select(iso3c = LOCATION, year = TIME, value = Value) elderly_pop ## # A tibble: 2,909 × 3 ## iso3c year value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AUS 1970 8.35 ## 2 AUS 1971 8.34 ## 3 AUS 1972 8.41 ## 4 AUS 1973 8.51 ## 5 AUS 1974 8.61 ## 6 AUS 1975 8.72 ## 7 AUS 1976 8.93 ## 8 AUS 1977 9.07 ## 9 AUS 1978 9.24 ## 10 AUS 1979 9.44 ## # … with 2,899 more rows elderly_pop_ext &lt;- elderly_pop %&gt;% left_join(wb_countries, by = &#39;iso3c&#39;) elderly_pop_ext ## # A tibble: 2,909 × 8 ## iso3c year value country region income lending other ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 AUS 1970 8.35 Australia East Asia &amp; Pacific High income &lt;NA&gt; &lt;NA&gt; ## 2 AUS 1971 8.34 Australia East Asia &amp; Pacific High income &lt;NA&gt; &lt;NA&gt; ## 3 AUS 1972 8.41 Australia East Asia &amp; Pacific High income &lt;NA&gt; &lt;NA&gt; ## 4 AUS 1973 8.51 Australia East Asia &amp; Pacific High income &lt;NA&gt; &lt;NA&gt; ## 5 AUS 1974 8.61 Australia East Asia &amp; Pacific High income &lt;NA&gt; &lt;NA&gt; ## 6 AUS 1975 8.72 Australia East Asia &amp; Pacific High income &lt;NA&gt; &lt;NA&gt; ## 7 AUS 1976 8.93 Australia East Asia &amp; Pacific High income &lt;NA&gt; &lt;NA&gt; ## 8 AUS 1977 9.07 Australia East Asia &amp; Pacific High income &lt;NA&gt; &lt;NA&gt; ## 9 AUS 1978 9.24 Australia East Asia &amp; Pacific High income &lt;NA&gt; &lt;NA&gt; ## 10 AUS 1979 9.44 Australia East Asia &amp; Pacific High income &lt;NA&gt; &lt;NA&gt; ## # … with 2,899 more rows elderly_pop_ext %&gt;% distinct(iso3c, country) ## # A tibble: 56 × 2 ## iso3c country ## &lt;chr&gt; &lt;chr&gt; ## 1 AUS Australia ## 2 AUT Austria ## 3 BEL Belgium ## 4 CAN Canada ## 5 CZE Czech Republic ## 6 DNK Denmark ## 7 FIN Finland ## 8 FRA France ## 9 DEU Germany ## 10 GRC Greece ## # … with 46 more rows 6.3.9.4 Visualising The following render the same chart. elderly_pop_ext %&gt;% filter(year == 2020, !is.na(region)) %&gt;% ggplot(aes(region, fill = income)) + geom_bar() + coord_flip() elderly_pop_ext %&gt;% filter(year == 2020, !is.na(region)) %&gt;% ggplot(aes(y = region, fill = income)) + geom_bar(orientation = &quot;y&quot;) elderly_pop_ext %&gt;% filter(iso3c %in% c(&quot;AUS&quot;, &quot;CAN&quot;, &quot;DEU&quot;, &quot;FRA&quot;, &quot;GBR&quot;, &quot;JPN&quot;, &quot;KOR&quot;, &quot;USA&quot;)) %&gt;% ggplot(aes(x = year, y = value, color = country)) + geom_line() elderly_pop_ext %&gt;% filter(income == &quot;High income&quot;) %&gt;% ggplot(aes(x = year, y = value, color = country)) + geom_line() + labs(title = &quot;High income&quot;) elderly_pop_ext %&gt;% filter(income == &quot;Upper middle income&quot;) %&gt;% ggplot(aes(x = year, y = value, color = country)) + geom_line() + labs(title = &quot;Upper middle income&quot;) elderly_pop_ext %&gt;% filter(income == &quot;Lower middle income&quot;) %&gt;% ggplot(aes(x = year, y = value, color = country)) + geom_line() + labs(title = &quot;Lower middle income&quot;) #### Modeling elderly_pop_all &lt;- elderly_pop_ext %&gt;% lm(value ~ year, .) elderly_pop_all %&gt;% summary() ## ## Call: ## lm(formula = value ~ year, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.0913 -3.6968 0.7292 3.3638 13.4150 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -275.96633 10.49675 -26.29 &lt;2e-16 *** ## year 0.14419 0.00526 27.41 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.254 on 2907 degrees of freedom ## Multiple R-squared: 0.2054, Adjusted R-squared: 0.2051 ## F-statistic: 751.4 on 1 and 2907 DF, p-value: &lt; 2.2e-16 elderly_pop_high &lt;- elderly_pop_ext %&gt;% filter(income == &quot;High income&quot;) %&gt;% lm(value ~ year, .) elderly_pop_high %&gt;% summary() ## ## Call: ## lm(formula = value ~ year, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.9763 -1.6256 0.4638 2.2517 11.5163 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.018e+02 9.756e+00 -30.93 &lt;2e-16 *** ## year 1.579e-01 4.889e-03 32.30 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.344 on 2077 degrees of freedom ## Multiple R-squared: 0.3344, Adjusted R-squared: 0.334 ## F-statistic: 1043 on 1 and 2077 DF, p-value: &lt; 2.2e-16 elderly_pop_upper_middle &lt;- elderly_pop_ext %&gt;% filter(income == &quot;Upper middle income&quot;) %&gt;% lm(value ~ year, .) elderly_pop_upper_middle %&gt;% summary() ## ## Call: ## lm(formula = value ~ year, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.198 -2.219 -1.372 1.987 11.470 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.161e+02 1.991e+01 -10.85 &lt;2e-16 *** ## year 1.120e-01 9.977e-03 11.23 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.405 on 517 degrees of freedom ## Multiple R-squared: 0.196, Adjusted R-squared: 0.1945 ## F-statistic: 126.1 on 1 and 517 DF, p-value: &lt; 2.2e-16 elderly_pop_lower_middle &lt;- elderly_pop_ext %&gt;% filter(income == &quot;Lower middle income&quot;) %&gt;% lm(value ~ year, .) elderly_pop_lower_middle %&gt;% summary() ## ## Call: ## lm(formula = value ~ year, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.35190 -0.24233 -0.09253 0.20393 1.07837 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.137e+02 4.039e+00 -28.16 &lt;2e-16 *** ## year 5.920e-02 2.024e-03 29.25 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3097 on 102 degrees of freedom ## Multiple R-squared: 0.8935, Adjusted R-squared: 0.8924 ## F-statistic: 855.6 on 1 and 102 DF, p-value: &lt; 2.2e-16 elderly_pop_summary &lt;- list(ALL = elderly_pop_all,High = elderly_pop_high, Upper_middle = elderly_pop_upper_middle, Loer_middle = elderly_pop_lower_middle) msummary(elderly_pop_summary, statistic = &#39;p.value&#39;) ALL High Upper_middle Loer_middle (Intercept) -275.966 -301.813 -216.065 -113.712 (&lt;0.001) (&lt;0.001) (&lt;0.001) (&lt;0.001) year 0.144 0.158 0.112 0.059 (&lt;0.001) (&lt;0.001) (&lt;0.001) (&lt;0.001) Num.Obs. 2909 2079 519 104 R2 0.205 0.334 0.196 0.893 R2 Adj. 0.205 0.334 0.194 0.892 AIC 16682.8 10923.6 2748.7 55.3 BIC 16700.7 10940.5 2761.4 63.3 Log.Lik. -8338.381 -5458.809 -1371.326 -24.672 RMSE 4.25 3.34 3.40 0.31 All data show the increase of the population of elderly people. There are differences in income level. Can you sess? Look at year, the slope. Can we state that they are significangly different? It is a statistical problem. 6.3.10 Education 6.3.10.1 Importing Go to UN data website: https://data.un.org/ At the bottom in popular statistical tables, scroll to education download from “education at the primary, secondary and tertiary level” edu_tmp &lt;- read_csv(&quot;data/edu.csv&quot;, skip = 1, n_max =6942) %&gt;% slice(-1) ## New names: ## Rows: 6940 Columns: 7 ## ── Column specification ## ────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; chr (4): ...2, Series, Footnotes, Source dbl (2): Region/Country/Area, Year num (1): Value ## ℹ Use `spec()` to retrieve the full column specification for this data. ℹ Specify the column types or set `show_col_types = ## FALSE` to quiet this message. ## • `` -&gt; `...2` edu_tmp ## # A tibble: 6,939 × 7 ## `Region/Country/Area` ...2 Year Series Value Footn…¹ Source ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Total, all countrie… 2005 Gross… 1.04e2 &lt;NA&gt; Unite… ## 2 1 Total, all countrie… 2005 Gross… 9.97e1 &lt;NA&gt; Unite… ## 3 1 Total, all countrie… 2005 Stude… 5.09e5 &lt;NA&gt; Unite… ## 4 1 Total, all countrie… 2005 Gross… 6.58e1 &lt;NA&gt; Unite… ## 5 1 Total, all countrie… 2005 Gross… 6.23e1 &lt;NA&gt; Unite… ## 6 1 Total, all countrie… 2005 Stude… 2.00e5 &lt;NA&gt; Unite… ## 7 1 Total, all countrie… 2005 Gross… 5.12e1 &lt;NA&gt; Unite… ## 8 1 Total, all countrie… 2005 Gross… 4.83e1 &lt;NA&gt; Unite… ## 9 1 Total, all countrie… 2010 Stude… 6.97e5 &lt;NA&gt; Unite… ## 10 1 Total, all countrie… 2010 Gross… 1.04e2 &lt;NA&gt; Unite… ## # … with 6,929 more rows, and abbreviated variable name ¹​Footnotes 6.3.10.2 Transforming edu_data &lt;- edu_tmp %&gt;% select(`Region/Country/Area`:Source) edu_data ## # A tibble: 6,939 × 7 ## `Region/Country/Area` ...2 Year Series Value Footn…¹ Source ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Total, all countrie… 2005 Gross… 1.04e2 &lt;NA&gt; Unite… ## 2 1 Total, all countrie… 2005 Gross… 9.97e1 &lt;NA&gt; Unite… ## 3 1 Total, all countrie… 2005 Stude… 5.09e5 &lt;NA&gt; Unite… ## 4 1 Total, all countrie… 2005 Gross… 6.58e1 &lt;NA&gt; Unite… ## 5 1 Total, all countrie… 2005 Gross… 6.23e1 &lt;NA&gt; Unite… ## 6 1 Total, all countrie… 2005 Stude… 2.00e5 &lt;NA&gt; Unite… ## 7 1 Total, all countrie… 2005 Gross… 5.12e1 &lt;NA&gt; Unite… ## 8 1 Total, all countrie… 2005 Gross… 4.83e1 &lt;NA&gt; Unite… ## 9 1 Total, all countrie… 2010 Stude… 6.97e5 &lt;NA&gt; Unite… ## 10 1 Total, all countrie… 2010 Gross… 1.04e2 &lt;NA&gt; Unite… ## # … with 6,929 more rows, and abbreviated variable name ¹​Footnotes colnames(edu_data) ## [1] &quot;Region/Country/Area&quot; &quot;...2&quot; &quot;Year&quot; ## [4] &quot;Series&quot; &quot;Value&quot; &quot;Footnotes&quot; ## [7] &quot;Source&quot; edu_tidy &lt;- edu_data %&gt;% select(country = `...2`, year = Year, series = Series, value = Value) edu_tidy ## # A tibble: 6,939 × 4 ## country year series value ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Total, all countries or areas 2005 Gross enrollment ratio - Primary … 1.04e2 ## 2 Total, all countries or areas 2005 Gross enrollment ratio - Primary … 9.97e1 ## 3 Total, all countries or areas 2005 Students enrolled in secondary ed… 5.09e5 ## 4 Total, all countries or areas 2005 Gross enrollment ratio - Secondar… 6.58e1 ## 5 Total, all countries or areas 2005 Gross enrollment ratio - Secondar… 6.23e1 ## 6 Total, all countries or areas 2005 Students enrolled in upper second… 2.00e5 ## 7 Total, all countries or areas 2005 Gross enrollment ratio - Upper se… 5.12e1 ## 8 Total, all countries or areas 2005 Gross enrollment ratio - Upper se… 4.83e1 ## 9 Total, all countries or areas 2010 Students enrolled in primary educ… 6.97e5 ## 10 Total, all countries or areas 2010 Gross enrollment ratio - Primary … 1.04e2 ## # … with 6,929 more rows edu_tidy %&gt;% distinct(series) ## # A tibble: 9 × 1 ## series ## &lt;chr&gt; ## 1 Gross enrollment ratio - Primary (male) ## 2 Gross enrollment ratio - Primary (female) ## 3 Students enrolled in secondary education (thousands) ## 4 Gross enrollment ratio - Secondary (male) ## 5 Gross enrollment ratio - Secondary (female) ## 6 Students enrolled in upper secondary education (thousands) ## 7 Gross enrollment ratio - Upper secondary level (male) ## 8 Gross enrollment ratio - Upper secondary level (female) ## 9 Students enrolled in primary education (thousands) edu_tidy %&gt;% group_by(year) %&gt;% summarize(n = n_distinct(country)) %&gt;% arrange(desc(n)) ## # A tibble: 15 × 2 ## year n ## &lt;dbl&gt; &lt;int&gt; ## 1 2005 198 ## 2 2015 187 ## 3 2010 186 ## 4 2019 106 ## 5 2018 78 ## 6 2009 25 ## 7 2014 16 ## 8 2017 16 ## 9 2016 15 ## 10 2007 11 ## 11 2008 11 ## 12 2012 11 ## 13 2013 10 ## 14 2006 7 ## 15 2011 6 6.3.10.3 Modeling From the correlation, we can see R squared values. edu_tidy %&gt;% filter(year == 2019) %&gt;% pivot_wider(names_from = series, values_from = value) %&gt;% select(primary = 3, secondary = 6, tertiary = 9) %&gt;% drop_na %&gt;% cor() ## primary secondary tertiary ## primary 1.0000000 0.9968584 0.9927488 ## secondary 0.9968584 1.0000000 0.9987411 ## tertiary 0.9927488 0.9987411 1.0000000 edu_tidy %&gt;% filter(year == 2019) %&gt;% pivot_wider(names_from = series, values_from = value) %&gt;% select(primary = 3, p_male = 4, p_female = 5, secondary = 6, s_mele = 7, s_female = 8, tertiary = 9, t_male = 10, t_female = 11) %&gt;% drop_na %&gt;% cor() ## primary p_male p_female secondary s_mele ## primary 1.00000000 -0.04596002 -0.06210728 0.99681390 -0.05181750 ## p_male -0.04596002 1.00000000 0.91721150 -0.05414662 0.06900585 ## p_female -0.06210728 0.91721150 1.00000000 -0.06803863 0.15039583 ## secondary 0.99681390 -0.05414662 -0.06803863 1.00000000 -0.03978368 ## s_mele -0.05181750 0.06900585 0.15039583 -0.03978368 1.00000000 ## s_female -0.05582660 0.06878992 0.16237304 -0.04326115 0.97846907 ## tertiary 0.99263244 -0.05881375 -0.07088605 0.99872764 -0.04128023 ## t_male -0.03978945 -0.07399155 -0.01141148 -0.02545667 0.86547359 ## t_female -0.05676420 -0.04125532 0.01764835 -0.04271911 0.88976428 ## s_female tertiary t_male t_female ## primary -0.05582660 0.99263244 -0.03978945 -0.05676420 ## p_male 0.06878992 -0.05881375 -0.07399155 -0.04125532 ## p_female 0.16237304 -0.07088605 -0.01141148 0.01764835 ## secondary -0.04326115 0.99872764 -0.02545667 -0.04271911 ## s_mele 0.97846907 -0.04128023 0.86547359 0.88976428 ## s_female 1.00000000 -0.04502461 0.85201284 0.91859578 ## tertiary -0.04502461 1.00000000 -0.02458739 -0.04279010 ## t_male 0.85201284 -0.02458739 1.00000000 0.95488259 ## t_female 0.91859578 -0.04279010 0.95488259 1.00000000 edu_tidy %&gt;% filter(year == 2019) %&gt;% pivot_wider(names_from = series, values_from = value) %&gt;% select(primary = 3, secondary = 6, tertiary = 9) %&gt;% drop_na %&gt;% ggplot(aes(x = secondary, y = tertiary)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; edu_tidy %&gt;% filter(year == 2019) %&gt;% pivot_wider(names_from = series, values_from = value) %&gt;% select(primary = 3, secondary = 6, tertiary = 9) %&gt;% drop_na %&gt;% filter(secondary &lt; 100000) %&gt;% ggplot(aes(x = secondary, y = tertiary)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; edu_tidy %&gt;% filter(year == 2019) %&gt;% pivot_wider(names_from = series, values_from = value) %&gt;% select(primary = 3, secondary = 6, tertiary = 9) %&gt;% drop_na %&gt;% lm(tertiary ~ secondary, .) %&gt;% summary() ## ## Call: ## lm(formula = tertiary ~ secondary, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4857.3 -130.4 -2.2 5.6 8678.4 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.185597 172.385304 -0.018 0.985 ## secondary 0.445145 0.002425 183.568 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1563 on 85 degrees of freedom ## Multiple R-squared: 0.9975, Adjusted R-squared: 0.9975 ## F-statistic: 3.37e+04 on 1 and 85 DF, p-value: &lt; 2.2e-16 "],["topics1.html", "Chapter 7 Topics in Exploratory Data Analysis I 7.1 Part I: Regression 1: 7.2 A time series data of World from 1960 to 2020 7.3 Regression 2 : CO2 Emission 7.4 Regression 3: CO2 emission (Standardization) 7.5 Regression of Categorical Variables and ANOVA 7.6 Week 7 Assignment", " Chapter 7 Topics in Exploratory Data Analysis I Prof. Taisei Kaizoji and R Notebook Compiled by HS Get AGCO2rv.csv from Moodle and place it at the same place as this file. Reference: Data Analysis for Researchers Week 7(02/02) Inference Statistics Regression Analysis Summary Give a descriptive name to each variable, when importing data by WDI command. Save data of investigation by write_csv, but if you have RNotebook, you do not need to save at every step. If you use pipe in tidyverse, i.e, %&gt;%, properly, you do not need to attach(data) or dettach(data). Dot Place Holder in lm. In the following, the following format is used. df %&gt;% lm(y ~ x, .) This is same as lm(y ~ x, data = df) ‘.’ is called the dot place holder. Recall that most of the function (or command) we use in tidyverse, the first argument is the data frame. When we use piping, we omit the first argiment. For example, df %&gt;% ggplot(aes(x, y)) + geom_point() is same as ggplot(df, aes(x,y)) + geom_point() However, the data frame argument of lm() is not the first one, therefore we use the dot place holder to show the place the data frame should be inserted. 7.1 Part I: Regression 1: Run the following only once, or you can install these packages using install packages in the Tool menu. install.packages(&quot;car&quot;) install.packages(&quot;modelsummary&quot;) 7.1.1 Why does GDP vary from country to country? library(tidyverse) # tidyverse Package, a collection of packages for data science library(WDI) # WDI Package for World Development Indicators library(car) #VIF Tool for checking multi-collinearity ## 要求されたパッケージ carData をロード中です ## ## 次のパッケージを付け加えます: &#39;car&#39; ## 以下のオブジェクトは &#39;package:dplyr&#39; からマスクされています: ## ## recode ## 以下のオブジェクトは &#39;package:purrr&#39; からマスクされています: ## ## some library(modelsummary) # Tool for writting tables of the regression results WDI Indicators NY.GDP.MKTP.CD: GDP (current US$) SP.POP.TOTL: Population, total AG.LND.TOTL.K2: Land area (sq. km) wb &lt;- as_tibble(WDI(country=&quot;all&quot;, indicator=c(gdp = &quot;NY.GDP.MKTP.CD&quot;, pop = &quot;SP.POP.TOTL&quot;, land = &quot;AG.LND.TOTL.K2&quot;), start=1960, end=2020, extra=TRUE)) wb ## # A tibble: 16,226 × 15 ## country iso2c iso3c year status lastupda…¹ gdp pop land region ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Afghanistan AF AFG 2020 &quot;&quot; 2022-09-16 2.01e10 3.89e7 652230 South… ## 2 Afghanistan AF AFG 2019 &quot;&quot; 2022-09-16 1.88e10 3.80e7 652230 South… ## 3 Afghanistan AF AFG 2017 &quot;&quot; 2022-09-16 1.88e10 3.63e7 652230 South… ## 4 Afghanistan AF AFG 1993 &quot;&quot; 2022-09-16 NA 1.58e7 652230 South… ## 5 Afghanistan AF AFG 1983 &quot;&quot; 2022-09-16 NA 1.25e7 652230 South… ## 6 Afghanistan AF AFG 2006 &quot;&quot; 2022-09-16 6.97e 9 2.64e7 652230 South… ## 7 Afghanistan AF AFG 2018 &quot;&quot; 2022-09-16 1.81e10 3.72e7 652230 South… ## 8 Afghanistan AF AFG 1981 &quot;&quot; 2022-09-16 3.48e 9 1.32e7 652230 South… ## 9 Afghanistan AF AFG 2016 &quot;&quot; 2022-09-16 1.81e10 3.54e7 652230 South… ## 10 Afghanistan AF AFG 1989 &quot;&quot; 2022-09-16 NA 1.19e7 652230 South… ## # … with 16,216 more rows, 5 more variables: capital &lt;chr&gt;, longitude &lt;chr&gt;, ## # latitude &lt;chr&gt;, income &lt;chr&gt;, lending &lt;chr&gt;, and abbreviated variable name ## # ¹​lastupdated wb_ag20 &lt;- wb %&gt;% filter(year == 2020 &amp; region==&quot;Aggregates&quot;) wb_ag20 ## # A tibble: 42 × 15 ## country iso2c iso3c year status lastu…¹ gdp pop land region capital ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Africa… ZH AFE 2020 &quot;&quot; 2022-0… 9.22e11 6.77e8 1.48e7 Aggre… &quot;&quot; ## 2 Africa… ZI AFW 2020 &quot;&quot; 2022-0… 7.84e11 4.59e8 9.05e6 Aggre… &quot;&quot; ## 3 Arab W… 1A ARB 2020 &quot;&quot; 2022-0… 2.50e12 4.36e8 1.31e7 Aggre… &quot;&quot; ## 4 Caribb… S3 CSS 2020 &quot;&quot; 2022-0… 6.59e10 7.44e6 4.05e5 Aggre… &quot;&quot; ## 5 Centra… B8 CEB 2020 &quot;&quot; 2022-0… 1.65e12 1.02e8 1.11e6 Aggre… &quot;&quot; ## 6 Early-… V2 EAR 2020 &quot;&quot; 2022-0… 1.08e13 3.33e9 3.33e7 Aggre… &quot;&quot; ## 7 East A… Z4 EAS 2020 &quot;&quot; 2022-0… 2.71e13 2.36e9 2.45e7 Aggre… &quot;&quot; ## 8 East A… 4E EAP 2020 &quot;&quot; 2022-0… 1.75e13 2.11e9 1.60e7 Aggre… &quot;&quot; ## 9 East A… T4 TEA 2020 &quot;&quot; 2022-0… 1.75e13 2.09e9 1.59e7 Aggre… &quot;&quot; ## 10 Euro a… XC EMU 2020 &quot;&quot; 2022-0… 1.30e13 3.43e8 2.68e6 Aggre… &quot;&quot; ## # … with 32 more rows, 4 more variables: longitude &lt;chr&gt;, latitude &lt;chr&gt;, ## # income &lt;chr&gt;, lending &lt;chr&gt;, and abbreviated variable name ¹​lastupdated 7.1.2 Regression wb_ag20 %&gt;% ggplot(aes(pop, gdp)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;GDP and Population&quot;, x = &quot;total population&quot;, y = &quot;GDP&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; gdp_pop &lt;-wb_ag20 %&gt;% lm(gdp ~ pop, .) summary(gdp_pop) ## ## Call: ## lm(formula = gdp ~ pop, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.352e+13 -7.740e+12 -2.406e+12 8.240e+11 4.118e+13 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.234e+12 2.591e+12 0.862 0.394 ## pop 6.498e+03 1.044e+03 6.224 2.29e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.291e+13 on 40 degrees of freedom ## Multiple R-squared: 0.492, Adjusted R-squared: 0.4793 ## F-statistic: 38.73 on 1 and 40 DF, p-value: 2.291e-07 wb_ag20 %&gt;% ggplot(aes(land, gdp)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;GDP and Land Area&quot;, x = &quot;land area&quot;, y = &quot;GDP&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; gdp_land &lt;-wb_ag20 %&gt;% lm(gdp ~ land, .) summary(gdp_land) ## ## Call: ## lm(formula = gdp ~ land, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.491e+13 -8.021e+12 -3.149e+12 9.542e+11 3.487e+13 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.100e+11 2.402e+12 0.087 0.931 ## land 4.860e+05 6.353e+04 7.649 2.38e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.154e+13 on 40 degrees of freedom ## Multiple R-squared: 0.594, Adjusted R-squared: 0.5838 ## F-statistic: 58.51 on 1 and 40 DF, p-value: 2.376e-09 gdp_pop_and_land &lt;-wb_ag20 %&gt;% lm(gdp ~ pop + land, .) summary(gdp_pop_and_land) ## ## Call: ## lm(formula = gdp ~ pop + land, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.397e+13 -9.135e+12 -3.362e+12 3.760e+12 3.334e+13 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.464e+11 2.422e+12 0.060 0.95209 ## pop -1.767e+03 2.740e+03 -0.645 0.52270 ## land 5.990e+05 1.865e+05 3.212 0.00264 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.163e+13 on 39 degrees of freedom ## Multiple R-squared: 0.5982, Adjusted R-squared: 0.5776 ## F-statistic: 29.04 on 2 and 39 DF, p-value: 1.893e-08 gpa_models &lt;- list(gdp_pop = gdp_pop, gdp_land = gdp_land, gdp_pop_and_land = gdp_pop_and_land) msummary(gpa_models, statistic = &#39;p.value&#39;) gdp_pop gdp_land gdp_pop_and_land (Intercept) 2e+12 2e+11 1e+11 (0.394) (0.931) (0.952) pop 6498.247 -1767.039 (&lt;0.001) (0.523) land 486003.841 598971.604 (&lt;0.001) (0.003) Num.Obs. 42 42 42 R2 0.492 0.594 0.598 R2 Adj. 0.479 0.584 0.578 AIC 2659.0 2649.6 2651.2 BIC 2664.3 2654.8 2658.1 Log.Lik. -1326.519 -1321.813 -1321.590 RMSE 1e+13 1e+13 1e+13 The default is: msummary(gpa_models). msummary(gpa_models, statistic = 'p.value') replaces the standard error by p-value. Compare the following with the one above and the Std.Error of the summaries of each model. msummary(gpa_models) gdp_pop gdp_land gdp_pop_and_land (Intercept) 2e+12 2e+11 1e+11 (3e+12) (2e+12) (2e+12) pop 6498.247 -1767.039 (1044.119) (2739.542) land 486003.841 598971.604 (63534.820) (186468.820) Num.Obs. 42 42 42 R2 0.492 0.594 0.598 R2 Adj. 0.479 0.584 0.578 AIC 2659.0 2649.6 2651.2 BIC 2664.3 2654.8 2658.1 Log.Lik. -1326.519 -1321.813 -1321.590 RMSE 1e+13 1e+13 1e+13 wb_ag20 %&gt;% ggplot(aes(pop, land)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;population and Land&quot;, x = &quot;population&quot;, y = &quot;land&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 7.1.3 Checking for multicollinearity pop_land &lt;-wb_ag20 %&gt;% lm(pop ~ land, .) summary(pop_land) ## ## Call: ## lm(formula = pop ~ land, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -994986967 -376750174 -122152277 304321331 1587865293 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.594e+07 1.397e+08 -0.257 0.798 ## land 6.393e+01 3.694e+00 17.307 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 671200000 on 40 degrees of freedom ## Multiple R-squared: 0.8822, Adjusted R-squared: 0.8792 ## F-statistic: 299.5 on 1 and 40 DF, p-value: &lt; 2.2e-16 7.1.4 vif: Determination of multi-collinearity VIF = 1/(1-{Ri}2) 7.1.4.1 VIF (Variance Inflation Factor) gdp_pop_and_land %&gt;% vif() ## pop land ## 8.487931 8.487931 7.1.4.2 Conclusion Regions which have large population have large GDP. It was also found that the larger the region, the more populated the region. This suggests that the GPs of regions with large regions and large populations are significant. This result suggests a positive relationship between population and economic activity. 7.2 A time series data of World from 1960 to 2020 wb_world &lt;- wb %&gt;% filter(country==&quot;World&quot;) %&gt;% arrange(year) wb_world ## # A tibble: 61 × 15 ## country iso2c iso3c year status lastupdated gdp pop land region ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 World 1W WLD 1960 &quot;&quot; 2022-09-16 1.39e12 3.03e9 NA Aggre… ## 2 World 1W WLD 1961 &quot;&quot; 2022-09-16 1.45e12 3.07e9 1.30e8 Aggre… ## 3 World 1W WLD 1962 &quot;&quot; 2022-09-16 1.55e12 3.12e9 1.30e8 Aggre… ## 4 World 1W WLD 1963 &quot;&quot; 2022-09-16 1.67e12 3.19e9 1.30e8 Aggre… ## 5 World 1W WLD 1964 &quot;&quot; 2022-09-16 1.83e12 3.26e9 1.30e8 Aggre… ## 6 World 1W WLD 1965 &quot;&quot; 2022-09-16 1.99e12 3.32e9 1.30e8 Aggre… ## 7 World 1W WLD 1966 &quot;&quot; 2022-09-16 2.16e12 3.39e9 1.30e8 Aggre… ## 8 World 1W WLD 1967 &quot;&quot; 2022-09-16 2.30e12 3.46e9 1.30e8 Aggre… ## 9 World 1W WLD 1968 &quot;&quot; 2022-09-16 2.48e12 3.53e9 1.30e8 Aggre… ## 10 World 1W WLD 1969 &quot;&quot; 2022-09-16 2.74e12 3.61e9 1.30e8 Aggre… ## # … with 51 more rows, and 5 more variables: capital &lt;chr&gt;, longitude &lt;chr&gt;, ## # latitude &lt;chr&gt;, income &lt;chr&gt;, lending &lt;chr&gt; wb_world %&gt;% ggplot() + geom_line(aes(x = year, y = gdp)) + labs(title = &quot;GDP (current US$)&quot;) wb_world %&gt;% ggplot() + geom_line(aes(x = year, y = pop)) + labs(title = &quot;Total population)&quot;) wb_world %&gt;% lm(gdp ~ pop, .) %&gt;% summary() ## ## Call: ## lm(formula = gdp ~ pop, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.334e+13 -7.513e+12 -1.944e+12 6.913e+12 1.350e+13 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6.857e+13 4.151e+12 -16.52 &lt;2e-16 *** ## pop 1.862e+04 7.563e+02 24.63 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.396e+12 on 59 degrees of freedom ## Multiple R-squared: 0.9113, Adjusted R-squared: 0.9098 ## F-statistic: 606.4 on 1 and 59 DF, p-value: &lt; 2.2e-16 wb_world %&gt;% ggplot() + geom_point(aes(pop, gdp)) + labs(title = &quot;World GDP and World population&quot;, x = &quot;Total population&quot;, y = &quot;GDP (current US$)&quot;) wb_world_extra &lt;- wb_world %&gt;% mutate(diff_gdp = gdp - lag(gdp), diff_pop = pop - lag(pop)) wb_world_extra ## # A tibble: 61 × 17 ## country iso2c iso3c year status lastupdated gdp pop land region ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 World 1W WLD 1960 &quot;&quot; 2022-09-16 1.39e12 3.03e9 NA Aggre… ## 2 World 1W WLD 1961 &quot;&quot; 2022-09-16 1.45e12 3.07e9 1.30e8 Aggre… ## 3 World 1W WLD 1962 &quot;&quot; 2022-09-16 1.55e12 3.12e9 1.30e8 Aggre… ## 4 World 1W WLD 1963 &quot;&quot; 2022-09-16 1.67e12 3.19e9 1.30e8 Aggre… ## 5 World 1W WLD 1964 &quot;&quot; 2022-09-16 1.83e12 3.26e9 1.30e8 Aggre… ## 6 World 1W WLD 1965 &quot;&quot; 2022-09-16 1.99e12 3.32e9 1.30e8 Aggre… ## 7 World 1W WLD 1966 &quot;&quot; 2022-09-16 2.16e12 3.39e9 1.30e8 Aggre… ## 8 World 1W WLD 1967 &quot;&quot; 2022-09-16 2.30e12 3.46e9 1.30e8 Aggre… ## 9 World 1W WLD 1968 &quot;&quot; 2022-09-16 2.48e12 3.53e9 1.30e8 Aggre… ## 10 World 1W WLD 1969 &quot;&quot; 2022-09-16 2.74e12 3.61e9 1.30e8 Aggre… ## # … with 51 more rows, and 7 more variables: capital &lt;chr&gt;, longitude &lt;chr&gt;, ## # latitude &lt;chr&gt;, income &lt;chr&gt;, lending &lt;chr&gt;, diff_gdp &lt;dbl&gt;, diff_pop &lt;dbl&gt; wb_world_extra %&gt;% ggplot(aes(x = year, y = diff_gdp)) + geom_line() + labs(title = &quot;World GDP changes&quot;, y = &quot;gdp changes&quot;) ## Warning: Removed 1 row containing missing values (`geom_line()`). wb_world_extra %&gt;% ggplot(aes(x = year, y = diff_pop)) + geom_line() + labs(title = &quot;World population changes&quot;, y = &quot;population changes&quot;) ## Warning: Removed 1 row containing missing values (`geom_line()`). wb_world_extra %&gt;% ggplot(aes(x = diff_pop, y = diff_gdp)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;World GDP changes and World GDP changes&quot;, x = &quot;population changes&quot;, y = &quot;GDP changes&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## Warning: Removed 1 rows containing non-finite values (`stat_smooth()`). ## Warning: Removed 1 rows containing missing values (`geom_point()`). wb_world_extra %&gt;% lm(diff_gdp ~ diff_pop, .) %&gt;% summary() ## ## Call: ## lm(formula = diff_gdp ~ diff_pop, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.350e+12 -9.506e+11 -3.699e+11 3.100e+11 5.679e+12 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.460e+12 2.553e+12 -1.355 0.1806 ## diff_pop 6.152e+04 3.219e+04 1.911 0.0609 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.114e+12 on 58 degrees of freedom ## ( 1 個の観測値が欠損のため削除されました ) ## Multiple R-squared: 0.05925, Adjusted R-squared: 0.04303 ## F-statistic: 3.653 on 1 and 58 DF, p-value: 0.06092 7.2.1 Excercise: Select a region from the following; East Asia &amp; Pacific Europe &amp; Central Asia Middle East &amp; North Africa Sub-Saharan Africa Latin America &amp; Caribbean Preform regression analysis. Select the best model. 7.3 Regression 2 : CO2 Emission 7.3.1 Project Problem: “What are the causes of increased CO2 emissions?” Plan: To develop and test the hypothesis that CO2 emissions depends on GDP, population and land under cereal production of each region. Data: Using the World Bank’s World Development Indicators (WDI), we will collect data on GDP, population and land area for regions around the world. WDIindicators GDP(Y):“NY.GDP.MKTP.CD” Total Population (X1): “SP.POP.TOTL” Land under cereal production (hectares): “AG.LND.CREL.HA” 7.3.1.1 Regression Analysis Regression equation:Y =c + a x1 + b x2 + d x3 Y: CO2 emission: independent variable x1: GDP (Y) : independent variable x2: Total Population (X1) : independent variable x3: Land under cereal production (hectares): independent variable We perform a regression analysis of the above regression equation (estimate the parameters, c (intercept), a, b, d) and elect the best model. 7.3.1.2 WDI Indicator cordes co2 = EN.ATM.CO2E.KT: CO2 Emission gdp = NY.GDP.MKTP.CD: GDP (current USD) pop = SP.POP.TOTL: Total Population cereal = AG.LND.CREL.HA: Land under cereal production (hectares) WDIsearch(string = &quot;EN.ATM.CO2E.KT&quot;, field = &quot;indicator&quot;, short = FALSE)[[3]] ## [1] &quot;Carbon dioxide emissions are those stemming from the burning of fossil fuels and the manufacture of cement. They include carbon dioxide produced during consumption of solid, liquid, and gas fuels and gas flaring.&quot; wb_co2&lt;- as_tibble(WDI(country=&quot;all&quot;, indicator=c(co2 = &quot;EN.ATM.CO2E.KT&quot;, gdp = &quot;NY.GDP.MKTP.CD&quot;, pop = &quot;SP.POP.TOTL&quot;, cereal = &quot;AG.LND.CREL.HA&quot;), start=1960, end=2020, extra=TRUE)) wb_co2 ## # A tibble: 16,226 × 16 ## country iso2c iso3c year status lastu…¹ co2 gdp pop cereal region ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Afghani… AF AFG 2020 &quot;&quot; 2022-0… NA 2.01e10 3.89e7 3.04e6 South… ## 2 Afghani… AF AFG 2019 &quot;&quot; 2022-0… 6080. 1.88e10 3.80e7 2.64e6 South… ## 3 Afghani… AF AFG 2017 &quot;&quot; 2022-0… 4780. 1.88e10 3.63e7 2.42e6 South… ## 4 Afghani… AF AFG 1993 &quot;&quot; 2022-0… 1340 NA 1.58e7 2.63e6 South… ## 5 Afghani… AF AFG 1983 &quot;&quot; 2022-0… NA NA 1.25e7 2.65e6 South… ## 6 Afghani… AF AFG 2006 &quot;&quot; 2022-0… 1760. 6.97e 9 2.64e7 2.99e6 South… ## 7 Afghani… AF AFG 2018 &quot;&quot; 2022-0… 6070. 1.81e10 3.72e7 1.91e6 South… ## 8 Afghani… AF AFG 1981 &quot;&quot; 2022-0… NA 3.48e 9 1.32e7 2.91e6 South… ## 9 Afghani… AF AFG 2016 &quot;&quot; 2022-0… 5300. 1.81e10 3.54e7 2.79e6 South… ## 10 Afghani… AF AFG 1989 &quot;&quot; 2022-0… NA NA 1.19e7 2.29e6 South… ## # … with 16,216 more rows, 5 more variables: capital &lt;chr&gt;, longitude &lt;chr&gt;, ## # latitude &lt;chr&gt;, income &lt;chr&gt;, lending &lt;chr&gt;, and abbreviated variable name ## # ¹​lastupdated In the lecture the folloiwng is used. However, regions are overlapping and it seems to be better to chooose non-aggregated data. wb_co2_16 &lt;- wb_co2 %&gt;% filter(year == 2016 &amp; region==&quot;Aggregates&quot;) wb_co2_16 ## # A tibble: 42 × 16 ## country iso2c iso3c year status lastu…¹ co2 gdp pop cereal region ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Africa … ZH AFE 2016 &quot;&quot; 2022-0… 5.79e5 8.82e11 6.10e8 5.44e7 Aggre… ## 2 Africa … ZI AFW 2016 &quot;&quot; 2022-0… 2.06e5 6.91e11 4.13e8 5.76e7 Aggre… ## 3 Arab Wo… 1A ARB 2016 &quot;&quot; 2022-0… 1.84e6 2.50e12 4.04e8 2.95e7 Aggre… ## 4 Caribbe… S3 CSS 2016 &quot;&quot; 2022-0… 3.73e4 6.95e10 7.27e6 2.52e5 Aggre… ## 5 Central… B8 CEB 2016 &quot;&quot; 2022-0… 6.56e5 1.32e12 1.03e8 2.24e7 Aggre… ## 6 Early-d… V2 EAR 2016 &quot;&quot; 2022-0… 6.96e6 1.05e13 3.16e9 2.45e8 Aggre… ## 7 East As… Z4 EAS 2016 &quot;&quot; 2022-0… 1.41e7 2.28e13 2.31e9 1.80e8 Aggre… ## 8 East As… 4E EAP 2016 &quot;&quot; 2022-0… 1.13e7 1.36e13 2.06e9 1.60e8 Aggre… ## 9 East As… T4 TEA 2016 &quot;&quot; 2022-0… 1.13e7 1.36e13 2.04e9 1.59e8 Aggre… ## 10 Euro ar… XC EMU 2016 &quot;&quot; 2022-0… 2.26e6 1.20e13 3.40e8 3.24e7 Aggre… ## # … with 32 more rows, 5 more variables: capital &lt;chr&gt;, longitude &lt;chr&gt;, ## # latitude &lt;chr&gt;, income &lt;chr&gt;, lending &lt;chr&gt;, and abbreviated variable name ## # ¹​lastupdated wb_co2 %&gt;% filter(region != &quot;Aggregates&quot;, !is.na(co2)) %&gt;% group_by(year) %&gt;% summarize(n = n_distinct(country)) %&gt;% arrange(desc(n), desc(year)) ## # A tibble: 30 × 2 ## year n ## &lt;int&gt; &lt;int&gt; ## 1 2019 190 ## 2 2018 190 ## 3 2017 190 ## 4 2016 190 ## 5 2015 190 ## 6 2014 190 ## 7 2013 190 ## 8 2012 190 ## 9 2011 190 ## 10 2010 190 ## # … with 20 more rows Let us choose non-aggregated data in 2018. wb_co2_18 &lt;- wb_co2 %&gt;% filter(region != &quot;Aggregates&quot;, !is.na(co2), year == 2018) wb_co2_18 ## # A tibble: 190 × 16 ## country iso2c iso3c year status lastu…¹ co2 gdp pop cereal region ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Afghan… AF AFG 2018 &quot;&quot; 2022-0… 6.07e3 1.81e10 3.72e7 1.91e6 South… ## 2 Albania AL ALB 2018 &quot;&quot; 2022-0… 5.11e3 1.52e10 2.87e6 1.40e5 Europ… ## 3 Algeria DZ DZA 2018 &quot;&quot; 2022-0… 1.66e5 1.75e11 4.22e7 3.11e6 Middl… ## 4 Andorra AD AND 2018 &quot;&quot; 2022-0… 4.90e2 3.22e 9 7.70e4 NA Europ… ## 5 Angola AO AGO 2018 &quot;&quot; 2022-0… 2.40e4 7.78e10 3.08e7 3.06e6 Sub-S… ## 6 Antigu… AG ATG 2018 &quot;&quot; 2022-0… 5.10e2 1.61e 9 9.63e4 2.4 e1 Latin… ## 7 Argent… AR ARG 2018 &quot;&quot; 2022-0… 1.77e5 5.25e11 4.45e7 1.51e7 Latin… ## 8 Armenia AM ARM 2018 &quot;&quot; 2022-0… 5.71e3 1.25e10 2.95e6 1.27e5 Europ… ## 9 Austra… AU AUS 2018 &quot;&quot; 2022-0… 3.87e5 1.43e12 2.50e7 1.66e7 East … ## 10 Austria AT AUT 2018 &quot;&quot; 2022-0… 6.31e4 4.55e11 8.84e6 7.79e5 Europ… ## # … with 180 more rows, 5 more variables: capital &lt;chr&gt;, longitude &lt;chr&gt;, ## # latitude &lt;chr&gt;, income &lt;chr&gt;, lending &lt;chr&gt;, and abbreviated variable name ## # ¹​lastupdated 7.3.2 Regression wb_co2_18 %&gt;% ggplot(aes(gdp, co2)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;CO2 emission and GDP&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## Warning: Removed 4 rows containing non-finite values (`stat_smooth()`). ## Warning: Removed 4 rows containing missing values (`geom_point()`). wb_co2_18 %&gt;% ggplot(aes(gdp, co2)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + scale_x_log10() + scale_y_log10() + labs(title = &quot;CO2 emission and GDP in Log-Log scale&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## Warning: Removed 4 rows containing non-finite values (`stat_smooth()`). ## Warning: Removed 4 rows containing missing values (`geom_point()`). Let us take log-log plot wb_co2_18 %&gt;% ggplot(aes(pop, co2)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + scale_x_log10() + scale_y_log10() + labs(title = &quot;CO2 emission and Population in Log-Log scale&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## Warning: Removed 1 rows containing non-finite values (`stat_smooth()`). ## Warning: Removed 1 rows containing missing values (`geom_point()`). wb_co2_18 %&gt;% ggplot(aes(cereal, co2)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + scale_x_log10() + scale_y_log10() + labs(title = &quot;CO2 emission and Land under cereal production in Log-Log scale&quot;) ## Warning: Transformation introduced infinite values in continuous x-axis ## Transformation introduced infinite values in continuous x-axis ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## Warning: Removed 16 rows containing non-finite values (`stat_smooth()`). ## Warning: Removed 14 rows containing missing values (`geom_point()`). wb_co2_18m &lt;- wb_co2_18 %&gt;% filter(!is.na(co2), !is.na(gdp), !is.na(pop), !is.na(cereal), cereal &gt; 0) co2_gdp &lt;- wb_co2_18m %&gt;% lm(log10(co2) ~ log10(gdp), .) co2_pop &lt;- wb_co2_18m %&gt;% lm(log10(co2) ~ log10(pop), .) co2_cereal &lt;- wb_co2_18m %&gt;% lm(log10(co2) ~ log10(cereal), .) co2_gdp_pop &lt;- wb_co2_18m %&gt;% lm(log10(co2) ~ log10(gdp) + log10(pop), .) co2_gdp_cereal &lt;- wb_co2_18m %&gt;% lm(log10(co2) ~ log10(gdp) + log10(cereal), .) co2_pop_cereal &lt;- wb_co2_18m %&gt;% lm(log10(co2) ~ log10(pop) + log10(cereal), .) co2_all &lt;- wb_co2_18m %&gt;% lm(log10(co2) ~ log10(gdp) + log10(pop) + log10(cereal), .) msummary(list(gdp = co2_gdp, pop = co2_pop, cereal = co2_cereal, gdp_pop = co2_gdp_pop, gdp_cereal = co2_gdp_cereal, pop_cereal = co2_pop_cereal, all = co2_all)) gdp pop cereal gdp_pop gdp_cereal pop_cereal all (Intercept) -6.330 -2.297 1.967 -6.333 -6.136 -2.840 -6.311 (0.271) (0.421) (0.240) (0.264) (0.279) (0.520) (0.292) log10(gdp) 0.987 0.899 0.944 0.900 (0.025) (0.038) (0.031) (0.039) log10(pop) 0.939 0.136 1.104 0.127 (0.060) (0.044) (0.111) (0.068) log10(cereal) 0.410 0.048 -0.109 0.005 (0.042) (0.020) (0.062) (0.030) Num.Obs. 170 170 170 170 170 170 170 R2 0.901 0.592 0.362 0.907 0.905 0.599 0.907 R2 Adj. 0.901 0.589 0.358 0.905 0.903 0.594 0.905 AIC 3683.4 3924.8 4000.7 3676.2 3679.6 3923.7 3678.1 BIC 3692.8 3934.2 4010.1 3688.7 3692.2 3936.2 3693.8 Log.Lik. -40.453 -161.134 -199.096 -35.814 -37.559 -159.566 -35.797 RMSE 0.31 0.62 0.78 0.30 0.30 0.62 0.30 vif(co2_gdp_pop) ## log10(gdp) log10(pop) ## 2.374532 2.374532 vif(co2_gdp_cereal) ## log10(gdp) log10(cereal) ## 1.5193 1.5193 vif(co2_pop_cereal) ## log10(pop) log10(cereal) ## 3.447368 3.447368 vif(co2_all) ## log10(gdp) log10(pop) log10(cereal) ## 2.438050 5.532058 3.539584 list(gdp_pop = vif(co2_gdp_pop), gdp_cereal = vif(co2_gdp_cereal), pop_cereal = vif(co2_pop_cereal), all = vif(co2_all)) ## $gdp_pop ## log10(gdp) log10(pop) ## 2.374532 2.374532 ## ## $gdp_cereal ## log10(gdp) log10(cereal) ## 1.5193 1.5193 ## ## $pop_cereal ## log10(pop) log10(cereal) ## 3.447368 3.447368 ## ## $all ## log10(gdp) log10(pop) log10(cereal) ## 2.438050 5.532058 3.539584 7.3.2.1 We select regression model; CO2 emission = c + a (GDP) + b (population) (Omit land because of multicollinearity) 7.3.2.2 A time series data of World from 1960 to 2020 wb_co2_world &lt;- wb_co2 %&gt;% filter(country==&quot;World&quot;) wb_co2_world ## # A tibble: 61 × 16 ## country iso2c iso3c year status lastup…¹ co2 gdp pop cereal region ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 World 1W WLD 1960 &quot;&quot; 2022-09… NA 1.39e12 3.03e9 NA Aggre… ## 2 World 1W WLD 1961 &quot;&quot; 2022-09… NA 1.45e12 3.07e9 5.15e8 Aggre… ## 3 World 1W WLD 1962 &quot;&quot; 2022-09… NA 1.55e12 3.12e9 5.17e8 Aggre… ## 4 World 1W WLD 1963 &quot;&quot; 2022-09… NA 1.67e12 3.19e9 5.24e8 Aggre… ## 5 World 1W WLD 1964 &quot;&quot; 2022-09… NA 1.83e12 3.26e9 5.32e8 Aggre… ## 6 World 1W WLD 1965 &quot;&quot; 2022-09… NA 1.99e12 3.32e9 5.30e8 Aggre… ## 7 World 1W WLD 1966 &quot;&quot; 2022-09… NA 2.16e12 3.39e9 5.35e8 Aggre… ## 8 World 1W WLD 1967 &quot;&quot; 2022-09… NA 2.30e12 3.46e9 5.47e8 Aggre… ## 9 World 1W WLD 1968 &quot;&quot; 2022-09… NA 2.48e12 3.53e9 5.52e8 Aggre… ## 10 World 1W WLD 1969 &quot;&quot; 2022-09… NA 2.74e12 3.61e9 5.52e8 Aggre… ## # … with 51 more rows, 5 more variables: capital &lt;chr&gt;, longitude &lt;chr&gt;, ## # latitude &lt;chr&gt;, income &lt;chr&gt;, lending &lt;chr&gt;, and abbreviated variable name ## # ¹​lastupdated wb_co2_world %&gt;% ggplot() + geom_line(aes(x = year, y = co2)) + labs(title = &quot;Co2 Emmision of the World&quot;) ## Warning: Removed 31 rows containing missing values (`geom_line()`). wb_co2_world %&gt;% ggplot() + geom_line(aes(x = year, y = gdp)) + labs(title = &quot;GDP (current US$)&quot;) wb_co2_world_extra &lt;- wb_co2_world %&gt;% mutate(diff_co2 = co2 - lag(co2), diff_gdp = gdp - lag(gdp)) wb_co2_world_extra ## # A tibble: 61 × 18 ## country iso2c iso3c year status lastup…¹ co2 gdp pop cereal region ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 World 1W WLD 1960 &quot;&quot; 2022-09… NA 1.39e12 3.03e9 NA Aggre… ## 2 World 1W WLD 1961 &quot;&quot; 2022-09… NA 1.45e12 3.07e9 5.15e8 Aggre… ## 3 World 1W WLD 1962 &quot;&quot; 2022-09… NA 1.55e12 3.12e9 5.17e8 Aggre… ## 4 World 1W WLD 1963 &quot;&quot; 2022-09… NA 1.67e12 3.19e9 5.24e8 Aggre… ## 5 World 1W WLD 1964 &quot;&quot; 2022-09… NA 1.83e12 3.26e9 5.32e8 Aggre… ## 6 World 1W WLD 1965 &quot;&quot; 2022-09… NA 1.99e12 3.32e9 5.30e8 Aggre… ## 7 World 1W WLD 1966 &quot;&quot; 2022-09… NA 2.16e12 3.39e9 5.35e8 Aggre… ## 8 World 1W WLD 1967 &quot;&quot; 2022-09… NA 2.30e12 3.46e9 5.47e8 Aggre… ## 9 World 1W WLD 1968 &quot;&quot; 2022-09… NA 2.48e12 3.53e9 5.52e8 Aggre… ## 10 World 1W WLD 1969 &quot;&quot; 2022-09… NA 2.74e12 3.61e9 5.52e8 Aggre… ## # … with 51 more rows, 7 more variables: capital &lt;chr&gt;, longitude &lt;chr&gt;, ## # latitude &lt;chr&gt;, income &lt;chr&gt;, lending &lt;chr&gt;, diff_co2 &lt;dbl&gt;, ## # diff_gdp &lt;dbl&gt;, and abbreviated variable name ¹​lastupdated wb_co2_world_extra %&gt;% ggplot(aes(x = year, y = diff_co2)) + geom_line() + labs(title = &quot;Changes of World CO2 emission&quot;) ## Warning: Removed 32 rows containing missing values (`geom_line()`). wb_co2_world_extra %&gt;% ggplot(aes(x = year, y = diff_gdp)) + geom_line() + labs(title = &quot;Changes of World GDP&quot;) ## Warning: Removed 1 row containing missing values (`geom_line()`). wb_co2_world_extra %&gt;% lm(diff_co2 ~ diff_gdp, .) %&gt;% summary() ## ## Call: ## lm(formula = diff_co2 ~ diff_gdp, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -707487 -230835 -30632 207767 839257 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.808e+05 7.780e+04 2.324 0.0279 * ## diff_gdp 1.306e-07 2.228e-08 5.863 3.04e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 321600 on 27 degrees of freedom ## ( 32 個の観測値が欠損のため削除されました ) ## Multiple R-squared: 0.5601, Adjusted R-squared: 0.5438 ## F-statistic: 34.37 on 1 and 27 DF, p-value: 3.039e-06 7.4 Regression 3: CO2 emission (Standardization) Prof. Kaizoji provided a data in csv format in Moodle and use scale function to standardize the data, let us proceed one step by one step. He starts with a WDI data in 2016 and use the aggregated part. There may be arguments on the set of data, let us use the same one. He defines variance in his slides, and the codes below uses unbiased variance. The difference is minor. wb_co2 ## # A tibble: 16,226 × 16 ## country iso2c iso3c year status lastu…¹ co2 gdp pop cereal region ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Afghani… AF AFG 2020 &quot;&quot; 2022-0… NA 2.01e10 3.89e7 3.04e6 South… ## 2 Afghani… AF AFG 2019 &quot;&quot; 2022-0… 6080. 1.88e10 3.80e7 2.64e6 South… ## 3 Afghani… AF AFG 2017 &quot;&quot; 2022-0… 4780. 1.88e10 3.63e7 2.42e6 South… ## 4 Afghani… AF AFG 1993 &quot;&quot; 2022-0… 1340 NA 1.58e7 2.63e6 South… ## 5 Afghani… AF AFG 1983 &quot;&quot; 2022-0… NA NA 1.25e7 2.65e6 South… ## 6 Afghani… AF AFG 2006 &quot;&quot; 2022-0… 1760. 6.97e 9 2.64e7 2.99e6 South… ## 7 Afghani… AF AFG 2018 &quot;&quot; 2022-0… 6070. 1.81e10 3.72e7 1.91e6 South… ## 8 Afghani… AF AFG 1981 &quot;&quot; 2022-0… NA 3.48e 9 1.32e7 2.91e6 South… ## 9 Afghani… AF AFG 2016 &quot;&quot; 2022-0… 5300. 1.81e10 3.54e7 2.79e6 South… ## 10 Afghani… AF AFG 1989 &quot;&quot; 2022-0… NA NA 1.19e7 2.29e6 South… ## # … with 16,216 more rows, 5 more variables: capital &lt;chr&gt;, longitude &lt;chr&gt;, ## # latitude &lt;chr&gt;, income &lt;chr&gt;, lending &lt;chr&gt;, and abbreviated variable name ## # ¹​lastupdated co2_ag16 &lt;- wb_co2 %&gt;% filter(year == 2016, region==&quot;Aggregates&quot;) %&gt;% select(iso2c, co2, gdp, pop, cereal) %&gt;% drop_na() co2_ag16 ## # A tibble: 42 × 5 ## iso2c co2 gdp pop cereal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ZH 578510. 8.82e11 609978946 54429019 ## 2 ZI 206030. 6.91e11 412551299 57557405 ## 3 1A 1838603. 2.50e12 404042892 29526231 ## 4 S3 37260. 6.95e10 7269385 251540 ## 5 B8 655580. 1.32e12 102994278 22398322 ## 6 V2 6962022. 1.05e13 3164439749 244663698 ## 7 Z4 14132393. 2.28e13 2307707227 180006919 ## 8 4E 11308839. 1.36e13 2062250022 159822459 ## 9 T4 11278720. 1.36e13 2036897092 158602892 ## 10 XC 2255170. 1.20e13 340481755 32425857 ## # … with 32 more rows co2_al16_std &lt;- co2_ag16 %&gt;% mutate(st_co2 = (co2 - mean(co2))/sd(co2), st_gdp = (gdp - mean(gdp))/sd(gdp), st_pop = (pop - mean(pop))/sd(pop), st_cereal = (cereal - mean(cereal))/sd(cereal)) co2_al16_std ## # A tibble: 42 × 9 ## iso2c co2 gdp pop cereal st_co2 st_gdp st_pop st_cereal ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ZH 578510. 8.82e11 609978946 54429019 -0.660 -0.642 -0.487 -0.546 ## 2 ZI 206030. 6.91e11 412551299 57557405 -0.708 -0.654 -0.593 -0.528 ## 3 1A 1838603. 2.50e12 404042892 29526231 -0.498 -0.541 -0.598 -0.692 ## 4 S3 37260. 6.95e10 7269385 251540 -0.730 -0.693 -0.812 -0.863 ## 5 B8 655580. 1.32e12 102994278 22398322 -0.651 -0.615 -0.760 -0.733 ## 6 V2 6962022. 1.05e13 3164439749 244663698 0.162 -0.0451 0.893 0.567 ## 7 Z4 14132393. 2.28e13 2307707227 180006919 1.09 0.723 0.430 0.189 ## 8 4E 11308839. 1.36e13 2062250022 159822459 0.723 0.152 0.298 0.0706 ## 9 T4 11278720. 1.36e13 2036897092 158602892 0.719 0.151 0.284 0.0635 ## 10 XC 2255170. 1.20e13 340481755 32425857 -0.444 0.0495 -0.632 -0.675 ## # … with 32 more rows co2_al16_scaled &lt;- co2_ag16 %&gt;% mutate(st_co2 = scale(co2), st_gdp = scale(gdp), st_pop = scale(pop), st_cereal = scale(cereal)) co2_al16_scaled ## # A tibble: 42 × 9 ## iso2c co2 gdp pop cereal st_co2…¹ st_gd…² st_po…³ st_ce…⁴ ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ZH 578510. 8.82e11 609978946 54429019 -0.660 -0.642 -0.487 -0.546 ## 2 ZI 206030. 6.91e11 412551299 57557405 -0.708 -0.654 -0.593 -0.528 ## 3 1A 1838603. 2.50e12 404042892 29526231 -0.498 -0.541 -0.598 -0.692 ## 4 S3 37260. 6.95e10 7269385 251540 -0.730 -0.693 -0.812 -0.863 ## 5 B8 655580. 1.32e12 102994278 22398322 -0.651 -0.615 -0.760 -0.733 ## 6 V2 6962022. 1.05e13 3164439749 244663698 0.162 -0.0451 0.893 0.567 ## 7 Z4 14132393. 2.28e13 2307707227 180006919 1.09 0.723 0.430 0.189 ## 8 4E 11308839. 1.36e13 2062250022 159822459 0.723 0.152 0.298 0.0706 ## 9 T4 11278720. 1.36e13 2036897092 158602892 0.719 0.151 0.284 0.0635 ## 10 XC 2255170. 1.20e13 340481755 32425857 -0.444 0.0495 -0.632 -0.675 ## # … with 32 more rows, and abbreviated variable names ¹​st_co2[,1], ²​st_gdp[,1], ## # ³​st_pop[,1], ⁴​st_cereal[,1] co2_ag16 %&gt;% select(-1) %&gt;% scale() %&gt;% as_tibble() ## # A tibble: 42 × 4 ## co2 gdp pop cereal ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.660 -0.642 -0.487 -0.546 ## 2 -0.708 -0.654 -0.593 -0.528 ## 3 -0.498 -0.541 -0.598 -0.692 ## 4 -0.730 -0.693 -0.812 -0.863 ## 5 -0.651 -0.615 -0.760 -0.733 ## 6 0.162 -0.0451 0.893 0.567 ## 7 1.09 0.723 0.430 0.189 ## 8 0.723 0.152 0.298 0.0706 ## 9 0.719 0.151 0.284 0.0635 ## 10 -0.444 0.0495 -0.632 -0.675 ## # … with 32 more rows co2_ag16_rv &lt;- read_csv(&quot;data/AGCO2rv.csv&quot;) ## Rows: 44 Columns: 4 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (4): EN.ATM.CO2E.KT, NY.GDP.MKTP.CD, SP.POP.TOTL, AG.LND.CREL.HA ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. co2_ag16_rv ## # A tibble: 44 × 4 ## EN.ATM.CO2E.KT NY.GDP.MKTP.CD SP.POP.TOTL AG.LND.CREL.HA ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1846601. 2.40e12 404042892 29143572 ## 2 32940650. 7.63e13 7433569330 733367261 ## 3 11254611. 1.36e13 2062232304 163412028 ## 4 2837305. 2.97e12 412538477 104861504 ## 5 2479230 2.93e12 1771187426 132269014 ## 6 656330 1.32e12 102994278 22398321 ## 7 2904100 1.39e13 445487730 54059756 ## 8 733332. 1.41e12 849739853 95574606 ## 9 11961830 4.84e13 1341701902 165097397 ## 10 228009. 4.45e11 39198032 1809678 ## # … with 34 more rows colnames(co2_ag16_rv) &lt;- c(&quot;st_co2&quot;, &quot;st_gdp&quot;, &quot;st_pop&quot;, &quot;st_cereal&quot;) co2_ag16_rv ## # A tibble: 44 × 4 ## st_co2 st_gdp st_pop st_cereal ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1846601. 2.40e12 404042892 29143572 ## 2 32940650. 7.63e13 7433569330 733367261 ## 3 11254611. 1.36e13 2062232304 163412028 ## 4 2837305. 2.97e12 412538477 104861504 ## 5 2479230 2.93e12 1771187426 132269014 ## 6 656330 1.32e12 102994278 22398321 ## 7 2904100 1.39e13 445487730 54059756 ## 8 733332. 1.41e12 849739853 95574606 ## 9 11961830 4.84e13 1341701902 165097397 ## 10 228009. 4.45e11 39198032 1809678 ## # … with 34 more rows 7.4.1 Regression co2_al16_scaled %&gt;% ggplot(aes(st_gdp, st_co2)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;CO2 emission and GDP: Standardized&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; co2_al16_scaled %&gt;% ggplot(aes(st_pop, st_co2)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;CO2 emission and Population: Standardized&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; co2_al16_scaled %&gt;% ggplot(aes(st_cereal, st_co2)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;CO2 emission and Land under cereal production: Standardized&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; st_co2_gdp &lt;- co2_al16_scaled %&gt;% lm(st_co2 ~ st_gdp, .) st_co2_pop &lt;- co2_al16_scaled %&gt;% lm(st_co2 ~ st_pop, .) st_co2_cereal &lt;- co2_al16_scaled %&gt;% lm(st_co2 ~ st_cereal, .) st_co2_gdp_pop &lt;- co2_al16_scaled %&gt;% lm(st_co2 ~ st_gdp + st_pop, .) st_co2_gdp_cereal &lt;- co2_al16_scaled %&gt;% lm(st_co2 ~ st_gdp + st_cereal, .) st_co2_pop_cereal &lt;- co2_al16_scaled %&gt;% lm(st_co2 ~ st_pop + st_cereal, .) st_co2_all &lt;- co2_al16_scaled %&gt;% lm(st_co2 ~ st_gdp + st_pop + st_cereal, .) msummary(list(st_gdp = st_co2_gdp, st_pop = st_co2_pop, st_cereal = st_co2_cereal, st_gdp_pop = st_co2_gdp_pop, st_gdp_cereal = st_co2_gdp_cereal, st_pop_cereal = st_co2_pop_cereal, st_all = st_co2_all)) st_gdp st_pop st_cereal st_gdp_pop st_gdp_cereal st_pop_cereal st_all (Intercept) 8e-17 2e-17 7e-17 5e-17 8e-17 6e-17 4e-17 (0.076) (0.064) (0.062) (0.038) (0.042) (0.062) (0.038) st_gdp 0.872 0.464 0.427 0.491 (0.077) (0.053) (0.063) (0.060) st_pop 0.911 0.590 0.142 0.829 (0.065) (0.053) (0.397) (0.257) st_cereal 0.919 0.605 0.779 -0.261 (0.062) (0.063) (0.397) (0.274) Num.Obs. 42 42 42 42 42 42 42 R2 0.761 0.830 0.845 0.942 0.928 0.845 0.944 R2 Adj. 0.755 0.826 0.841 0.939 0.924 0.837 0.939 AIC 64.1 49.8 46.0 6.4 15.6 47.9 7.4 BIC 69.3 55.0 51.2 13.4 22.6 54.8 16.1 Log.Lik. -29.067 -21.908 -19.997 0.784 -3.806 -19.928 1.278 RMSE 0.48 0.41 0.39 0.24 0.26 0.39 0.23 vif(st_co2_gdp_pop) ## st_gdp st_pop ## 1.916177 1.916177 vif(st_co2_gdp_cereal) ## st_gdp st_cereal ## 2.182838 2.182838 vif(st_co2_pop_cereal) ## st_pop st_cereal ## 39.64498 39.64498 vif(st_co2_all) ## st_gdp st_pop st_cereal ## 2.446917 44.441219 50.625778 7.4.2 Not Standardized co2_al16 &lt;- wb_co2 %&gt;% filter(region != &quot;Aggregates&quot;, year == 2016) co2_al16 ## # A tibble: 216 × 16 ## country iso2c iso3c year status lastu…¹ co2 gdp pop cereal ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan AF AFG 2016 &quot;&quot; 2022-0… 5300. 1.81e10 3.54e7 2.79e6 ## 2 Albania AL ALB 2016 &quot;&quot; 2022-0… 4480. 1.19e10 2.88e6 1.48e5 ## 3 Algeria DZ DZA 2016 &quot;&quot; 2022-0… 154910. 1.60e11 4.06e7 3.38e6 ## 4 American Sam… AS ASM 2016 &quot;&quot; 2022-0… NA 6.71e 8 5.57e4 NA ## 5 Andorra AD AND 2016 &quot;&quot; 2022-0… 470. 2.90e 9 7.73e4 NA ## 6 Angola AO AGO 2016 &quot;&quot; 2022-0… 29760. 4.98e10 2.88e7 2.73e6 ## 7 Antigua and … AG ATG 2016 &quot;&quot; 2022-0… 500 1.44e 9 9.45e4 3.5 e1 ## 8 Argentina AR ARG 2016 &quot;&quot; 2022-0… 183160. 5.58e11 4.36e7 1.18e7 ## 9 Armenia AM ARM 2016 &quot;&quot; 2022-0… 5070. 1.05e10 2.94e6 1.95e5 ## 10 Aruba AW ABW 2016 &quot;&quot; 2022-0… NA 2.98e 9 1.05e5 NA ## # … with 206 more rows, 6 more variables: region &lt;chr&gt;, capital &lt;chr&gt;, ## # longitude &lt;chr&gt;, latitude &lt;chr&gt;, income &lt;chr&gt;, lending &lt;chr&gt;, and ## # abbreviated variable name ¹​lastupdated 7.4.3 Regression co2_al16 %&gt;% ggplot(aes(gdp, co2)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;CO2 emission and GDP&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## Warning: Removed 30 rows containing non-finite values (`stat_smooth()`). ## Warning: Removed 30 rows containing missing values (`geom_point()`). co2_al16 %&gt;% ggplot(aes(pop, co2)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;CO2 emission and Population&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## Warning: Removed 27 rows containing non-finite values (`stat_smooth()`). ## Warning: Removed 27 rows containing missing values (`geom_point()`). co2_al16 %&gt;% ggplot(aes(cereal, co2)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;CO2 emission and Land under cereal production&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## Warning: Removed 41 rows containing non-finite values (`stat_smooth()`). ## Warning: Removed 41 rows containing missing values (`geom_point()`). orig_co2_gdp &lt;- wb_co2_18 %&gt;% lm(co2 ~ gdp, .) orig_co2_pop &lt;- wb_co2_18 %&gt;% lm(co2 ~ pop, .) orig_co2_cereal &lt;- wb_co2_18 %&gt;% lm(co2 ~ cereal, .) orig_co2_gdp_pop &lt;- wb_co2_18 %&gt;% lm(co2 ~ gdp + pop, .) orig_co2_gdp_cereal &lt;- wb_co2_18 %&gt;% lm(co2 ~ gdp + cereal, .) orig_co2_pop_cereal &lt;- wb_co2_18 %&gt;% lm(co2 ~ pop + cereal, .) orig_co2_all &lt;- wb_co2_18 %&gt;% lm(co2 ~ gdp + pop + cereal, .) msummary(list(orog_gdp = orig_co2_gdp, orig_pop = orig_co2_pop, orig_cereal = orig_co2_cereal, orig_gdp_pop = orig_co2_gdp_pop, orig_gdp_cereal = orig_co2_gdp_cereal, orig_pop_cereal = orig_co2_pop_cereal, orig_all = orig_co2_all)) orog_gdp orig_pop orig_cereal orig_gdp_pop orig_gdp_cereal orig_pop_cereal orig_all (Intercept) 3515.953 -15951.506 -63228.012 -55340.997 -73957.970 -52752.510 -55825.130 (35988.776) (38195.903) (40611.145) (24242.415) (30130.231) (40260.650) (26764.555) gdp 0.0000004 0.0000003 0.0000002 0.0000003 (2e-08) (1e-08) (2e-08) (2e-08) pop 0.005 0.003 0.002 0.003 (0.0003) (0.0002) (0.0007) (0.0005) cereal 0.062 0.036 0.039 -0.005 (0.003) (0.003) (0.009) (0.007) Num.Obs. 186 189 176 186 172 175 172 R2 0.708 0.666 0.684 0.871 0.835 0.697 0.872 R2 Adj. 0.706 0.664 0.682 0.870 0.833 0.694 0.870 AIC 5396.1 5505.5 5129.4 5245.3 4906.9 5095.6 4865.5 BIC 5405.8 5515.2 5138.9 5258.2 4919.5 5108.2 4881.2 Log.Lik. -2695.047 -2749.741 -2561.711 -2618.668 -2449.437 -2543.785 -2427.731 RMSE 474757.79 503810.82 506985.87 314872.13 370265.90 497313.02 326367.04 vif(orig_co2_gdp_pop) ## gdp pop ## 1.498289 1.498289 vif(orig_co2_gdp_cereal) ## gdp cereal ## 1.795285 1.795285 vif(orig_co2_pop_cereal) ## pop cereal ## 8.098126 8.098126 vif(orig_co2_all) ## gdp pop cereal ## 1.859018 8.401085 10.101136 7.4.3.1 Standadized (Again) msummary(list(st_gdp = st_co2_gdp, st_pop = st_co2_pop, st_cereal = st_co2_cereal, st_gdp_pop = st_co2_gdp_pop, st_gdp_cereal = st_co2_gdp_cereal, st_pop_cereal = st_co2_pop_cereal, st_all = st_co2_all)) st_gdp st_pop st_cereal st_gdp_pop st_gdp_cereal st_pop_cereal st_all (Intercept) 8e-17 2e-17 7e-17 5e-17 8e-17 6e-17 4e-17 (0.076) (0.064) (0.062) (0.038) (0.042) (0.062) (0.038) st_gdp 0.872 0.464 0.427 0.491 (0.077) (0.053) (0.063) (0.060) st_pop 0.911 0.590 0.142 0.829 (0.065) (0.053) (0.397) (0.257) st_cereal 0.919 0.605 0.779 -0.261 (0.062) (0.063) (0.397) (0.274) Num.Obs. 42 42 42 42 42 42 42 R2 0.761 0.830 0.845 0.942 0.928 0.845 0.944 R2 Adj. 0.755 0.826 0.841 0.939 0.924 0.837 0.939 AIC 64.1 49.8 46.0 6.4 15.6 47.9 7.4 BIC 69.3 55.0 51.2 13.4 22.6 54.8 16.1 Log.Lik. -29.067 -21.908 -19.997 0.784 -3.806 -19.928 1.278 RMSE 0.48 0.41 0.39 0.24 0.26 0.39 0.23 vif(st_co2_gdp_pop) ## st_gdp st_pop ## 1.916177 1.916177 vif(st_co2_gdp_cereal) ## st_gdp st_cereal ## 2.182838 2.182838 vif(st_co2_pop_cereal) ## st_pop st_cereal ## 39.64498 39.64498 vif(st_co2_all) ## st_gdp st_pop st_cereal ## 2.446917 44.441219 50.625778 7.4.4 We select regression model; CO2 emission = c + a (GDP) + b (population) (Omit land because of multicollinearity) 7.5 Regression of Categorical Variables and ANOVA If you start from here, you need to load the following packages. library(tidyverse) #tidyverse Package, a collection of packages for data science library(WDI) #WDI Package for World Development Indicators library(car) #VIF function library(modelsummary) #Table of the regression results wb_co2 ## # A tibble: 16,226 × 16 ## country iso2c iso3c year status lastu…¹ co2 gdp pop cereal region ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Afghani… AF AFG 2020 &quot;&quot; 2022-0… NA 2.01e10 3.89e7 3.04e6 South… ## 2 Afghani… AF AFG 2019 &quot;&quot; 2022-0… 6080. 1.88e10 3.80e7 2.64e6 South… ## 3 Afghani… AF AFG 2017 &quot;&quot; 2022-0… 4780. 1.88e10 3.63e7 2.42e6 South… ## 4 Afghani… AF AFG 1993 &quot;&quot; 2022-0… 1340 NA 1.58e7 2.63e6 South… ## 5 Afghani… AF AFG 1983 &quot;&quot; 2022-0… NA NA 1.25e7 2.65e6 South… ## 6 Afghani… AF AFG 2006 &quot;&quot; 2022-0… 1760. 6.97e 9 2.64e7 2.99e6 South… ## 7 Afghani… AF AFG 2018 &quot;&quot; 2022-0… 6070. 1.81e10 3.72e7 1.91e6 South… ## 8 Afghani… AF AFG 1981 &quot;&quot; 2022-0… NA 3.48e 9 1.32e7 2.91e6 South… ## 9 Afghani… AF AFG 2016 &quot;&quot; 2022-0… 5300. 1.81e10 3.54e7 2.79e6 South… ## 10 Afghani… AF AFG 1989 &quot;&quot; 2022-0… NA NA 1.19e7 2.29e6 South… ## # … with 16,216 more rows, 5 more variables: capital &lt;chr&gt;, longitude &lt;chr&gt;, ## # latitude &lt;chr&gt;, income &lt;chr&gt;, lending &lt;chr&gt;, and abbreviated variable name ## # ¹​lastupdated wb_co2_4anova &lt;- wb_co2 %&gt;% filter(year == 2016, region != &quot;Aggregates&quot;) %&gt;% select(iso2c, co2, gdp, income) %&gt;% drop_na() %&gt;% mutate(log_co2 = log10(co2), log_gdp = log10(gdp)) wb_co2_4anova ## # A tibble: 186 × 6 ## iso2c co2 gdp income log_co2 log_gdp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AF 5300. 1.81e10 Low income 3.72 10.3 ## 2 AL 4480. 1.19e10 Upper middle income 3.65 10.1 ## 3 DZ 154910. 1.60e11 Lower middle income 5.19 11.2 ## 4 AD 470. 2.90e 9 High income 2.67 9.46 ## 5 AO 29760. 4.98e10 Lower middle income 4.47 10.7 ## 6 AG 500 1.44e 9 High income 2.70 9.16 ## 7 AR 183160. 5.58e11 Upper middle income 5.26 11.7 ## 8 AM 5070. 1.05e10 Upper middle income 3.71 10.0 ## 9 AU 384990. 1.21e12 High income 5.59 12.1 ## 10 AT 63680. 3.96e11 High income 4.80 11.6 ## # … with 176 more rows CO2 &lt;- as_tibble(read.csv(&quot;CO2CAPITA.csv&quot;)) CO2 str(CO2) head(CO2) tail(CO2) wb_co2_4anova %&gt;% group_by(income) %&gt;% summarize(n = n_distinct(iso2c), co2_mean = mean(co2), gdp_mean = mean(gdp), log_co2_mean = mean(log_co2), log_gdp_mean = mean(log_gdp)) ## # A tibble: 4 × 6 ## income n co2_mean gdp_mean log_co2_mean log_gdp_mean ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 High income 56 217188. 857319296294. 4.46 11.1 ## 2 Low income 25 5053. 17841983559. 3.39 9.99 ## 3 Lower middle income 53 96503. 129620974752. 3.98 10.3 ## 4 Upper middle income 52 286264. 375800809797. 4.10 10.4 7.5.1 Regression y &lt;- CO2$EN.ATM.CO2E.PC #CO2 Emission per capita x &lt;- CO2$NY.GDP.PCAP.PP.CD/10000 #GDP per capita, PPP summary(x) summary(y) wb_co2_4anova %&gt;% ggplot(aes(log_gdp, log_co2)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;CO2 emission per capita and GDP per capita in log-log scale&quot;, x = &quot;GDP per capita&quot;, y = &quot;CO2 emission per capita&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; CO2 %&gt;% ggplot(aes(x, y)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;CO2 emission per capita and GDP per capita&quot;, x = &quot;GDP per capita&quot;, y = &quot;CO2 emission per capita&quot;) model_1 &lt;-lm(y ~ x, data=CO2) summary(model_1) msummary(model_1, statistic = &#39;p.value&#39;) wb_co2_4anova %&gt;% ggplot(aes(x = log_co2, fill = income)) + geom_histogram( ) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. wb_co2_4anova %&gt;% ggplot(aes(x = log_co2, y = income, fill = income)) + geom_boxplot() wb_co2_anova &lt;- wb_co2_4anova %&gt;% mutate(hi = as.numeric(income == &quot;High income&quot;)) wb_co2_model &lt;- wb_co2_anova %&gt;% lm(co2 ~ gdp + hi, .) wb_co2_model %&gt;% summary() ## ## Call: ## lm(formula = co2 ~ gdp + hi, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2775444 -69456 -66758 54673 5112814 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.850e+04 4.110e+04 1.667 0.09727 . ## gdp 4.178e-07 2.066e-08 20.221 &lt; 2e-16 *** ## hi -2.095e+05 7.570e+04 -2.768 0.00623 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 466100 on 183 degrees of freedom ## Multiple R-squared: 0.6912, Adjusted R-squared: 0.6878 ## F-statistic: 204.8 on 2 and 183 DF, p-value: &lt; 2.2e-16 wb_co2_log_model &lt;- wb_co2_anova %&gt;% lm(log_co2 ~ log_gdp + hi, .) wb_co2_log_model %&gt;% summary() ## ## Call: ## lm(formula = log_co2 ~ log_gdp + hi, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.97183 -0.19548 -0.02953 0.16620 0.69532 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6.72924 0.23113 -29.114 &lt; 2e-16 *** ## log_gdp 1.03559 0.02235 46.337 &lt; 2e-16 *** ## hi -0.26638 0.05022 -5.304 3.24e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2943 on 183 degrees of freedom ## Multiple R-squared: 0.9258, Adjusted R-squared: 0.925 ## F-statistic: 1142 on 2 and 183 DF, p-value: &lt; 2.2e-16 income_model &lt;- wb_co2_anova %&gt;% lm(co2 ~ factor(income), .) income_model %&gt;% summary() ## ## Call: ## lm(formula = co2 ~ factor(income), data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -286254 -215768 -96018 -3903 9588396 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 217188 111597 1.946 0.0532 . ## factor(income)Low income -212135 200875 -1.056 0.2923 ## factor(income)Lower middle income -120685 160040 -0.754 0.4518 ## factor(income)Upper middle income 69076 160829 0.429 0.6681 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 835100 on 182 degrees of freedom ## Multiple R-squared: 0.01392, Adjusted R-squared: -0.002335 ## F-statistic: 0.8563 on 3 and 182 DF, p-value: 0.4649 msummary(list(co2_model = wb_co2_model, co2_log_model = wb_co2_log_model, co2_income_model = income_model), statistic = &#39;p.value&#39;) co2_model co2_log_model co2_income_model (Intercept) 68502.943 -6.729 217187.858 (0.097) (&lt;0.001) (0.053) gdp 0.0000004 (&lt;0.001) hi -209509.183 -0.266 (0.006) (&lt;0.001) log_gdp 1.036 (&lt;0.001) factor(income)Low income -212135.058 (0.292) factor(income)Lower middle income -120685.215 (0.452) factor(income)Upper middle income 69075.992 (0.668) Num.Obs. 186 186 186 R2 0.691 0.926 0.014 R2 Adj. 0.688 0.925 -0.002 AIC 5388.2 77.7 5606.1 BIC 5401.1 90.7 5622.3 Log.Lik. -2690.100 -34.874 -2798.072 RMSE 462298.61 0.29 826089.87 7.5.1.1 t-test high &lt;- wb_co2_4anova %&gt;% filter(income == &quot;High income&quot;) %&gt;% pull(co2) low &lt;- wb_co2_4anova %&gt;% filter(income == &quot;Low income&quot;) %&gt;% pull(co2) t.test(high, low) ## ## Welch Two Sample t-test ## ## data: high and low ## t = 2.3527, df = 55.024, p-value = 0.02224 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 31435.4 392834.7 ## sample estimates: ## mean of x mean of y ## 217187.9 5052.8 7.5.2 Regression analysis with categorical variables model_3=lm(EN.ATM.CO2E.PC ~ factor(income), data = CO2) summary(model_3) # summary msummary(model_3, statistic = ‘p.value’) 7.5.3 ANOVA (Analysis of Variance) wb_co2_4anova %&gt;% aov(co2 ~ factor(income), .) %&gt;% summary() ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## factor(income) 3 1.792e+12 5.972e+11 0.856 0.465 ## Residuals 182 1.269e+14 6.974e+11 7.6 Week 7 Assignment Week 7: Assignment on regression analysis and PPDAC cycle Work on the following problem. Problem: Find one problem from the SGDs goal. Example: WDI environment https://datatopics.worldbank.org/world-development-indicators/themes/environment.html Plan: Analyze the data to find out why the problem is occurring. Data: Collect data from WDI and other sources (OECD, UN, IMF, etc.). Analysis: Analyze the data. Use what you have learned about regression analysis and descriptive statistics in the previous lectures. Conclusion: Briefly describe what you learn from the data analysis. Submission: Your rmd file (Rmarkdown) on Moodle box. *Deadline : Feb. 8. "],["topics2.html", "Chapter 8 Topics in Exploratory Data Analysis II 8.1 Classification IRIS dataset 8.2 Conclusions", " Chapter 8 Topics in Exploratory Data Analysis II Lecture by Prof. Keisuke Ishibashi and R Notebook Compiled by HS Project Classification Example: simple guessing by sepal length Example: classification by sepal length using glm Evaluation of classifications Example: classification with various combinations Evaluation Summary of Classification References 8.1 Classification IRIS dataset 8.1.1 Setup library(tidyverse) library(modelr) # a part of tidyverse package but not a core and need to load library(modelsummary) # handy comparison introduced by Prof. Kaizoji library(datasets) # attached as default 8.1.2 Data: iris We want to consider a problem to classify iris by species using the data provided. data(iris) # renew the data head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa Since it is easier to handle data and we add many columns later, we shorten the column names. iris_tbl &lt;- as_tibble(iris) #%&gt;% colnames(iris_tbl) &lt;- c(&quot;sl&quot;, &quot;sw&quot;, &quot;pl&quot;, &quot;pw&quot;, &quot;species&quot;) #rename(sl = Sepal.Length, sw = Sepal.Width, pl = Petal.Length, pw = Petal.Width, species = Species) iris_tbl ## # A tibble: 150 × 5 ## sl sw pl pw species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # … with 140 more rows Before we start, let us look at the basic imformation of the data. There are three kinds of species. iris_tbl %&gt;% summary() ## sl sw pl pw ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## 8.1.2.1 Visualization The following already tell something about species. iris_tbl %&gt;% ggplot(aes(pw,pl, color = species)) + geom_point() iris_tbl %&gt;% group_by(species) %&gt;% summarize(sl_mean = mean(sl), sw_mean = mean(sw), pl_mean = mean(pl), pw_mean = mean(pw)) ## # A tibble: 3 × 5 ## species sl_mean sw_mean pl_mean pw_mean ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 3.43 1.46 0.246 ## 2 versicolor 5.94 2.77 4.26 1.33 ## 3 virginica 6.59 2.97 5.55 2.03 The mean of setosa’s petal length is very small compared with others. So it may be possible to classify setosa by petal lenght. The following are a boxplot and a frequency polygon of the data by petal length. iris_tbl %&gt;% ggplot(aes(pl, fill = species)) + geom_boxplot() iris_tbl %&gt;% ggplot(aes(pl, color = species)) + geom_freqpoly() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. iris_tbl %&gt;% group_by(species) %&gt;% summarize(min_of_petal_lenght = min(pl), max_of_petal_lenght = max(pl)) ## # A tibble: 3 × 3 ## species min_of_petal_lenght max_of_petal_lenght ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 1 1.9 ## 2 versicolor 3 5.1 ## 3 virginica 4.5 6.9 Hence if we set the threshhold to be around 2.5, we can separate setosa. iris_tbl2 &lt;- iris_tbl %&gt;% filter(pl &gt;= 2.5) summary(iris_tbl2) ## sl sw pl pw ## Min. :4.900 Min. :2.000 Min. :3.000 Min. :1.000 ## 1st Qu.:5.800 1st Qu.:2.700 1st Qu.:4.375 1st Qu.:1.300 ## Median :6.300 Median :2.900 Median :4.900 Median :1.600 ## Mean :6.262 Mean :2.872 Mean :4.906 Mean :1.676 ## 3rd Qu.:6.700 3rd Qu.:3.025 3rd Qu.:5.525 3rd Qu.:2.000 ## Max. :7.900 Max. :3.800 Max. :6.900 Max. :2.500 ## species ## setosa : 0 ## versicolor:50 ## virginica :50 ## ## ## Since it looks difficult to classify versicolor and virginica only by petal length, let us look at other variables. iris_tbl %&gt;% ggplot(aes(sl,pl, color = species)) + geom_point() 8.1.2.2 Sepal Length You can find different features of charts, boxplots and frequency polygons of sepal length. iris_tbl2 %&gt;% ggplot(aes(sl, fill = species)) + geom_boxplot() iris_tbl2 %&gt;% ggplot(aes(sl, color = species)) + geom_freqpoly() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 8.1.3 Binary classification Let us set a numerical marker 1 to virginica. iris_tbl2ext &lt;- iris_tbl2 %&gt;% mutate(virginica = as.numeric(species == &#39;virginica&#39;)) iris_tbl2ext %&gt;% arrange(-virginica) ## # A tibble: 100 × 6 ## sl sw pl pw species virginica ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 6.3 3.3 6 2.5 virginica 1 ## 2 5.8 2.7 5.1 1.9 virginica 1 ## 3 7.1 3 5.9 2.1 virginica 1 ## 4 6.3 2.9 5.6 1.8 virginica 1 ## 5 6.5 3 5.8 2.2 virginica 1 ## 6 7.6 3 6.6 2.1 virginica 1 ## 7 4.9 2.5 4.5 1.7 virginica 1 ## 8 7.3 2.9 6.3 1.8 virginica 1 ## 9 6.7 2.5 5.8 1.8 virginica 1 ## 10 7.2 3.6 6.1 2.5 virginica 1 ## # … with 90 more rows 8.1.3.1 Manual Classificaiton Using Boxplot iris_tbl2ext %&gt;% group_by(species) %&gt;% summarize(min = quantile(sl)[1], `25%` = quantile(sl)[2], `55%` = quantile(sl)[3], `75%` = quantile(sl)[4], max = quantile(sl)[5]) ## # A tibble: 2 × 6 ## species min `25%` `55%` `75%` max ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 versicolor 4.9 5.6 5.9 6.3 7 ## 2 virginica 4.9 6.22 6.5 6.9 7.9 By the boxplot, the value between 6.225 and 6.3 can be a candidate. So let us take 6.263. iris_tbl2ext &lt;- iris_tbl2ext %&gt;% mutate(v0 = as.integer(sl &gt; 6.263)) iris_tbl2ext ## # A tibble: 100 × 7 ## sl sw pl pw species virginica v0 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; ## 1 7 3.2 4.7 1.4 versicolor 0 1 ## 2 6.4 3.2 4.5 1.5 versicolor 0 1 ## 3 6.9 3.1 4.9 1.5 versicolor 0 1 ## 4 5.5 2.3 4 1.3 versicolor 0 0 ## 5 6.5 2.8 4.6 1.5 versicolor 0 1 ## 6 5.7 2.8 4.5 1.3 versicolor 0 0 ## 7 6.3 3.3 4.7 1.6 versicolor 0 1 ## 8 4.9 2.4 3.3 1 versicolor 0 0 ## 9 6.6 2.9 4.6 1.3 versicolor 0 1 ## 10 5.2 2.7 3.9 1.4 versicolor 0 0 ## # … with 90 more rows 8.1.3.2 Logistic Model Using glm() General Linear Model: Sepal Length iris_mod1 &lt;-iris_tbl2ext %&gt;% glm(virginica~sl, family = binomial, .) iris_mod1 %&gt;% summary() ## ## Call: ## glm(formula = virginica ~ sl, family = binomial, data = .) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.85340 -0.90001 -0.04717 0.96861 2.35458 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -12.5708 2.9068 -4.325 1.53e-05 *** ## sl 2.0129 0.4654 4.325 1.53e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 138.63 on 99 degrees of freedom ## Residual deviance: 110.55 on 98 degrees of freedom ## AIC: 114.55 ## ## Number of Fisher Scoring iterations: 4 iris_tbl2ext_1 &lt;- iris_tbl2ext %&gt;% add_predictions(iris_mod1, var = &quot;pred_1&quot;, type = &quot;response&quot;) %&gt;% add_residuals(iris_mod1, var = &quot;resid_1&quot;) %&gt;% mutate(v1 = as.integer(pred_1 &gt;= 0.5)) iris_tbl2ext_1 %&gt;% arrange(desc(sl)) ## # A tibble: 100 × 10 ## sl sw pl pw species virginica v0 pred_1 resid_1 v1 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 7.9 3.8 6.4 2 virginica 1 1 0.965 -2.33 1 ## 2 7.7 3.8 6.7 2.2 virginica 1 1 0.949 -1.93 1 ## 3 7.7 2.6 6.9 2.3 virginica 1 1 0.949 -1.93 1 ## 4 7.7 2.8 6.7 2 virginica 1 1 0.949 -1.93 1 ## 5 7.7 3 6.1 2.3 virginica 1 1 0.949 -1.93 1 ## 6 7.6 3 6.6 2.1 virginica 1 1 0.939 -1.73 1 ## 7 7.4 2.8 6.1 1.9 virginica 1 1 0.911 -1.32 1 ## 8 7.3 2.9 6.3 1.8 virginica 1 1 0.893 -1.12 1 ## 9 7.2 3.6 6.1 2.5 virginica 1 1 0.872 -0.922 1 ## 10 7.2 3.2 6 1.8 virginica 1 1 0.872 -0.922 1 ## # … with 90 more rows iris_tbl2ext_1 %&gt;% ggplot() + geom_point(aes(x = sl, y = virginica, color = species)) + geom_line(aes(x = sl, y = pred_1)) iris_tbl2ext_1 %&gt;% filter(v0 == 1, virginica == 1) %&gt;% nrow()/50 ## [1] 0.74 iris_tbl2ext_1 %&gt;% filter(v1 == 1, virginica == 1) %&gt;% nrow()/50 ## [1] 0.74 For both models, 74% of virginica was classified as virginica. 8.1.3.3 Confusion Matrices iris_tbl2ext_1 %&gt;% xtabs(~ v0 + virginica, .) ## virginica ## v0 0 1 ## 0 36 13 ## 1 14 37 iris_tbl2ext_1 %&gt;% xtabs(~ v1 + virginica, .) ## virginica ## v1 0 1 ## 0 36 13 ## 1 14 37 8.1.4 Various Logistic Models 8.1.4.1 One Variable iris_mod2 &lt;-iris_tbl2ext %&gt;% glm(virginica~sw, family = binomial, .) iris_mod3 &lt;-iris_tbl2ext %&gt;% glm(virginica~pl, family = binomial, .) iris_mod4 &lt;-iris_tbl2ext %&gt;% glm(virginica~pw, family = binomial, .) iris_tbl2ext_one &lt;- iris_tbl2ext_1 %&gt;% add_predictions(iris_mod2, var = &quot;pred_2&quot;, type = &quot;response&quot;) %&gt;% mutate(v2 = as.integer(pred_2 &gt;= 0.5)) %&gt;% add_predictions(iris_mod3, var = &quot;pred_3&quot;, type = &quot;response&quot;) %&gt;% mutate(v3 = as.integer(pred_3 &gt;= 0.5)) %&gt;% add_predictions(iris_mod4, var = &quot;pred_4&quot;, type = &quot;response&quot;) %&gt;% mutate(v4 = as.integer(pred_4 &gt;= 0.5)) iris_tbl2ext_one %&gt;% arrange(desc(sl)) ## # A tibble: 100 × 16 ## sl sw pl pw species virginica v0 pred_1 resid_1 v1 pred_2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 7.9 3.8 6.4 2 virginica 1 1 0.965 -2.33 1 0.874 ## 2 7.7 3.8 6.7 2.2 virginica 1 1 0.949 -1.93 1 0.874 ## 3 7.7 2.6 6.9 2.3 virginica 1 1 0.949 -1.93 1 0.362 ## 4 7.7 2.8 6.7 2 virginica 1 1 0.949 -1.93 1 0.462 ## 5 7.7 3 6.1 2.3 virginica 1 1 0.949 -1.93 1 0.566 ## 6 7.6 3 6.6 2.1 virginica 1 1 0.939 -1.73 1 0.566 ## 7 7.4 2.8 6.1 1.9 virginica 1 1 0.911 -1.32 1 0.462 ## 8 7.3 2.9 6.3 1.8 virginica 1 1 0.893 -1.12 1 0.515 ## 9 7.2 3.6 6.1 2.5 virginica 1 1 0.872 -0.922 1 0.821 ## 10 7.2 3.2 6 1.8 virginica 1 1 0.872 -0.922 1 0.665 ## # … with 90 more rows, and 5 more variables: v2 &lt;int&gt;, pred_3 &lt;dbl&gt;, v3 &lt;int&gt;, ## # pred_4 &lt;dbl&gt;, v4 &lt;int&gt; iris_tbl2ext_one %&gt;% filter(v2 == 1, virginica == 1) %&gt;% nrow()/50 ## [1] 0.62 iris_tbl2ext_one %&gt;% filter(v3 == 1, virginica == 1) %&gt;% nrow()/50 ## [1] 0.94 iris_tbl2ext_one %&gt;% filter(v4 == 1, virginica == 1) %&gt;% nrow()/50 ## [1] 0.92 8.1.4.2 Model Summary msummary(list(mod1 = iris_mod1, mod2 = iris_mod2, mod3 = iris_mod3, mod4 = iris_mod3), statistic = &#39;p.value&#39;) mod1 mod2 mod3 mod4 (Intercept) -12.571 -6.001 -43.781 -43.781 (&lt;0.001) (0.004) (&lt;0.001) (&lt;0.001) sl 2.013 (&lt;0.001) sw 2.089 (0.003) pl 9.002 9.002 (&lt;0.001) (&lt;0.001) Num.Obs. 100 100 100 100 AIC 114.5 132.6 37.4 37.4 BIC 119.8 137.8 42.6 42.6 Log.Lik. -55.273 -64.290 -16.716 -16.716 RMSE 0.43 0.48 0.22 0.22 8.1.4.3 Two Variables iris_mod12 &lt;-iris_tbl2ext %&gt;% glm(virginica~sl + sw, family = binomial, .) iris_mod13 &lt;-iris_tbl2ext %&gt;% glm(virginica~sl + pl, family = binomial, .) iris_mod14 &lt;-iris_tbl2ext %&gt;% glm(virginica~sl + pw, family = binomial, .) iris_mod23 &lt;-iris_tbl2ext %&gt;% glm(virginica~sw + pl, family = binomial, .) iris_mod24 &lt;-iris_tbl2ext %&gt;% glm(virginica~sw + pw, family = binomial, .) iris_mod34 &lt;-iris_tbl2ext %&gt;% glm(virginica~pl + pw, family = binomial, .) iris_tbl2ext_two &lt;- iris_tbl2ext_one %&gt;% add_predictions(iris_mod12, var = &quot;pred_12&quot;, type = &quot;response&quot;) %&gt;% mutate(v12 = as.integer(pred_12 &gt;= 0.5)) %&gt;% add_predictions(iris_mod13, var = &quot;pred_13&quot;, type = &quot;response&quot;) %&gt;% mutate(v13 = as.integer(pred_13 &gt;= 0.5)) %&gt;% add_predictions(iris_mod14, var = &quot;pred_14&quot;, type = &quot;response&quot;) %&gt;% mutate(v14 = as.integer(pred_14 &gt;= 0.5)) %&gt;% add_predictions(iris_mod23, var = &quot;pred_23&quot;, type = &quot;response&quot;) %&gt;% mutate(v23 = as.integer(pred_23 &gt;= 0.5)) %&gt;% add_predictions(iris_mod24, var = &quot;pred_24&quot;, type = &quot;response&quot;) %&gt;% mutate(v24 = as.integer(pred_24 &gt;= 0.5)) %&gt;% add_predictions(iris_mod34, var = &quot;pred_34&quot;, type = &quot;response&quot;) %&gt;% mutate(v34 = as.integer(pred_34 &gt;= 0.5)) iris_tbl2ext_two %&gt;% arrange(desc(sl)) ## # A tibble: 100 × 28 ## sl sw pl pw species virginica v0 pred_1 resid_1 v1 pred_2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 7.9 3.8 6.4 2 virginica 1 1 0.965 -2.33 1 0.874 ## 2 7.7 3.8 6.7 2.2 virginica 1 1 0.949 -1.93 1 0.874 ## 3 7.7 2.6 6.9 2.3 virginica 1 1 0.949 -1.93 1 0.362 ## 4 7.7 2.8 6.7 2 virginica 1 1 0.949 -1.93 1 0.462 ## 5 7.7 3 6.1 2.3 virginica 1 1 0.949 -1.93 1 0.566 ## 6 7.6 3 6.6 2.1 virginica 1 1 0.939 -1.73 1 0.566 ## 7 7.4 2.8 6.1 1.9 virginica 1 1 0.911 -1.32 1 0.462 ## 8 7.3 2.9 6.3 1.8 virginica 1 1 0.893 -1.12 1 0.515 ## 9 7.2 3.6 6.1 2.5 virginica 1 1 0.872 -0.922 1 0.821 ## 10 7.2 3.2 6 1.8 virginica 1 1 0.872 -0.922 1 0.665 ## # … with 90 more rows, and 17 more variables: v2 &lt;int&gt;, pred_3 &lt;dbl&gt;, v3 &lt;int&gt;, ## # pred_4 &lt;dbl&gt;, v4 &lt;int&gt;, pred_12 &lt;dbl&gt;, v12 &lt;int&gt;, pred_13 &lt;dbl&gt;, v13 &lt;int&gt;, ## # pred_14 &lt;dbl&gt;, v14 &lt;int&gt;, pred_23 &lt;dbl&gt;, v23 &lt;int&gt;, pred_24 &lt;dbl&gt;, ## # v24 &lt;int&gt;, pred_34 &lt;dbl&gt;, v34 &lt;int&gt; iris_tbl2ext_two %&gt;% filter(v12 == 1, virginica == 1) %&gt;% nrow()/50 ## [1] 0.74 iris_tbl2ext_two %&gt;% filter(v13 == 1, virginica == 1) %&gt;% nrow()/50 ## [1] 0.96 iris_tbl2ext_two %&gt;% filter(v14 == 1, virginica == 1) %&gt;% nrow()/50 ## [1] 0.92 iris_tbl2ext_two %&gt;% filter(v23 == 1, virginica == 1) %&gt;% nrow()/50 ## [1] 0.92 iris_tbl2ext_two %&gt;% filter(v24 == 1, virginica == 1) %&gt;% nrow()/50 ## [1] 0.94 iris_tbl2ext_two %&gt;% filter(v34 == 1, virginica == 1) %&gt;% nrow()/50 ## [1] 0.94 8.1.4.4 Model Summary msummary(list(mod12 = iris_mod12, mod13 = iris_mod13, mod14 = iris_mod14, mod23 = iris_mod23, mod24 = iris_mod24, mod34 = iris_mod34), statistic = &#39;p.value&#39;) mod12 mod13 mod14 mod23 mod24 mod34 (Intercept) -13.046 -39.839 -22.874 -37.058 -14.379 -45.272 (&lt;0.001) (0.002) (&lt;0.001) (&lt;0.001) (0.005) (&lt;0.001) sl 1.902 -4.017 0.306 (&lt;0.001) (0.013) (0.715) sw 0.405 -2.509 -3.907 (0.639) (0.184) (0.025) pl 13.313 9.098 5.755 (&lt;0.001) (&lt;0.001) (0.013) pw 12.845 15.700 10.447 (&lt;0.001) (&lt;0.001) (0.005) Num.Obs. 100 100 100 100 100 100 AIC 116.3 29.9 39.3 37.5 33.4 26.6 BIC 124.1 37.7 47.1 45.3 41.2 34.4 Log.Lik. -55.163 -11.925 -16.643 -15.756 -13.700 -10.282 RMSE 0.43 0.18 0.22 0.22 0.21 0.19 8.1.4.5 Three and All Variables iris_mod123 &lt;-iris_tbl2ext %&gt;% glm(virginica~sl + sw + pl, family = binomial, .) iris_mod124 &lt;-iris_tbl2ext %&gt;% glm(virginica~sl + sw + pw, family = binomial, .) iris_mod134 &lt;-iris_tbl2ext %&gt;% glm(virginica~sl + pl + pw, family = binomial, .) iris_mod234 &lt;-iris_tbl2ext %&gt;% glm(virginica~sw + pl + pw, family = binomial, .) ## Warning: glm.fit: 数値的に 0 か 1 である確率が生じました iris_mod1234 &lt;-iris_tbl2ext %&gt;% glm(virginica~sl + sw + pl + pw, family = binomial, .) iris_tbl2ext_all &lt;- iris_tbl2ext_two %&gt;% add_predictions(iris_mod123, var = &quot;pred_123&quot;, type = &quot;response&quot;) %&gt;% mutate(v123 = as.integer(pred_123 &gt;= 0.5)) %&gt;% add_predictions(iris_mod124, var = &quot;pred_124&quot;, type = &quot;response&quot;) %&gt;% mutate(v124 = as.integer(pred_124 &gt;= 0.5)) %&gt;% add_predictions(iris_mod134, var = &quot;pred_134&quot;, type = &quot;response&quot;) %&gt;% mutate(v134 = as.integer(pred_134 &gt;= 0.5)) %&gt;% add_predictions(iris_mod234, var = &quot;pred_234&quot;, type = &quot;response&quot;) %&gt;% mutate(v234 = as.integer(pred_234 &gt;= 0.5)) %&gt;% add_predictions(iris_mod1234, var = &quot;pred_1234&quot;, type = &quot;response&quot;) %&gt;% mutate(v1234 = as.integer(pred_1234 &gt;= 0.5)) iris_tbl2ext_all %&gt;% arrange(desc(sl)) ## # A tibble: 100 × 38 ## sl sw pl pw species virginica v0 pred_1 resid_1 v1 pred_2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 7.9 3.8 6.4 2 virginica 1 1 0.965 -2.33 1 0.874 ## 2 7.7 3.8 6.7 2.2 virginica 1 1 0.949 -1.93 1 0.874 ## 3 7.7 2.6 6.9 2.3 virginica 1 1 0.949 -1.93 1 0.362 ## 4 7.7 2.8 6.7 2 virginica 1 1 0.949 -1.93 1 0.462 ## 5 7.7 3 6.1 2.3 virginica 1 1 0.949 -1.93 1 0.566 ## 6 7.6 3 6.6 2.1 virginica 1 1 0.939 -1.73 1 0.566 ## 7 7.4 2.8 6.1 1.9 virginica 1 1 0.911 -1.32 1 0.462 ## 8 7.3 2.9 6.3 1.8 virginica 1 1 0.893 -1.12 1 0.515 ## 9 7.2 3.6 6.1 2.5 virginica 1 1 0.872 -0.922 1 0.821 ## 10 7.2 3.2 6 1.8 virginica 1 1 0.872 -0.922 1 0.665 ## # … with 90 more rows, and 27 more variables: v2 &lt;int&gt;, pred_3 &lt;dbl&gt;, v3 &lt;int&gt;, ## # pred_4 &lt;dbl&gt;, v4 &lt;int&gt;, pred_12 &lt;dbl&gt;, v12 &lt;int&gt;, pred_13 &lt;dbl&gt;, v13 &lt;int&gt;, ## # pred_14 &lt;dbl&gt;, v14 &lt;int&gt;, pred_23 &lt;dbl&gt;, v23 &lt;int&gt;, pred_24 &lt;dbl&gt;, ## # v24 &lt;int&gt;, pred_34 &lt;dbl&gt;, v34 &lt;int&gt;, pred_123 &lt;dbl&gt;, v123 &lt;int&gt;, ## # pred_124 &lt;dbl&gt;, v124 &lt;int&gt;, pred_134 &lt;dbl&gt;, v134 &lt;int&gt;, pred_234 &lt;dbl&gt;, ## # v234 &lt;int&gt;, pred_1234 &lt;dbl&gt;, v1234 &lt;int&gt; iris_tbl2ext_all %&gt;% filter(v123 == 1, virginica == 1) %&gt;% nrow()/50 ## [1] 0.96 iris_tbl2ext_all %&gt;% filter(v124 == 1, virginica == 1) %&gt;% nrow()/50 ## [1] 0.96 iris_tbl2ext_all %&gt;% filter(v134 == 1, virginica == 1) %&gt;% nrow()/50 ## [1] 0.98 iris_tbl2ext_all %&gt;% filter(v234 == 1, virginica == 1) %&gt;% nrow()/50 ## [1] 0.98 iris_tbl2ext_all %&gt;% filter(v1234 == 1, virginica == 1) %&gt;% nrow()/50 ## [1] 0.98 8.1.4.6 Confusion Matrices iris_tbl2ext_all %&gt;% xtabs(~ v134 + virginica, .) ## virginica ## v134 0 1 ## 0 48 1 ## 1 2 49 iris_tbl2ext_all %&gt;% xtabs(~ v234 + virginica, .) ## virginica ## v234 0 1 ## 0 48 1 ## 1 2 49 iris_tbl2ext_all %&gt;% xtabs(~ v1234 + virginica, .) ## virginica ## v1234 0 1 ## 0 49 1 ## 1 1 49 8.1.4.7 Model Summary msummary(list(mod123 = iris_mod123, mod124 = iris_mod124, mod134 = iris_mod134, mod234 = iris_mod234, mod1234 = iris_mod1234), statistic = &#39;p.value&#39;) mod123 mod124 mod134 mod234 mod1234 (Intercept) -38.215 -20.287 -40.831 -50.527 -42.638 (0.007) (0.012) (0.028) (0.035) (0.097) sl -3.852 1.295 -3.839 -2.465 (0.024) (0.234) (0.071) (0.303) sw -0.639 -4.823 -8.376 -6.681 (0.780) (0.021) (0.079) (0.136) pl 13.147 9.754 7.875 9.429 (&lt;0.001) (0.029) (0.040) (0.047) pw 15.923 10.102 21.430 18.286 (&lt;0.001) (0.028) (0.045) (0.061) Num.Obs. 100 100 100 100 100 AIC 31.8 33.9 23.5 21.3 21.9 BIC 42.2 44.3 33.9 31.7 34.9 Log.Lik. -11.886 -12.951 -7.746 -6.633 -5.949 RMSE 0.18 0.20 0.16 0.15 0.14 iris_tbl2ext_all %&gt;% select(virginica, v0, v1, v2, v3, v4, v12, v13, v14, v23, v24, v34, v123, v124, v134, v234, v1234) ## # A tibble: 100 × 17 ## virginica v0 v1 v2 v3 v4 v12 v13 v14 v23 v24 v34 ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 0 1 1 1 0 0 1 0 0 0 0 0 ## 2 0 1 1 1 0 0 1 0 0 0 0 0 ## 3 0 1 1 1 1 0 1 0 0 0 0 0 ## 4 0 0 0 0 0 0 0 0 0 0 0 0 ## 5 0 1 1 0 0 0 1 0 0 0 0 0 ## 6 0 0 0 0 0 0 0 0 0 0 0 0 ## 7 0 1 1 1 0 0 1 0 0 0 0 0 ## 8 0 0 0 0 0 0 0 0 0 0 0 0 ## 9 0 1 1 1 0 0 1 0 0 0 0 0 ## 10 0 0 0 0 0 0 0 0 0 0 0 0 ## # … with 90 more rows, and 5 more variables: v123 &lt;int&gt;, v124 &lt;int&gt;, ## # v134 &lt;int&gt;, v234 &lt;int&gt;, v1234 &lt;int&gt; 8.2 Conclusions 8.2.1 Hypothesis generation vs. hypothesis confirmation Each observation can either be used for exploration or confirmation, not both. You can use an observation as many times as you like for exploration, but you can only use it once for confirmation. As soon as you use an observation twice, you’ve switched from confirmation to exploration. If you are serious about doing an confirmatory analysis, one approach is to split your data into three pieces before you begin the analysis: 60% of your data goes into a training (or exploration) set. You’re allowed to do anything you like with this data: visualise it and fit tons of models to it. 20% goes into a query set. You can use this data to compare models or visualisations by hand, but you’re not allowed to use it as part of an automated process. 20% is held back for a test set. You can only use this data ONCE, to test your final model. 8.2.2 References Iris Classification: https://github.com/trevorwitter/Iris-classification-R "],["guest.html", "Chapter 9 Guest Lecture and/or Examples 9.1 Guest Lecture by Zheng Nan 9.2 Topics of EDA 9.3 Part II. 2022-02-16 Presentation", " Chapter 9 Guest Lecture and/or Examples 9.1 Guest Lecture by Zheng Nan 9.1.1 Exploring the bitcoin market efficiency using R Data, methodology, and interpretations Opening: Challenges in data analysis Collecting data, defining data, and preprocessing data. Methodology and models Interpretations of the results Background Bitcoin Efficient Market Hypothesis (EMH) Random Walk Hypothesis Augmented Dickey-Fuller (ADF) test Bitcoin exchange (BX) rate Methodologies Practice - R Script and Data are in Moodle Results 9.2 Topics of EDA 9.2.1 Contents Financial Data Sites and Packages: Quandl, quanmod Stock Market and Crypt Currency Model Analysis Linear Regression Review Stratification and Confounder R squared and p-value About Sixth Assignment - in Moodle Presentation on 2021-02-17 and 2021-02-24 The Eighth Assignment 9.2.2 Financial Data 9.2.2.1 Quandl Quandl R package: https://cran.r-project.org/web/packages/Quandl/Quandl.pdf Quandl: Bitcoin Market Price USD 9.2.2.2 Quantmod quantmod: Quantitative Financial Modelling &amp; Trading Framework for R R package `quantmod: https://cran.r-project.org/web/packages/quantmod/quantmod.pdf quantmod R documentation Yahoo Finance Data Using quantmod Reference: CryptCurrency Bitcoin Analysis Using quantmod 9.2.3 Your Course Project, Part I All documents must contain ‘ID’, ‘Name’, ‘Date’ of submission ‘Short Paper’ (that can be much longer than the Paper Due: 2021-02-09 for Interim Report (and 2021-03-03: supporting doc for Paper) Contents: Objective: What and Why Data Reproducible Exploratory Data Analysis with Explanations Questions based on your findings and technical quesitons Format: R Notebook (*.nb.html) Presentation: 10 minutes (5-7 min. presentation and 3-5 min. QA) On 2021-02-17 [or 2021-02-24, a reserve] With a digital file (.nb.html, html, pdf, word, ppt, … ) by file share Note: Be ready to show your codes by R Notebook or R Scripts, when requested 9.2.4 Your Course Project, Part II Paper: 5 to 10 pages Due: 2021-03-03 Contents: Exploratory Data Analysis Using Public Data Introduction - include what and why Description of Data Exposition of Your Exploration with Visualization of Data Concluding Remarks References, if any Acknowledgements, if any (can give a credit to your classmate) Note: Give logical explanations of your observations using data tables and charts No need to include the whole process Include codes only when necessary Format: pdf. (Rmd &gt; pdf, Rmd &gt; MS Word &gt; pdf, Rmd &gt; MS Word &gt; Google Doc &gt; pdf) 9.2.5 Presentation Format 9.2.5.1 10 minutes (5-7 min. presentation and 3-5 min. QA) On 2021-02-17 and 2021-02-24 Revised: 15 minutes (7-10 min. presentation and 5-8 min. QA) With a digital file (.nb.html, html, pdf, word, ppt, … ) by file share Note: Be ready to show your codes by R Notebook or R Scripts, when requested 9.2.6 The Eighth Assignment (in Moodle) A. Give your feedback to your classmates’ posts on the Forum, Seventh Assignment as [Reply] to keep in a thread. Comments on the project? Write your questions related to the topic? B. Add explanations or responses to your topic. Add questions to investigate. Share the difficulties you are facing. Respond to the comments of your classmates. C. Option: Share the link(s) to your R Notebook(s) in RStudio.cloud Submit your response to Moodle (The Seventh Assignment) by 2021-02-16 23:59:00 9.2.7 Learning Resources, VIII R for Data Science, Part III Wrangle - Tidy and Relational R for Data Science, Part IV Model 9.2.7.1 RStudio Primers: See References in Moodle at the bottom The Basics – r4ds: Explore, I Work with Data – r4ds: Wrangle, I Visualize Data – r4ds: Explore, II Exploratory Data Analysis, Bar Charts, Histograms Boxplots and Counts, Scatterplots, Line Plots Overplotting and Big Data, Customize Your Plots Tidy Your Data – r4ds: Wrangle, II Reshape Data, Separate and Unite Columns, Join Data Sets 9.3 Part II. 2022-02-16 Presentation Presentation 10 minutes (5-7 min. presentation and 3-5 min. QA) On 2021-02-17 and 2021-02-24 Revised: 15 minutes (7-10 min. presentation and 5-8 min. QA) With a digital file (.nb.html, html, pdf, word, ppt, … ) by file share Note: Be ready to show your codes by R Notebook or R Scripts, when requested Share Your Exploratory Data Analysis Experience Presentation Example: Motivation and Questions: What and Why About the Data Findings Problems and Difficulties What Remains to be Done; Questions and/or Analysis 9.3.1 The Ninth Assignment (in Moodle) Write your feedback to each of your classmates’ presentation on the Forum in Moodle. First, include the name of the presenter and/or the title of the presentation on the topic of your post Write on the following Comments on the presentation, Suggestions on the project, or Technical or general questions related to the presentation You can write comments on your own presentation, and/or responses to comments. Submit your response to Moodle (The Ninth Assignment) by 2021-02-23 23:59:00 9.3.2 Learning Resources, IX R for Data Science, Part III Wrangle R for Data Science, Part VI Communicate Data analysis write-ups Structure of a Data Analysis Report 9.3.2.1 RStudio Primers: See References in Moodle at the bottom The Basics – r4ds: Explore, I Work with Data – r4ds: Wrangle, I Visualize Data – r4ds: Explore, II Exploratory Data Analysis, Bar Charts, Histograms Boxplots and Counts, Scatterplots, Line Plots Overplotting and Big Data, Customize Your Plots Tidy Your Data – r4ds: Wrangle, II Reshape Data, Separate and Unite Columns, Join Data Sets "],["presentation-learning-resources.html", "Chapter 10 Presentation, Learning Resources 10.1 Part I: Presentation, Part II 10.2 Part II: Roundup, Learning Resources 10.3 Learning Resources", " Chapter 10 Presentation, Learning Resources 10.1 Part I: Presentation, Part II 10.1.1 The Tenth Assignment (in Moodle) A. Write your feedback to each of your classmates’ presentation on the Forum in Moodle. First, include the name of the presenter and/or the title of the presentation on the topic Write on the following Comments on the presentation, Suggestions on the project, or Technical or general questions related to the presentation You may add comments on your own presentation, and/or responses to comments. B. Comments and/or questions about the data analysis (or data science) Your plan for the future. What do you want to learn as the next step? Technical or general questions related to the final paper Submit your response to Moodle (The Tenth Assignment) by 2021-03-02 23:59:00 10.2 Part II: Roundup, Learning Resources 10.2.1 Contents Review of Data Analysis for Researchers Your Course Project R Notebook, MS Word and PDF, Workflow Chunk Options Run Codes by R Studio References for Your Course Project Learning Resources 10.2.2 Focus of This Course – Exploratory Data Analysis Introduction to EDA – three weeks EDA for Beginners – three to four weeks Experiencing EDA – three to four weeks 10.2.2.1 EDA: Visualize (surprize but not sclae), Model (scale but not surprise)) EDA from r4ds 10.2.3 Schedule and Grades 10.2.3.1 Moodle Submission Deadline: 2021-03-03 ‘Short Paper’: Supporting Document of Reproducible EDA with Explanation for you Paper in R Notebook (*.nb.html) Paper: EDA Using Public Data, 5 to 10 pages, in PDF (*.pdf) 10.2.3.2 Grades - Class participation and online quizzes - 30 % (3 x 10) Short paper: research proposal - 20 % Presentation - 10 % Final paper - 40 % 10.2.4 Your Course Project, Part I All documents must contain ‘ID’, ‘Name’, ‘Date’ of submission ‘Short Paper’ (that can be much longer than the Paper Due: 2021-02-09 for Interim Report (and 2021-03-03: supporting doc for Paper) Contents: Objective: What and Why Data Reproducible Exploratory Data Analysis with Explanations Questions based on your findings and technical quesitons Format: R Notebook (*.nb.html) Presentation: 10 minutes (5-7 min. presentation and 3-5 min. QA) On 2021-02-17 [or 2021-02-24, a reserve] With a digital file (.nb.html, html, pdf, word, ppt, … ) by file share Note: Be ready to show your codes by R Notebook or R Scripts, when requested 10.2.5 Your Course Project, Part II Paper: 5 to 10 pages Due: 2021-03-03 Contents: Exploratory Data Analysis Using Public Data Introduction - include what and why Description of Data Exposition of Your Exploration with Visualization of Data Concluding Remarks References, if any Acknowledgements, if any (can give a credit to your classmate) Note: Give logical explanations of your observations using data tables and charts No need to include the whole process Include codes only when necessary Format: pdf. (Rmd &gt; pdf, Rmd &gt; MS Word &gt; pdf, Rmd &gt; MS Word &gt; Google Doc &gt; pdf) 10.2.6 R Notebook, MS Word and PDF, Workflow Rmd &gt; pdf: Use knit pdf, one of the knit options you can find under the knit button. Rmd &gt; MS Word &gt; pdf: Use knit Word, one of the knit options you can find under the knit button. Then, use one of the print options of MS Word to obtain a pdf file. Rmd &gt; MS Word &gt; Google Doc &gt; pdf: If you do not have MS Word, upload your MS Word file obtained by knit Word into your Google Doc and edit it. You can create and download a PDF file easily using Google Doc. Since you can edit the file as an MS Word file now with Google Doc, you can make the style file mentioned above. In the following, we focus on the second option and explain how to handle chunks and the MS Word style file. title: &quot;Title of Your Paper&quot; author: &quot;Your Name&quot; date: &#39;2021-03-03&#39; output: html_notebook: toc: yes number_sections: yes toc_float: yes html_document: toc: yes number_sections: yes toc_float: yes pdf_document: default word_document: number_sections: yes # reference_docx: &quot;word-styles-reference-01.docx&quot; 10.2.7 Export MS Word Using a Style File Use R Markdown to create a Word document Save as: ``word-styles-reference-01.docx’’ Edit the Word styles you find there Edit the styles of file ``word-styles-reference-01.docx’’ . e.g. Change the page size: US Letter to A4 Paper Save this document as your style reference docx file Format an Rmd report using the styles reference docx file References: Happy collaboration with Rmd to docx: https://rmarkdown.rstudio.com/articles_docx.html R Markdown: Definitive Guide - Word Document: https://bookdown.org/yihui/rmarkdown/word-document.html Custom Word templates: R Markdown Cookbook: https://bookdown.org/yihui/rmarkdown-cookbook/word-template.html 10.2.8 Edit Chunk Options 10.2.8.1 Before Exporting MS Word Document by knit Word In the paper, you may want to deal with the following issues. You want to include the chart but not the code. You do not want to show your codes, results, messages, or warnings. You want to adjust the size of the figure to your document. You do not want to include a long table. … and many more. 10.2.9 Chunk Options include = FALSE prevents code and results from appearing in the finished file. R Markdown still runs the chunk code, and other chunks can use the resulting output. You should add this option after you check the code does not have an error. echo = FALSE prevents code, but not the results from appearing in the finished file. It is a useful way to embed figures. message = FALSE prevents messages generated by code from appearing in the finished file. warning = FALSE prevents warnings generated by code from appearing in the finished. You should add this option after you check the warnings. You may get errors or unexpected output. fig.cap = “…” adds a caption to graphical results. fig.height, fig.width: The width and height to use in R for plots created by the chunk (in inches). Link to Chunk Option Quick Reference: https://rmarkdown.rstudio.com/lesson-3.html See the R Markdown Reference Guide for a complete list of knit chunk options. 10.2.10 Examples and References It is a good idea to explain the packages you used but does not need to show everything in your paper as the number of pages is restricted. You can hide your setting by include = FALSE. In HTML, you can include a big table, but in MS Word, you need to control the number of rows to appear. Here I used slice(1:10), a tidyverse command corresponding to head(10) in base R. You can include your chart without showing your code by echo = FALSE. Chunk Option Quick Reference: https://rmarkdown.rstudio.com/lesson-3.html R Markdown Reference Guide for a complete list of knit chunk options. 10.2.11 References for Your Short-Paper and Paper R Markdown R Markdown from R Studio R Markdown: The Definitive Guide R Markdown Cookbook bookdown: Authoring Books and Technical Documents with R Markdown Data Analysis Write-Ups R for Data Science, Part VI Communicate Data analysis write-ups: https://jgscott.github.io/teaching/writeups/write_ups/ Structure of a Data Analysis Report: http://www.stat.cmu.edu/~brian/701/notes/paper-structure.pdf 10.2.12 R Studio: Run Codes There are many ways to run your codes. You can use key bindings as well, I believe. However, before complete editing, your final product, make sure that your code chunks run correctly without error. You need to check this process because, with R Notebook, you can edit back and forth, but to be consistent, your product has to be read consistently from top to bottom. There are many options under the Run button, including Run All and Restart R, and Run All Chunks. I recommend you the following: Restart R and run all chunks. Knit your R Markdown file with the format of the final product. Check the output carefully. Good luck! See Roundup.nb.html in Moodle 10.2.13 R Studio 1.4 - Released on 2021-01-19 10.2.13.1 Announcing RStudio 1.4 by Daniel Petzold and Carl Howe A visual markdown editor provides improved productivity for composing longer-form articles and analyses with R Markdown. The ability to add source columns to the IDE workspace for side-by-side text editing. To switch into the visual mode for a markdown document, use the button with the compass icon at the top-right of the editor toolbar A new command palette (accessible via Ctrl+Shift+P) provides easy keyboard access to all RStudio commands, add-ins, and options. Support for rainbow parentheses in the source editor (enabled via Options -&gt; Code -&gt; Display). RStudio.cloud is in 1.4. For your computer you need to install RStudio 1.4. 10.2.14 Learning Resources, X – Interactive Practicum 10.2.14.1 Swirl Courses: Github R Programming: The basics of programming in R — done Regression Models: The basics of regression modeling in R Statistical Inference: The basics of statistical inference in R Exploratory Data Analysis: The basics of exploring data in R 10.2.14.2 RStudio Primers https://rstudio.cloud/learn/primers The Basics – r4ds: Explore, I Work with Data – r4ds: Wrangle, I Visualize Data – r4ds: Explore, II Tidy Your Data – r4ds: Wrangle, II Iterate – r4ds: Program Write Functions – r4ds: Program Report Reproducibly and more … Coming Soon 10.2.15 Learning Resources, X – Books R for Data Science by Hadley Wickham and Garrett Grolemund – you are ready to learn ggplot2: Elegant Graphics for Data Analysis by Hadley Wickham the on-line version of work-in-progress 3rd edition Bookdown: https://bookdown.org Introduction to Data Science by Rafael A. Irizarry R Graphics Cookbook, 2nd edition, by Winston Chang many more … Books on Statistics difficult to choose… 10.2.16 Learning Resources, X – Moocs and Online Courses edX HarvardX Data Science - 9 courses. Textbook coursera JHU Data Science - 10 courses JHU Data Science: Foundations using R Specialization, 5 courses Interactive Learning Sites: DataCamp: https://www.datacamp.com Dataquest: https://www.dataquest.io 10.3 Learning Resources 10.3.1 Learning Resources, X – Interactive Practicum 10.3.1.1 Swirl Courses: Github R Programming: The basics of programming in R — done Regression Models: The basics of regression modeling in R Statistical Inference: The basics of statistical inference in R Exploratory Data Analysis: The basics of exploring data in R 10.3.1.2 RStudio Primers https://rstudio.cloud/learn/primers The Basics – r4ds: Explore, I Work with Data – r4ds: Wrangle, I Visualize Data – r4ds: Explore, II Tidy Your Data – r4ds: Wrangle, II Iterate – r4ds: Program Write Functions – r4ds: Program Report Reproducibly and more … Coming Soon 10.3.2 Learning Resources, X – Books R for Data Science by Hadley Wickham and Garrett Grolemund – you are ready to learn ggplot2: Elegant Graphics for Data Analysis by Hadley Wickham the on-line version of work-in-progress 3rd edition Bookdown: https://bookdown.org Introduction to Data Science by Rafael A. Irizarry R Graphics Cookbook, 2nd edition, by Winston Chang many more … Books on Statistics difficult to choose… 10.3.3 Learning Resources, X – Moocs and Online Courses edX HarvardX Data Science - 9 courses. Textbook coursera JHU Data Science - 10 courses JHU Data Science: Foundations using R Specialization, 5 courses Interactive Learning Sites: DataCamp: https://www.datacamp.com Dataquest: https://www.dataquest.io "],["nightingale.html", "A Appendix A Nightingale’s Data A.1 Basic References A.2 HistData: Data Sets from the History of Statistics and Data Visualization A.3 Exploratory Data Analysis Using tidyverse Package A.4 Original Code in Nightingale document", " A Appendix A Nightingale’s Data Nightingale’s data is contained in HistData Package of R. See https://www.rdocumentation.org/packages/HistData/versions/0.8-6/topics/Nightingale A.1 Basic References Florence Nightingale Museum in London: https://www.florence-nightingale.co.uk Florence Nightingale biography: https://www.florence-nightingale.co.uk/florence-nightingale-biography/ BBC: Florence Nightingale: Saving lives with statistics: https://www.bbc.co.uk/teach/florence-nightingale-saving-lives-with-statistics/zjksmfr Insights in Social History by Hugh Small: http://www.florence-nightingale-avenging-angel.co.uk Florence Nightingale’s most famous infographic (1858): http://www.florence-nightingale-avenging-angel.co.uk/?page_id=2382 Florence Nightingale’s Public Health Act, Covid-19 and the empowerment of local government, by Hugh Small, 12 October 2020 Life Expectancy: Office for National Statistics, UK Wikipedia: https://en.wikipedia.org/wiki/Florence_Nightingale Life expectancy (from birth) in the United Kingdom from 1765 to 2020: https://www.statista.com/statistics/1040159/life-expectancy-united-kingdom-all-time/ Cure: Medical Treatment Care: Nursing Prevention: Public Health A.2 HistData: Data Sets from the History of Statistics and Data Visualization URL: https://cran.r-project.org/web/packages/HistData/index.html Description: The ‘HistData’ package provides a collection of small data sets that are interesting and important in the history of statistics and data visualization. The goal of the package is to make these available, both for instructional use and for historical research. Some of these present interesting challenges for graphics or analysis in R. Reference Manual: HistData.pdf Vignettes: Duplicate and Missing Cases in Snow.deaths Reverse Depend: UsingR A.2.1 Nightingale Datasets Details: For a given cause of death, D, annual rates per 1000 are calculated as 12 * 1000 * D / Army, rounded to 1 decimal. The two panels of Nightingale’s Coxcomb correspond to dates before and after March 1855 Format: A data frame with 24 observations on the following 10 variables. Date: a Date, composed as as.Date(paste(Year, Month, 1, sep=‘-’), “%Y-%b-%d”) Month: Month of the Crimean War, an ordered factor Year: Year of the Crimean War Army: Estimated average monthly strength of the British army Disease: Number of deaths from preventable or mitagable zymotic diseases Wounds: Number of deaths directly from battle wounds Other: Number of deaths from other causes Disease.rate: Annual rate of deaths from preventable or mitagable zymotic diseases, per 1000 Wounds.rate: Annual rate of deaths directly from battle wounds, per 1000 Other.rate: Annual rate of deaths from other causes, per 1000 A.2.2 References Nightingale, F. (1858) Notes on Matters Affecting the Health, Efficiency, and Hospital Administration of the British Army Harrison and Sons, 1858 Nightingale, F. (1859) A Contribution to the Sanitary History of the British Army during the Late War with Russia London: John W. Parker and Son. Small, H. (1998) Florence Nightingale’s statistical diagrams http://www.florence-nightingale-avenging-angel.co.uk/GraphicsPaper/Graphics.htm Pearson, M. and Short, I. (2008) Nightingale’s Rose (flash animation). http://understandinguncertainty.org/files/animations/Nightingale11/Nightingale1.html A.3 Exploratory Data Analysis Using tidyverse Package A.3.1 Reading Nightingale Data and Glimpse the Structure library(HistData) library(tidyverse) data(Nightingale) Nightingale ## Date Month Year Army Disease Wounds Other Disease.rate Wounds.rate ## 1 1854-04-01 Apr 1854 8571 1 0 5 1.4 0.0 ## 2 1854-05-01 May 1854 23333 12 0 9 6.2 0.0 ## 3 1854-06-01 Jun 1854 28333 11 0 6 4.7 0.0 ## 4 1854-07-01 Jul 1854 28722 359 0 23 150.0 0.0 ## 5 1854-08-01 Aug 1854 30246 828 1 30 328.5 0.4 ## 6 1854-09-01 Sep 1854 30290 788 81 70 312.2 32.1 ## 7 1854-10-01 Oct 1854 30643 503 132 128 197.0 51.7 ## 8 1854-11-01 Nov 1854 29736 844 287 106 340.6 115.8 ## 9 1854-12-01 Dec 1854 32779 1725 114 131 631.5 41.7 ## 10 1855-01-01 Jan 1855 32393 2761 83 324 1022.8 30.7 ## 11 1855-02-01 Feb 1855 30919 2120 42 361 822.8 16.3 ## 12 1855-03-01 Mar 1855 30107 1205 32 172 480.3 12.8 ## 13 1855-04-01 Apr 1855 32252 477 48 57 177.5 17.9 ## 14 1855-05-01 May 1855 35473 508 49 37 171.8 16.6 ## 15 1855-06-01 Jun 1855 38863 802 209 31 247.6 64.5 ## 16 1855-07-01 Jul 1855 42647 382 134 33 107.5 37.7 ## 17 1855-08-01 Aug 1855 44614 483 164 25 129.9 44.1 ## 18 1855-09-01 Sep 1855 47751 189 276 20 47.5 69.4 ## 19 1855-10-01 Oct 1855 46852 128 53 18 32.8 13.6 ## 20 1855-11-01 Nov 1855 37853 178 33 32 56.4 10.5 ## 21 1855-12-01 Dec 1855 43217 91 18 28 25.3 5.0 ## 22 1856-01-01 Jan 1856 44212 42 2 48 11.4 0.5 ## 23 1856-02-01 Feb 1856 43485 24 0 19 6.6 0.0 ## 24 1856-03-01 Mar 1856 46140 15 0 35 3.9 0.0 ## Other.rate ## 1 7.0 ## 2 4.6 ## 3 2.5 ## 4 9.6 ## 5 11.9 ## 6 27.7 ## 7 50.1 ## 8 42.8 ## 9 48.0 ## 10 120.0 ## 11 140.1 ## 12 68.6 ## 13 21.2 ## 14 12.5 ## 15 9.6 ## 16 9.3 ## 17 6.7 ## 18 5.0 ## 19 4.6 ## 20 10.1 ## 21 7.8 ## 22 13.0 ## 23 5.2 ## 24 9.1 glimpse(Nightingale) ## Rows: 24 ## Columns: 10 ## $ Date &lt;date&gt; 1854-04-01, 1854-05-01, 1854-06-01, 1854-07-01, 1854-08-… ## $ Month &lt;ord&gt; Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec, Jan, Feb, Ma… ## $ Year &lt;int&gt; 1854, 1854, 1854, 1854, 1854, 1854, 1854, 1854, 1854, 185… ## $ Army &lt;int&gt; 8571, 23333, 28333, 28722, 30246, 30290, 30643, 29736, 32… ## $ Disease &lt;int&gt; 1, 12, 11, 359, 828, 788, 503, 844, 1725, 2761, 2120, 120… ## $ Wounds &lt;int&gt; 0, 0, 0, 0, 1, 81, 132, 287, 114, 83, 42, 32, 48, 49, 209… ## $ Other &lt;int&gt; 5, 9, 6, 23, 30, 70, 128, 106, 131, 324, 361, 172, 57, 37… ## $ Disease.rate &lt;dbl&gt; 1.4, 6.2, 4.7, 150.0, 328.5, 312.2, 197.0, 340.6, 631.5, … ## $ Wounds.rate &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.4, 32.1, 51.7, 115.8, 41.7, 30.7, 1… ## $ Other.rate &lt;dbl&gt; 7.0, 4.6, 2.5, 9.6, 11.9, 27.7, 50.1, 42.8, 48.0, 120.0, … A.3.2 Comparison of Death Causes df_cause &lt;- Nightingale %&gt;% select(Disease, Wounds, Other) %&gt;% pivot_longer(cols = everything(), names_to = &quot;Cause&quot;, values_to = &quot;Death&quot;) df_cause %&gt;% ggplot(aes(x = Cause, y = Death)) + geom_bar(stat = &quot;identity&quot;) df_cause %&gt;% ggplot(aes(x = &quot;&quot;, y = Death, fill = Cause)) + geom_bar(width = 1, stat = &quot;identity&quot;) + coord_polar(&quot;y&quot;, start=0) total = sum(df_cause$Death) df_cause %&gt;% group_by(Cause) %&gt;% summarize(Rate = round(sum(Death)/total*100, digits = 1)) ## # A tibble: 3 × 2 ## Cause Rate ## &lt;chr&gt; &lt;dbl&gt; ## 1 Disease 80.5 ## 2 Other 9.7 ## 3 Wounds 9.8 df_rate &lt;- Nightingale %&gt;% select(Date, Army, Disease, Wounds, Other) %&gt;% mutate(Death_Rate = (Disease + Wounds + Other)/Army, Disease_Rate = Disease/Army) df_rate %&gt;% ggplot(aes(x = Date)) + geom_line(aes(y = Death_Rate)) + geom_line(aes(y = Disease_Rate), color = &quot;blue&quot;) + geom_vline(xintercept = as.Date(&quot;1855-04-01&quot;), color = &quot;red&quot;) A.3.3 Data Wrangling - Tidying Data First focus on the rates by cause Month, Year columns are redundant and use Date When rates are considered, Army, Desease, Wounds and Other columns are not necessary. We use long table to apply ggplot2 to visualize data. dat %&gt;% pivot_longer(cols = &quot;columns kept as a vector&quot;, names_to = &quot;variable&quot;, values_to = &quot;date&quot;) df_fn &lt;- Nightingale %&gt;% select(Date, &quot;Disease_Rate&quot; = Disease.rate, &quot;Wounds_Rate&quot; = Wounds.rate, &quot;Other_Rate&quot; = Other.rate) %&gt;% pivot_longer(cols = Disease_Rate:Other_Rate, names_to = &quot;Cause&quot;, values_to = &quot;Deaths&quot;) df_fn ## # A tibble: 72 × 3 ## Date Cause Deaths ## &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1854-04-01 Disease_Rate 1.4 ## 2 1854-04-01 Wounds_Rate 0 ## 3 1854-04-01 Other_Rate 7 ## 4 1854-05-01 Disease_Rate 6.2 ## 5 1854-05-01 Wounds_Rate 0 ## 6 1854-05-01 Other_Rate 4.6 ## 7 1854-06-01 Disease_Rate 4.7 ## 8 1854-06-01 Wounds_Rate 0 ## 9 1854-06-01 Other_Rate 2.5 ## 10 1854-07-01 Disease_Rate 150 ## # … with 62 more rows ggplot(df_fn) + geom_bar(aes(x = Date, y = Deaths, fill = Cause), stat=&quot;identity&quot;) Default of the position is “stack”. The other options are “dodge” and “identity”. The option “identity” is not useful for bars, because it overlaps them. See that overlapping by setting a small value for alpha, transparancy. ggplot(df_fn) + geom_bar(aes(x = Date, y = Deaths, fill = Cause), stat = &quot;identity&quot;, position = &quot;dodge&quot;) df_fn %&gt;% filter(Date &gt;= as.Date(&quot;1855-08-01&quot;)) %&gt;% ggplot() + geom_bar(aes(x = Date, y = Deaths, fill = Cause), stat = &quot;identity&quot;, position = &quot;identity&quot;, alpha = 0.4) df_fn %&gt;% filter(Date &gt;= as.Date(&quot;1855-08-01&quot;)) %&gt;% ggplot() + geom_bar(aes(x = Date, y = Deaths, fill = Cause), stat = &quot;identity&quot;, position = &quot;dodge&quot;) Let us split the data into two and see the change before and after the Sanitary Commission arrived in the middle of the war, i.e, March 6, 1885. df_fn_ba &lt;- df_fn %&gt;% mutate(Regime = if_else(Date &lt; as.Date(&quot;1855-04-01&quot;), &quot;Before&quot;, &quot;After&quot;)) df_fn_ba %&gt;% filter(Date &gt; as.Date(&quot;1855-01-01&quot;) &amp; Date &lt; as.Date(&quot;1855-06-01&quot;)) ## # A tibble: 12 × 4 ## Date Cause Deaths Regime ## &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1855-02-01 Disease_Rate 823. Before ## 2 1855-02-01 Wounds_Rate 16.3 Before ## 3 1855-02-01 Other_Rate 140. Before ## 4 1855-03-01 Disease_Rate 480. Before ## 5 1855-03-01 Wounds_Rate 12.8 Before ## 6 1855-03-01 Other_Rate 68.6 Before ## 7 1855-04-01 Disease_Rate 178. After ## 8 1855-04-01 Wounds_Rate 17.9 After ## 9 1855-04-01 Other_Rate 21.2 After ## 10 1855-05-01 Disease_Rate 172. After ## 11 1855-05-01 Wounds_Rate 16.6 After ## 12 1855-05-01 Other_Rate 12.5 After df_fn_ba %&gt;% filter(Regime == &quot;Before&quot;) %&gt;% ggplot() + geom_bar(aes(x = as.factor(Date), y=Deaths, fill = Cause), width = 1, position=&quot;identity&quot;, stat=&quot;identity&quot;, alpha = 0.5) + scale_y_sqrt() + coord_polar(start = 3*pi/2) + labs(title = &quot;Causes of Mortality in the Army in the East&quot;) df_fn_ba %&gt;% filter(Regime == &quot;After&quot;) %&gt;% ggplot() + geom_bar(aes(x = as.factor(Date), y=Deaths, fill = Cause), width = 1, position=&quot;identity&quot;, stat=&quot;identity&quot;, alpha = 0.5) + scale_y_sqrt() + coord_polar(start = 3*pi/2) + labs(title = &quot;Causes of Mortality in the Army in the East&quot;) Please refer to the folloing code, f you want to use facet_grid. The argument scales = “free” of facet_grid does not support coord_polar. However, if you add the first two lines, it seems to work. See https://github.com/tidyverse/ggplot2/issues/2815. cp &lt;- coord_polar(theta = &quot;x&quot;, start = 3*pi/2) cp$is_free &lt;- function() TRUE df_fn_ba %&gt;% #filter(Regime == &quot;Before&quot;) %&gt;% ggplot() + geom_bar(aes(x = as.factor(Date), y=Deaths, fill = Cause), width = 1, position=&quot;identity&quot;, stat=&quot;identity&quot;, alpha = 0.5) + scale_y_sqrt() + # death scale is proportional to the area cp + facet_grid(. ~ Regime, labeller = label_both, scales = &quot;free&quot;) + labs(title = &quot;Causes of Mortality in the Army in the East&quot;) + theme(aspect.ratio = 1) df_fn_before &lt;- df_fn %&gt;% filter(Date &lt; as.Date(&quot;1855-04-01&quot;)) nrow(df_fn_before) ## [1] 36 df_fn_after &lt;- df_fn %&gt;% filter(Date &gt;= as.Date(&quot;1855-04-01&quot;)) nrow(df_fn_after) ## [1] 36 ggplot(df_fn_before) + geom_bar(aes(x = Date, y = Deaths, fill = Cause), stat = &quot;identity&quot;, position = &quot;dodge&quot;) ggplot(df_fn_after) + geom_bar(aes(x = Date, y = Deaths, fill = Cause), stat = &quot;identity&quot;, position = &quot;dodge&quot;) A.4 Original Code in Nightingale document A.4.1 reshape Package: Flexibly Reshape Data Description: Flexibly restructure and aggregate data using just two functions: melt and cast. URL: https://CRAN.R-project.org/package=reshape HP of the Author: http://had.co.nz/reshape/ Article: https://www.jstatsoft.org/article/view/v021i12 Reference Manual: reshape.pdf # NOT RUN { library(HistData) data(Nightingale) # For some graphs, it is more convenient to reshape death rates to long format # keep only Date and death rates require(reshape) ## 要求されたパッケージ reshape をロード中です ## ## 次のパッケージを付け加えます: &#39;reshape&#39; ## 以下のオブジェクトは &#39;package:dplyr&#39; からマスクされています: ## ## rename ## 以下のオブジェクトは &#39;package:tidyr&#39; からマスクされています: ## ## expand, smiths Night&lt;- Nightingale[,c(1,8:10)] melted &lt;- melt(Night, &quot;Date&quot;) names(melted) &lt;- c(&quot;Date&quot;, &quot;Cause&quot;, &quot;Deaths&quot;) melted$Cause &lt;- sub(&quot;\\\\.rate&quot;, &quot;&quot;, melted$Cause) melted$Regime &lt;- ordered( rep(c(rep(&#39;Before&#39;, 12), rep(&#39;After&#39;, 12)), 3), levels=c(&#39;Before&#39;, &#39;After&#39;)) Night &lt;- melted # subsets, to facilitate separate plotting Night1 &lt;- subset(Night, Date &lt; as.Date(&quot;1855-04-01&quot;)) Night2 &lt;- subset(Night, Date &gt;= as.Date(&quot;1855-04-01&quot;)) # sort according to Deaths in decreasing order, so counts are not obscured [thx: Monique Graf] Night1 &lt;- Night1[order(Night1$Deaths, decreasing=TRUE),] Night2 &lt;- Night2[order(Night2$Deaths, decreasing=TRUE),] # merge the two sorted files Night &lt;- rbind(Night1, Night2) require(ggplot2) # Before plot cxc1 &lt;- ggplot(Night1, aes(x = factor(Date), y=Deaths, fill = Cause)) + # do it as a stacked bar chart first geom_bar(width = 1, position=&quot;identity&quot;, stat=&quot;identity&quot;, color=&quot;black&quot;) + # set scale so area ~ Deaths scale_y_sqrt() # A coxcomb plot = bar chart + polar coordinates cxc1 + coord_polar(start=3*pi/2) + ggtitle(&quot;Causes of Mortality in the Army in the East&quot;) + xlab(&quot;&quot;) # After plot cxc2 &lt;- ggplot(Night2, aes(x = factor(Date), y=Deaths, fill = Cause)) + geom_bar(width = 1, position=&quot;identity&quot;, stat=&quot;identity&quot;, color=&quot;black&quot;) + scale_y_sqrt() cxc2 + coord_polar(start=3*pi/2) + ggtitle(&quot;Causes of Mortality in the Army in the East&quot;) + xlab(&quot;&quot;) The following part does not work. Error! # do both together, with faceting cxc &lt;- ggplot(Night, aes(x = factor(Date), y=Deaths, fill = Cause)) + geom_bar(width = 1, position=&quot;identity&quot;, stat=&quot;identity&quot;, color=&quot;black&quot;) + scale_y_sqrt() + facet_grid(. ~ Regime, scales=&quot;free&quot;, labeller=label_both) facet_grid(. ~ Regime, labeller=label_both) cxc + coord_polar(start=3*pi/2) + ggtitle(&quot;Causes of Mortality in the Army in the East&quot;) + xlab(&quot;&quot;) # NOT RUN { ## What if she had made a set of line graphs? # these plots are best viewed with width ~ 2 * height colors &lt;- c(&quot;blue&quot;, &quot;red&quot;, &quot;black&quot;) with(Nightingale, { plot(Date, Disease.rate, type=&quot;n&quot;, cex.lab=1.25, ylab=&quot;Annual Death Rate&quot;, xlab=&quot;Date&quot;, xaxt=&quot;n&quot;, main=&quot;Causes of Mortality of the British Army in the East&quot;); # background, to separate before, after rect(as.Date(&quot;1854/4/1&quot;), -10, as.Date(&quot;1855/3/1&quot;), 1.02*max(Disease.rate), col=gray(.90), border=&quot;transparent&quot;); text( as.Date(&quot;1854/4/1&quot;), .98*max(Disease.rate), &quot;Before Sanitary\\nCommission&quot;, pos=4); text( as.Date(&quot;1855/4/1&quot;), .98*max(Disease.rate), &quot;After Sanitary\\nCommission&quot;, pos=4); # plot the data points(Date, Disease.rate, type=&quot;b&quot;, col=colors[1], lwd=3); points(Date, Wounds.rate, type=&quot;b&quot;, col=colors[2], lwd=2); points(Date, Other.rate, type=&quot;b&quot;, col=colors[3], lwd=2) } ) # add custom Date axis and legend axis.Date(1, at=seq(as.Date(&quot;1854/4/1&quot;), as.Date(&quot;1856/3/1&quot;), &quot;3 months&quot;), format=&quot;%b %Y&quot;) legend(as.Date(&quot;1855/10/20&quot;), 700, c(&quot;Preventable disease&quot;, &quot;Wounds and injuries&quot;, &quot;Other&quot;), col=colors, fill=colors, title=&quot;Cause&quot;, cex=1.25) # Alternatively, show each cause of death as percent of total Nightingale &lt;- within(Nightingale, { Total &lt;- Disease + Wounds + Other Disease.pct &lt;- 100*Disease/Total Wounds.pct &lt;- 100*Wounds/Total Other.pct &lt;- 100*Other/Total }) colors &lt;- c(&quot;blue&quot;, &quot;red&quot;, &quot;black&quot;) with(Nightingale, { plot(Date, Disease.pct, type=&quot;n&quot;, ylim=c(0,100), cex.lab=1.25, ylab=&quot;Percent deaths&quot;, xlab=&quot;Date&quot;, xaxt=&quot;n&quot;, main=&quot;Percentage of Deaths by Cause&quot;); # background, to separate before, after rect(as.Date(&quot;1854/4/1&quot;), -10, as.Date(&quot;1855/3/1&quot;), 1.02*max(Disease.rate), col=gray(.90), border=&quot;transparent&quot;); text( as.Date(&quot;1854/4/1&quot;), .98*max(Disease.pct), &quot;Before Sanitary\\nCommission&quot;, pos=4); text( as.Date(&quot;1855/4/1&quot;), .98*max(Disease.pct), &quot;After Sanitary\\nCommission&quot;, pos=4); # plot the data points(Date, Disease.pct, type=&quot;b&quot;, col=colors[1], lwd=3); points(Date, Wounds.pct, type=&quot;b&quot;, col=colors[2], lwd=2); points(Date, Other.pct, type=&quot;b&quot;, col=colors[3], lwd=2) } ) # add custom Date axis and legend axis.Date(1, at=seq(as.Date(&quot;1854/4/1&quot;), as.Date(&quot;1856/3/1&quot;), &quot;3 months&quot;), format=&quot;%b %Y&quot;) legend(as.Date(&quot;1854/8/20&quot;), 60, c(&quot;Preventable disease&quot;, &quot;Wounds and injuries&quot;, &quot;Other&quot;), col=colors, fill=colors, title=&quot;Cause&quot;, cex=1.25) # } "],["public.html", "B Appendix B Public Data B.1 Introduction B.2 World Bank B.3 United Nations", " B Appendix B Public Data B.1 Introduction B.1.1 Open Data Defined by World Bank See the following URL: http://opendatatoolkit.worldbank.org The term “Open Data” has a very precise meaning. Data or content is open if anyone is free to use, re-use or redistribute it, subject at most to measures that preserve provenance and openness. The data must be , which means they must be placed in the public domain or under liberal terms of use with minimal restrictions. The data must be , which means they must be published in electronic formats that are machine readable and non-proprietary, so that anyone can access and use the data using common, freely available software tools. Data must also be publicly available and accessible on a public server, without password or firewall restrictions. To make Open Data easier to find, most organizations create and manage Open Data catalogs. B.1.2 A List of Open Data Catalogue B.1.2.1 International Institutions World Bank: New Ways of Looking at Poverty Open Data: https://data.worldbank.org World Development Indicators: http://datatopics.worldbank.org/world-development-indicators/ UN Data: http://data.un.org WHO Data: https://www.who.int/gho/en/ OECD: https://data.oecd.org European Union: http://data.europa.eu/euodp/en/home African Union: https://au.int/en/ea/statistics B.1.2.2 Goverments United States: https://www.data.gov United Kingdom: https://data.gov.uk China: http://www.stats.gov.cn/english/ Japan: https://www.data.go.jp/list-of-database/?lang=en B.1.2.3 Other Open Public Data Google Public Data Explore: https://www.google.com/publicdata/directory?hl=en_US Google Dataset Search: https://toolbox.google.com/datasetsearch Google Trends: https://trends.google.com/trends/?geo=US Open Knowledge Foundation: https://okfn.org Global Open Data Index: https://index.okfn.org A global, non-profit network that promotes and shares information at no charge, including both content and data. It was founded by Rufus Pollock on 20 May 2004 and launched on 24 May 2004 in Cambridge, UK. It is incorporated in England and Wales as a company limited by guarantee. (Wikipedia) Our World in Data: https://ourworldindata.org A scientific online publication that focuses on large global problems such as poverty, disease, hunger, climate change, war, existential risks, and inequality. The publication’s founder is the social historian and development economist Max Roser. The research team is based at the University of Oxford. (Wikipedia) B.1.2.4 Financial Data B.1.2.4.1 Quandl Quandl R package: https://cran.r-project.org/web/packages/Quandl/Quandl.pdf Quandl: Bitcoin Market Price USD B.1.2.4.2 Quantmod quantmod: Quantitative Financial Modelling &amp; Trading Framework for R R package `quantmod: https://cran.r-project.org/web/packages/quantmod/quantmod.pdf quantmod R documentation Yahoo Finance Data Using quantmod Reference: CryptCurrency Bitcoin Analysis Using quantmod B.1.3 Examples B.1.3.1 Florence Nightingale (1820 – 1910) Florence Nightingale was an English social reformer, statistician and the founder of modern nursing. (wikipedia) Diagram of the Causes of Motality in the Army in the East Insights in Social History, Books and Research by Hugh Small Florence Nightingale’s Statistical Diagrams: https://www.york.ac.uk/depts/maths/histstat/small.htm Florence Nightingale Museum https://www.florence-nightingale.co.uk/learning/ Meet Miss Nightingale: https://www.florence-nightingale.co.uk/meet-miss-nightingale/ Book: A contribution to the sanitary history of the British army during the late war with Russia Project Gutenberg: Books by Nightingale, Florence Notes on Nursing: What It Is, and What It Is Not Nightingale: The Journal of the Data Visualization Society, Medium B.1.3.2 Hans Rosling (1948 – 2017) Hans Rosling was a Swedish physician, academic, and public speaker. He was a professor of international health at Karolinska Institute[4] and was the co-founder and chairman of the Gapminder Foundation, which developed the Trendalyzer software system. (wikipedia) Books: Factfulness: Ten Reasons We’re Wrong About The World - And Why Things Are Better Than You Think, 2018 How I Learned to Understand the World: A Memoir, 2020 Gapminder: https://www.gapminder.org You are probably wrong about: Upgrade Your World View Bubble Chart: Income vs Life Expectancy over time, 1800 - 2020 How many variables? Videos: The best stats you’ve ever seen, Hans Rosling Google Public Data: Example: World Development Indicator B.1.3.2.1 Factfulness is … From the book recognizing when a decision feels urgent and remembering that it rarely is. To control the urgency instinct, take small steps. Take a breath. When your urgency instinct is triggered, your other instincts kick in and your analysis shuts down. Ask for more time and more information. It’s rarely now or never and it’s rarely either/or. Insist on the data. If something is urgent and important, it should be measured. Beware of data that is relevant but inaccurate, or accurate but irrelevant. Only relevant and accurate data is useful. Beware of fortune-tellers. Any prediction about the future is uncertain. Be wary of predictions that fail to acknowledge that. Insist on a full range of scenarios, never just the best or worst case. Ask how often such predictions have been right before. Be wary of drastic action. Ask what the side effects will be. Ask how the idea has been tested. Step-by-step practical improvements, and evaluation of their impact, are less dramatic but usually more effective. B.2 World Bank B.2.1 About World Bank: https://www.worldbank.org Who we are: To end extreme poverty: By reducing the share of the global population that lives in extreme poverty to 3 percent by 2030. To promote shared prosperity: By increasing the incomes of the poorest 40 percent of people in every country. World Bank Open Data: https://data.worldbank.org B.2.2 WDI - World Development Indicaters World Development Indicators (WDI): the World Bank’s premier compilation of cross-country comparable data on development. Poverty and Inequality People Environment Economy States and Markets Global Links B.2.3 R Package WDI WDI: World Development Indicators and Other World Bank Data Search and download data from over 40 databases hosted by the World Bank, including the World Development Indicators (‘WDI’), International Debt Statistics, Doing Business, Human Capital Index, and Sub-national Poverty indicators. Version: 2.7.4 Materials: README - usage NEWS - version history Published: 2021-04-06 Reference manual: WDI.pdf B.2.3.1 Function WDI: World Development Indicators (World Bank) Description: Downloads the requested data by using the World Bank’s API, parses the resulting XML file, and formats it in long country-year format. Usage WDI( country = &quot;all&quot;, indicator = &quot;NY.GDP.PCAP.KD&quot;, start = 1960, end = 2020, extra = FALSE, cache = NULL, latest = NULL, language = &quot;en&quot; ) Arguments country: Vector of countries (ISO-2 character codes, e.g. “BR”, “US”, “CA”) for which the data is needed. Using the string “all” instead of individual iso codes pulls data for every available country. https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2 https://www.nationsonline.org/oneworld/country_code_list.htm indicator: Character vector of indicators codes. See the WDIsearch() function. If you supply a named vector, the indicators will be automatically renamed: ‘c(’women_private_sector’ = ‘BI.PWK.PRVS.FE.ZS’)’ start: Start date, usually a year in integer format. Must be 1960 or greater. end: End date, usually a year in integer format. Must be greater than the ‘start’ argument. extra: TRUE returns extra variables such as region, iso3c code, and incomeLevel cache: NULL (optional) a list created by WDIcache() to be used with the extra=TRUE argument Value: Data frame with country-year observations. You can extract a data.frame with indicator names and descriptive labels by inspecting the label attribute of the resulting data.frame: attr(dat, 'label') B.2.3.2 Function WDIsearch Search names and descriptions of available WDI series Description Data frame with series code, name, description, and source for the WDI series which match the given criteria Usage WDIsearch(string = “gdp”, field = “name”, short = TRUE, cache = NULL) Arguments string: Character string. Search for this string using grep with ignore.case=TRUE. field: Character string. Search this field. Admissible fields: ‘indicator’, ‘name’, ‘description’, ‘sourceDatabase’, ‘sourceOrganization’ short; TRUE: Returns only the indicator’s code and name. FALSE: Returns the indicator’s code, name, description, and source. cache; Data list generated by the WDIcache function. If omitted, WDIsearch will search a local list of series. Value; Data frame with code, name, source, and description of all series which match the criteria. B.2.4 The First Example B.2.4.1 Setup In this R Notebook, we will use the following packages. Istall them before you compile this R Notebook. tidyverse Package, a collection of packages for data science WDI Package for World Development Indicators library(tidyverse) library(WDI) B.2.4.2 GDP Per Capita The following is taken from the usage. WDI( country = &quot;all&quot;, indicator = &quot;NY.GDP.PCAP.KD&quot;, start = 1960, end = 2020, extra = FALSE, cache = NULL, latest = NULL, language = &quot;en&quot; ) WDIsearch(string = &quot;NY.GDP.PCAP.KD&quot;, field = &quot;indicator&quot;, cache = NULL) ## indicator name ## 11431 NY.GDP.PCAP.KD GDP per capita (constant 2015 US$) ## 11432 NY.GDP.PCAP.KD.ZG GDP per capita growth (annual %) WDIsearch(string = &quot;NY.GDP.PCAP.KD&quot;, field = &quot;indicator&quot;, short = FALSE, cache = NULL) ## indicator name ## 11431 NY.GDP.PCAP.KD GDP per capita (constant 2015 US$) ## 11432 NY.GDP.PCAP.KD.ZG GDP per capita growth (annual %) ## description ## 11431 GDP per capita is gross domestic product divided by midyear population. GDP is the sum of gross value added by all resident producers in the economy plus any product taxes and minus any subsidies not included in the value of the products. It is calculated without making deductions for depreciation of fabricated assets or for depletion and degradation of natural resources. Data are in constant 2015 U.S. dollars. ## 11432 Annual percentage growth rate of GDP per capita based on constant local currency. GDP per capita is gross domestic product divided by midyear population. GDP at purchaser&#39;s prices is the sum of gross value added by all resident producers in the economy plus any product taxes and minus any subsidies not included in the value of the products. It is calculated without making deductions for depreciation of fabricated assets or for depletion and degradation of natural resources. ## sourceDatabase ## 11431 World Development Indicators ## 11432 World Development Indicators ## sourceOrganization ## 11431 World Bank national accounts data, and OECD National Accounts data files. ## 11432 World Bank national accounts data, and OECD National Accounts data files. df &lt;- as_tibble(WDI( country = c(&quot;CN&quot;, &quot;IN&quot;, &quot;US&quot;, &quot;ID&quot;,&quot;PK&quot;, &quot;BR&quot;, &quot;NG&quot;, &quot;BD&quot;, &quot;RU&quot;, &quot;MX&quot;, &quot;JP&quot;), indicator = &quot;NY.GDP.PCAP.KD&quot;, start = 1960, end = 2020, extra = FALSE, cache = NULL, latest = NULL, language = &quot;en&quot; )) df ## # A tibble: 671 × 5 ## country iso2c iso3c year NY.GDP.PCAP.KD ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Bangladesh BD BGD 2020 1620. ## 2 Bangladesh BD BGD 2019 1582. ## 3 Bangladesh BD BGD 2018 1481. ## 4 Bangladesh BD BGD 2017 1395. ## 5 Bangladesh BD BGD 2016 1323. ## 6 Bangladesh BD BGD 2015 1248. ## 7 Bangladesh BD BGD 2014 1185. ## 8 Bangladesh BD BGD 2013 1130. ## 9 Bangladesh BD BGD 2012 1078. ## 10 Bangladesh BD BGD 2011 1024. ## # … with 661 more rows ggplot(df) + geom_line(aes(x = year, y = NY.GDP.PCAP.KD, color = country)) + labs(title = &quot;GDP per capita (constant 2010 US$)&quot;) ## Warning: Removed 29 rows containing missing values (`geom_line()`). ggplot(df) + geom_line(aes(x = year, y = NY.GDP.PCAP.KD, color = country)) + scale_y_continuous(trans=&#39;log10&#39;) + labs(title = &quot;GDP per capita (constant 2010 US$)&quot;, subtitle = &quot;Log10 Scale&quot;) ## Warning: Removed 29 rows containing missing values (`geom_line()`). ggplot(df) + geom_line(aes(x = year, y = NY.GDP.PCAP.KD, color = country)) + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(vars(country)) + labs(title = &quot;GDP per capita (constant 2010 US$)&quot;, subtitle = &quot;Log10 Scale&quot;) ## Warning: Removed 29 rows containing missing values (`geom_line()`). ### More Examples B.2.4.3 Search Indicators Related to “GDP” as_tibble(WDIsearch(string = &quot;gdp&quot;, field = &quot;name&quot;, cache = NULL)) ## # A tibble: 540 × 2 ## indicator name ## &lt;chr&gt; &lt;chr&gt; ## 1 5.51.01.10.gdp &quot;Per capita GDP growth&quot; ## 2 6.0.GDP_current &quot;GDP (current $)&quot; ## 3 6.0.GDP_growth &quot;GDP growth (annual %)&quot; ## 4 6.0.GDP_usd &quot;GDP (constant 2005 $)&quot; ## 5 6.0.GDPpc_constant &quot;GDP per capita, PPP (constant 2011 international $) &quot; ## 6 BG.GSR.NFSV.GD.ZS &quot;Trade in services (% of GDP)&quot; ## 7 BG.KAC.FNEI.GD.PP.ZS &quot;Gross private capital flows (% of GDP, PPP)&quot; ## 8 BG.KAC.FNEI.GD.ZS &quot;Gross private capital flows (% of GDP)&quot; ## 9 BG.KLT.DINV.GD.PP.ZS &quot;Gross foreign direct investment (% of GDP, PPP)&quot; ## 10 BG.KLT.DINV.GD.ZS &quot;Gross foreign direct investment (% of GDP)&quot; ## # … with 530 more rows B.2.4.4 Population WDIsearch(string = &quot;Population, Total&quot;, field = &quot;name&quot;, cache = NULL) ## indicator name ## 9659 JI.POP.URBN.ZS Urban population, total (% of total population) ## 17674 SP.POP.TOTL Population, total WDIsearch(string = &quot;SP.POP.TOTL&quot;, field = &quot;indicator&quot;, cache = NULL) ## indicator name ## 17674 SP.POP.TOTL Population, total ## 17675 SP.POP.TOTL.FE.IN Population, female ## 17676 SP.POP.TOTL.FE.ZS Population, female (% of total population) ## 17677 SP.POP.TOTL.ICP SP.POP.TOTL.ICP:Population ## 17678 SP.POP.TOTL.ICP.ZS SP.POP.TOTL.ICP.ZS:Population shares (World=100) ## 17679 SP.POP.TOTL.MA.IN Population, male ## 17680 SP.POP.TOTL.MA.ZS Population, male (% of total population) ## 17681 SP.POP.TOTL.ZS Population (% of total) B.2.4.5 More Than One Indicator dfp &lt;- as_tibble(WDI( country = c(&quot;CN&quot;, &quot;IN&quot;, &quot;US&quot;, &quot;ID&quot;,&quot;PK&quot;, &quot;BR&quot;, &quot;NG&quot;, &quot;BD&quot;, &quot;RU&quot;, &quot;MX&quot;, &quot;JP&quot;), indicator = c(&quot;NY.GDP.PCAP.KD&quot;,&quot;SP.POP.TOTL&quot;), start = 1960, end = 2020, extra = FALSE, cache = NULL, latest = NULL, language = &quot;en&quot; )) dfp ## # A tibble: 671 × 6 ## country iso2c iso3c year NY.GDP.PCAP.KD SP.POP.TOTL ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Bangladesh BD BGD 1960 463. 48013505 ## 2 Bangladesh BD BGD 1961 478. 49362834 ## 3 Bangladesh BD BGD 1962 490. 50752150 ## 4 Bangladesh BD BGD 1963 474. 52202008 ## 5 Bangladesh BD BGD 1964 511. 53741721 ## 6 Bangladesh BD BGD 1965 504. 55385114 ## 7 Bangladesh BD BGD 1966 501. 57157651 ## 8 Bangladesh BD BGD 1967 476. 59034250 ## 9 Bangladesh BD BGD 1968 505. 60918452 ## 10 Bangladesh BD BGD 1969 497. 62679765 ## # … with 661 more rows ggplot(dfp) + geom_line(aes(x = year, y = SP.POP.TOTL, color = country)) + labs(title = &quot;Population, total&quot;) ggplot(dfp) + geom_line(aes(x = year, y = SP.POP.TOTL, color = country)) + scale_y_continuous(trans=&#39;log10&#39;) + labs(title = &quot;Population, total&quot;, subtitle = &quot;Log10 Scale&quot;) B.2.4.5.1 All Countries df_gdp_all &lt;- as_tibble(WDI( country = &quot;all&quot;, indicator = c(&quot;NY.GDP.PCAP.KD&quot;,&quot;SP.POP.TOTL&quot;), start = 1960, end = 2020, extra = FALSE, cache = NULL, latest = NULL, language = &quot;en&quot; )) df_gdp_all ## # A tibble: 16,226 × 6 ## country iso2c iso3c year NY.GDP.PCAP.KD SP.POP.TOTL ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan AF AFG 1960 NA 8996967 ## 2 Afghanistan AF AFG 1961 NA 9169406 ## 3 Afghanistan AF AFG 1962 NA 9351442 ## 4 Afghanistan AF AFG 1963 NA 9543200 ## 5 Afghanistan AF AFG 1964 NA 9744772 ## 6 Afghanistan AF AFG 1965 NA 9956318 ## 7 Afghanistan AF AFG 1966 NA 10174840 ## 8 Afghanistan AF AFG 1967 NA 10399936 ## 9 Afghanistan AF AFG 1968 NA 10637064 ## 10 Afghanistan AF AFG 1969 NA 10893772 ## # … with 16,216 more rows df_gdp_all %&gt;% filter(year == 2020) %&gt;% arrange(desc(SP.POP.TOTL)) ## # A tibble: 266 × 6 ## country iso2c iso3c year NY.GDP.PCAP.KD SP.POP.TOTL ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 World 1W &quot;WLD&quot; 2020 10549. 7763932702 ## 2 IDA &amp; IBRD total ZT &quot;IBT&quot; 2020 5017. 6571053159 ## 3 Low &amp; middle income XO &quot;LMY&quot; 2020 4862. 6494812232 ## 4 Middle income XP &quot;MIC&quot; 2020 5341. 5811279241 ## 5 IBRD only XF &quot;IBD&quot; 2020 6290. 4862446431 ## 6 Early-demographic dividend V2 &quot;EAR&quot; 2020 3374. 3332103561 ## 7 Lower middle income XN &quot;&quot; 2020 2297. 3318682068 ## 8 Upper middle income XT &quot;&quot; 2020 9395. 2492597173 ## 9 East Asia &amp; Pacific Z4 &quot;EAS&quot; 2020 11136. 2361517682 ## 10 Late-demographic dividend V3 &quot;LTE&quot; 2020 9711. 2316803603 ## # … with 256 more rows df_gdp_all %&gt;% filter(year == 2020) %&gt;% arrange(NY.GDP.PCAP.KD) ## # A tibble: 266 × 6 ## country iso2c iso3c year NY.GDP.PCAP.KD SP.POP.TOTL ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Burundi BI BDI 2020 271. 11890781 ## 2 Malawi MW MWI 2020 394. 19129955 ## 3 Central African Republic CF CAF 2020 415. 4829764 ## 4 Madagascar MG MDG 2020 442. 27691019 ## 5 Somalia SO SOM 2020 445. 15893219 ## 6 Congo, Dem. Rep. CD COD 2020 505. 89561404 ## 7 Niger NE NER 2020 523. 24206636 ## 8 Afghanistan AF AFG 2020 530. 38928341 ## 9 Mozambique MZ MOZ 2020 575. 31255435 ## 10 Liberia LR LBR 2020 616. 5057677 ## # … with 256 more rows df_gdp_all %&gt;% filter(year == 2020) %&gt;% arrange(desc(NY.GDP.PCAP.KD)) ## # A tibble: 266 × 6 ## country iso2c iso3c year NY.GDP.PCAP.KD SP.POP.TOTL ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Monaco MC MCO 2020 159222. 39244 ## 2 Luxembourg LU LUX 2020 104879. 630419 ## 3 Bermuda BM BMU 2020 99729. 63893 ## 4 Switzerland CH CHE 2020 85685. 8636561 ## 5 Ireland IE IRL 2020 78733. 4985674 ## 6 Cayman Islands KY CYM 2020 77959. 65720 ## 7 Norway NO NOR 2020 75017. 5379475 ## 8 Singapore SG SGP 2020 58982. 5685807 ## 9 United States US USA 2020 58060. 331501080 ## 10 Australia AU AUS 2020 58030. 25693267 ## # … with 256 more rows df_gdp_all %&gt;% filter(year == 2020) %&gt;% mutate(GDP = NY.GDP.PCAP.KD * SP.POP.TOTL) %&gt;% arrange(desc(GDP)) ## # A tibble: 266 × 7 ## country iso2c iso3c year NY.GDP.PCAP.KD SP.POP.T…¹ GDP ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 World 1W &quot;WLD&quot; 2020 10549. 7763932702 8.19e13 ## 2 High income XD &quot;&quot; 2020 40336. 1240684527 5.00e13 ## 3 OECD members OE &quot;OED&quot; 2020 35869. 1372980201 4.92e13 ## 4 Post-demographic dividend V4 &quot;PST&quot; 2020 41227. 1117278019 4.61e13 ## 5 IDA &amp; IBRD total ZT &quot;IBT&quot; 2020 5017. 6571053159 3.30e13 ## 6 Low &amp; middle income XO &quot;LMY&quot; 2020 4862. 6494812232 3.16e13 ## 7 Middle income XP &quot;MIC&quot; 2020 5341. 5811279241 3.10e13 ## 8 IBRD only XF &quot;IBD&quot; 2020 6290. 4862446431 3.06e13 ## 9 East Asia &amp; Pacific Z4 &quot;EAS&quot; 2020 11136. 2361517682 2.63e13 ## 10 Upper middle income XT &quot;&quot; 2020 9395. 2492597173 2.34e13 ## # … with 256 more rows, and abbreviated variable name ¹​SP.POP.TOTL B.2.4.6 Gender as_tibble(WDIsearch(string = &quot;Gender&quot;, field = &quot;name&quot;, cache = NULL)) ## # A tibble: 382 × 2 ## indicator name ## &lt;chr&gt; &lt;chr&gt; ## 1 2.3_GIR.GPI &quot;Gender parity index for gross intake ratio in grade 1&quot; ## 2 2.6_PCR.GPI &quot;Gender parity index for primary completion rate &quot; ## 3 5.51.01.07.gender &quot;Gender equality&quot; ## 4 BI.EMP.PWRK.PB.FE.ZS &quot;Public sector employment, as a share of paid employmen… ## 5 BI.EMP.PWRK.PB.MA.ZS &quot;Public sector employment, as a share of paid employmen… ## 6 BI.EMP.TOTL.PB.FE.ZS &quot;Public sector employment, as a share of total employme… ## 7 BI.EMP.TOTL.PB.MA.ZS &quot;Public sector employment, as a share of total employme… ## 8 BI.WAG.PREM.PB.FE &quot;Public sector wage premium, by gender: Female (compare… ## 9 BI.WAG.PREM.PB.FM &quot;P-Value: Public sector wage premium, by gender (compar… ## 10 BI.WAG.PREM.PB.FM.ED &quot;P-Value: Gender wage premium in the public sector, by … ## # … with 372 more rows as_tibble(WDIsearch(string = &quot;Females&quot;, field = &quot;name&quot;, cache = NULL)) ## # A tibble: 69 × 2 ## indicator name ## &lt;chr&gt; &lt;chr&gt; ## 1 BI.PWK.PRVS.CK.FE.ZS Females, as a share of private paid employees by occupa… ## 2 BI.PWK.PRVS.EO.FE.ZS Females, as a share of private paid employees by occupa… ## 3 BI.PWK.PRVS.FE.Q1.ZS Females, as a share of private paid employees by wage q… ## 4 BI.PWK.PRVS.FE.Q2.ZS Females, as a share of private paid employees by wage q… ## 5 BI.PWK.PRVS.FE.Q3.ZS Females, as a share of private paid employees by wage q… ## 6 BI.PWK.PRVS.FE.Q4.ZS Females, as a share of private paid employees by wage q… ## 7 BI.PWK.PRVS.FE.Q5.ZS Females, as a share of private paid employees by wage q… ## 8 BI.PWK.PRVS.FE.ZS Females, as a share of private paid employees ## 9 BI.PWK.PRVS.PN.FE.ZS Females, as a share of private paid employees by occupa… ## 10 BI.PWK.PRVS.SN.FE.ZS Females, as a share of private paid employees by occupa… ## # … with 59 more rows WDIsearch(string = &quot;BI.PWK.PRVS.FE.ZS&quot;, field = &quot;indicator&quot;, cache = NULL) ## indicator name ## 1636 BI.PWK.PRVS.FE.ZS Females, as a share of private paid employees WDIsearch(string = &quot;BI.PWK.PUBS.FE.ZS&quot;, field = &quot;indicator&quot;, cache = NULL) ## indicator name ## 1659 BI.PWK.PUBS.FE.ZS Females, as a share of public paid employees # Rename indicators on the fly dfwe &lt;- as_tibble(WDI(country = c(&quot;CN&quot;, &quot;IN&quot;, &quot;US&quot;, &quot;ID&quot;,&quot;PK&quot;, &quot;BR&quot;, &quot;NG&quot;, &quot;BD&quot;, &quot;RU&quot;, &quot;MX&quot;, &quot;JP&quot;), indicator = c(&#39;women_private_sector&#39; = &#39;BI.PWK.PRVS.FE.ZS&#39;, &#39;women_public_sector&#39; = &#39;BI.PWK.PUBS.FE.ZS&#39;))) dfwe ## # A tibble: 231 × 6 ## country iso2c iso3c year women_private_sector women_public_sector ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Bangladesh BD BGD 2000 NA NA ## 2 Bangladesh BD BGD 2001 NA NA ## 3 Bangladesh BD BGD 2002 NA NA ## 4 Bangladesh BD BGD 2003 0.193 0.231 ## 5 Bangladesh BD BGD 2004 NA NA ## 6 Bangladesh BD BGD 2005 NA NA ## 7 Bangladesh BD BGD 2006 NA NA ## 8 Bangladesh BD BGD 2007 NA NA ## 9 Bangladesh BD BGD 2008 NA NA ## 10 Bangladesh BD BGD 2009 NA NA ## # … with 221 more rows summary(dfwe, country) ## country iso2c iso3c year ## Length:231 Length:231 Length:231 Min. :2000 ## Class :character Class :character Class :character 1st Qu.:2005 ## Mode :character Mode :character Mode :character Median :2010 ## Mean :2010 ## 3rd Qu.:2015 ## Max. :2020 ## ## women_private_sector women_public_sector ## Min. :0.08965 Min. :0.1285 ## 1st Qu.:0.30555 1st Qu.:0.4159 ## Median :0.40629 Median :0.5508 ## Mean :0.35629 Mean :0.4679 ## 3rd Qu.:0.46033 3rd Qu.:0.5760 ## Max. :0.58404 Max. :0.6583 ## NA&#39;s :153 NA&#39;s :152 dfwe %&gt;% group_by(country) %&gt;% select(2,4,5,6) %&gt;% summarize(private_n = sum(is.na(women_private_sector)), private = mean(is.na(women_private_sector)), public_n = sum(is.na(women_public_sector)), public = mean(is.na(women_public_sector))) ## Adding missing grouping variables: `country` ## # A tibble: 11 × 5 ## country private_n private public_n public ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Bangladesh 17 0.810 17 0.810 ## 2 Brazil 6 0.286 6 0.286 ## 3 China 19 0.905 19 0.905 ## 4 India 17 0.810 17 0.810 ## 5 Indonesia 20 0.952 20 0.952 ## 6 Japan 21 1 21 1 ## 7 Mexico 5 0.238 5 0.238 ## 8 Nigeria 19 0.905 18 0.857 ## 9 Pakistan 13 0.619 13 0.619 ## 10 Russia 4 0.190 4 0.190 ## 11 United States 12 0.571 12 0.571 B.2.5 World Bank Country and Lending Groups and an Option extra = TRUE B.2.5.1 Review Basics: World Development Indicators: ?WDI Basic Usage WDI(country = &quot;all&quot;, indicator = &quot;NY.GDP.PCAP.KD&quot;) Vector Notation WDI(country = c(&quot;US&quot;, &quot;CN&quot;, JP&quot;), # ISO-2 codes indicator = c(&quot;gdp_pcap&quot; = &quot;NY.GDP.PCAP.KD&quot;, &quot;life_exp&quot; = &quot;SP.DYN.LE00.IN&quot;)) Use Extra WDI(country = &quot;all&quot;, indicator = c(&quot;gdp_pcap&quot; = NY.GDP.PCAP.KD&quot;, &quot;life_exp&quot; = &quot;SP.DYN.LE00.IN&quot;), extra = TRUE) extra: TRUE returns extra variables such as region, iso3c code, and incomeLevel B.2.5.2 World Bank Country and Lending Groups URL World Bank Country and Lending Groups The current classification by income in XLS format B.2.5.2.1 About CLASS.xls This table classifies all World Bank member countries (189), and all other economies with populations of more than 30,000. For operational and analytical purposes, economies are divided among income groups according to 2019 gross national income (GNI) per capita, calculated using the World Bank Atlas method. The groups are: low income, $1,035 or less; lower middle income, $1,036 - 4,045; upper middle income, $4,046 - 12,535; and high income, $12,536 or more. The effective operational cutoff for IDA eligibility is $1,185 or less. B.2.5.2.2 Geographic classifications IBRD: International Bank for Reconstruction and Development IDA: International Development Association   IDA countries are those that lack the financial ability to borrow from IBRD. IDA credits are deeply concessional—interest-free loans and grants for programs aimed at boosting economic growth and improving living conditions. IBRD loans are noncessional. Blend countries are eligible for IDA credits because of their low per capita incomes but are also eligible for IBRD because they are financially creditworthy. B.2.5.2.3 Note The term country, used interchangeably with economy, does not imply political independence but refers to any territory for which authorities report separate social or economic statistics. Income classifications set on 1 July 2020 remain in effect until 1 July 2021. Argentina, which was temporarily unclassified in July 2016 pending release of revised national accounts statistics, was classified as upper middle income for FY17 as of 29 September 2016 based on alternative conversion factors. Also effective 29 September 2016, Syrian Arab Republic is reclassified from IBRD lending category to IDA-only. On 29 March 2017, new country codes were introduced to align World Bank 3-letter codes with ISO 3-letter codes: Andorra (AND), Dem. Rep. Congo (COD), Isle of Man (IMN), Kosovo (XKX), Romania (ROU), Timor-Leste (TLS), and West Bank and Gaza (PSE). B.2.5.3 Importing Excel Files CLASS.xlsx: - copy the following link The current classification by income in XLS format readxl: https://readxl.tidyverse.org Help: read_excel, read_xls, read_xlsx url_class &lt;- &quot;https://databankfiles.worldbank.org/data/download/site-content/CLASS.xlsx&quot; download.file(url = url_class, destfile = &quot;data/CLASS.xlsx&quot;) B.2.5.3.1 Countries Let us look at the first sheet. The column names are in the 5th row. The country data starts from the 7th row. Zimbabue is at the last row. library(readxl) wb_countries_tmp &lt;- read_excel(&quot;data/CLASS.xlsx&quot;, sheet = 1, skip = 0, n_max =219) wb_countries &lt;- wb_countries_tmp %&gt;% select(country = Economy, iso3c = Code, region = Region, income = `Income group`, lending = &quot;Lending category&quot;, other = &quot;Other (EMU or HIPC)&quot;) wb_countries ## # A tibble: 218 × 6 ## country iso3c region income lending other ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Aruba ABW Latin America &amp; Caribbean High inc… &lt;NA&gt; &lt;NA&gt; ## 2 Afghanistan AFG South Asia Low inco… IDA HIPC ## 3 Angola AGO Sub-Saharan Africa Lower mi… IBRD &lt;NA&gt; ## 4 Albania ALB Europe &amp; Central Asia Upper mi… IBRD &lt;NA&gt; ## 5 Andorra AND Europe &amp; Central Asia High inc… &lt;NA&gt; &lt;NA&gt; ## 6 United Arab Emirates ARE Middle East &amp; North Africa High inc… &lt;NA&gt; &lt;NA&gt; ## 7 Argentina ARG Latin America &amp; Caribbean Upper mi… IBRD &lt;NA&gt; ## 8 Armenia ARM Europe &amp; Central Asia Upper mi… IBRD &lt;NA&gt; ## 9 American Samoa ASM East Asia &amp; Pacific Upper mi… &lt;NA&gt; &lt;NA&gt; ## 10 Antigua and Barbuda ATG Latin America &amp; Caribbean High inc… IBRD &lt;NA&gt; ## # … with 208 more rows B.2.5.3.2 Regions readxl: https://readxl.tidyverse.org Help: read_excel, read_xls, read_xlsx Regions start from the 227th row. Regions end at the 272th row. wb_regions_tmp &lt;- read_excel(&quot;data/CLASS.xlsx&quot;, sheet = 1, skip = 0, n_max =266) %&gt;% slice(-(1:220)) wb_regions &lt;- wb_regions_tmp %&gt;% select(region = Economy, iso3c = Code) %&gt;% drop_na() wb_regions ## # A tibble: 45 × 2 ## region iso3c ## &lt;chr&gt; &lt;chr&gt; ## 1 Caribbean small states CSS ## 2 Central Europe and the Baltics CEB ## 3 Early-demographic dividend EAR ## 4 East Asia &amp; Pacific EAS ## 5 East Asia &amp; Pacific (excluding high income) EAP ## 6 East Asia &amp; Pacific (IDA &amp; IBRD) TEA ## 7 Euro area EMU ## 8 Europe &amp; Central Asia ECS ## 9 Europe &amp; Central Asia (excluding high income) ECA ## 10 Europe &amp; Central Asia (IDA &amp; IBRD) TEC ## # … with 35 more rows Let us look at the second sheet. wb_groups_tmp &lt;- read_excel(&quot;data/CLASS.xlsx&quot;, sheet = &quot;Groups&quot;) # sheet = 3 wb_groups &lt;- wb_groups_tmp %&gt;% select(gcode = GroupCode, group = GroupName, iso3c = CountryCode, country = CountryName) B.2.5.4 Filtering Join Description Filtering joins filter rows from x based on the presence or absence of matches in y: semi_join() return all rows from x with a match in y. anti_join() return all rows from x without a match in y. https://dplyr.tidyverse.org/reference/filter-joins.html gdp_pcap &lt;- WDI(country = &quot;all&quot;, indicator = &quot;NY.GDP.PCAP.KD&quot;) head(gdp_pcap) ## country iso2c iso3c year NY.GDP.PCAP.KD ## 1 Africa Eastern and Southern ZH AFE 2021 1477.249 ## 2 Africa Eastern and Southern ZH AFE 2020 1452.730 ## 3 Africa Eastern and Southern ZH AFE 2019 1534.890 ## 4 Africa Eastern and Southern ZH AFE 2018 1544.078 ## 5 Africa Eastern and Southern ZH AFE 2017 1546.796 ## 6 Africa Eastern and Southern ZH AFE 2016 1548.813 gdp_pcap_extra &lt;- WDI(country = &quot;all&quot;, indicator = &quot;NY.GDP.PCAP.KD&quot;, extra = TRUE) head(gdp_pcap_extra) ## country iso2c iso3c year NY.GDP.PCAP.KD status lastupdated region ## 1 Afghanistan AF AFG 2015 556.0072 2022-09-16 South Asia ## 2 Afghanistan AF AFG 2011 511.9985 2022-09-16 South Asia ## 3 Afghanistan AF AFG 2014 565.1793 2022-09-16 South Asia ## 4 Afghanistan AF AFG 2013 568.9645 2022-09-16 South Asia ## 5 Afghanistan AF AFG 2012 557.9497 2022-09-16 South Asia ## 6 Afghanistan AF AFG 2007 392.7105 2022-09-16 South Asia ## capital longitude latitude income lending ## 1 Kabul 69.1761 34.5228 Low income IDA ## 2 Kabul 69.1761 34.5228 Low income IDA ## 3 Kabul 69.1761 34.5228 Low income IDA ## 4 Kabul 69.1761 34.5228 Low income IDA ## 5 Kabul 69.1761 34.5228 Low income IDA ## 6 Kabul 69.1761 34.5228 Low income IDA Compare the following: 16,226 rows = 13,054 rows (country) + 3172 rows (not country) gdp_pcap_country &lt;- gdp_pcap %&gt;% semi_join(wb_countries, by = &quot;country&quot;) head(gdp_pcap_country) ## country iso2c iso3c year NY.GDP.PCAP.KD ## 1 Afghanistan AF AFG 2021 NA ## 2 Afghanistan AF AFG 2020 529.7412 ## 3 Afghanistan AF AFG 2019 555.1390 ## 4 Afghanistan AF AFG 2018 546.7430 ## 5 Afghanistan AF AFG 2017 553.3551 ## 6 Afghanistan AF AFG 2016 552.9969 gdp_pcap_extra %&gt;% filter(region != &quot;Aggregates&quot;) %&gt;% slice(10) ## country iso2c iso3c year NY.GDP.PCAP.KD status lastupdated region ## 1 Afghanistan AF AFG 2003 332.22 2022-09-16 South Asia ## capital longitude latitude income lending ## 1 Kabul 69.1761 34.5228 Low income IDA What is the difference? 13,176 rows is not equal to 13,054 rows + 183 rows gdp_pcap_extra %&gt;% filter(region != &quot;Aggregates&quot;) %&gt;% anti_join(gdp_pcap_country) %&gt;% slice(10) ## Joining, by = c(&quot;country&quot;, &quot;iso2c&quot;, &quot;iso3c&quot;, &quot;year&quot;, &quot;NY.GDP.PCAP.KD&quot;) ## country iso2c iso3c year NY.GDP.PCAP.KD status lastupdated ## 1 Cote d&#39;Ivoire CI CIV 2003 1566.577 2022-09-16 ## region capital longitude latitude income ## 1 Sub-Saharan Africa Yamoussoukro -4.0305 5.332 Lower middle income ## lending ## 1 IDA gdp_pcap %&gt;% anti_join(wb_countries) %&gt;% slice(10) ## Joining, by = c(&quot;country&quot;, &quot;iso3c&quot;) ## country iso2c iso3c year NY.GDP.PCAP.KD ## 1 Africa Eastern and Southern ZH AFE 2012 1513.37 Note: World Bank provides the WDI_csv for more information. WDICountry-Series.csv - 965,9KB WDICountry.csv - 125.8KB WDIData.csv - 193 MB WDIFootNote.csv - 61.1MB WDISeries-Time.csv - 46.1KB WDISeries.csv” - 3.5MB B.2.5.5 Join Tables There are three types of joining tables. Commands are from tidyverse packages though there is a way to do the same by Base R with appropriate arguments. Bind rows: bind_rows(), intersect(), setdiff(), union() Bind columns: bind_cols(), left_join(), right_join(), inner_join(), full_join() Filtering join: semi_join(), anti_join() https://dplyr.tidyverse.org/reference/bind.html B.2.5.6 Join Tables: Quick References https://r4ds.had.co.nz/relational-data.html#relational-data Cheatsheet: Data Transformation, pages 2 and 3. You can download it from RStudio &gt; Help. Tidyverse Homepage: Efficiently bind multiple data frames by row and column: bind_rows(), bind_cols() Set operations: intersect(), setdiff(), union() Mutating joins: left_join(), right_join(), inner_join(), full_join() Filtering joins: semi_join(), anti_join() R Studio Primers: Tidy Your Data – r4ds: Wrangle, II Reshape Data, Separate and Unite Columns, Join Data Sets B.3 United Nations ` * UN Data: https://data.un.org - Datamarts: http://data.un.org/Explorer.aspx B.3.1 Importing Data Get the URL (uniform resource locator) - copy the link url_of_data &lt;- \"https://data.un.org/--long url--.csv\" Download the file into the destfile in data folder: download.file(url = url_of_data, destfile = \"data/un_pop.csv\") Read the file: df_un_pop &lt;- read_csv(\"data/un_pop.csv\") Alternative, skip 2 and read the file using the URL. * read_csv(url) B.3.2 Example B.3.2.1 Population Population, surface area and density in PDF The followind do not look line the pdf above. df_un_pop &lt;- read_csv(&quot;https://data.un.org/_Docs/SYB/CSV/SYB64_1_202110_Population,%20Surface%20Area%20and%20Density.csv&quot;, skip = 1) ## New names: ## Rows: 7260 Columns: 7 ## ── Column specification ## ────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; chr (4): ...2, Series, Footnotes, Source dbl (2): Region/Country/Area, Year num (1): Value ## ℹ Use `spec()` to retrieve the full column specification for this data. ℹ Specify the column types or set `show_col_types = ## FALSE` to quiet this message. ## • `` -&gt; `...2` head(df_un_pop) ## # A tibble: 6 × 7 ## `Region/Country/Area` ...2 Year Series Value Footn…¹ Source ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Total, all countries … 2010 Popul… 6957. &lt;NA&gt; Unite… ## 2 1 Total, all countries … 2010 Popul… 3508. &lt;NA&gt; Unite… ## 3 1 Total, all countries … 2010 Popul… 3449. &lt;NA&gt; Unite… ## 4 1 Total, all countries … 2010 Sex r… 102. &lt;NA&gt; Unite… ## 5 1 Total, all countries … 2010 Popul… 27 &lt;NA&gt; Unite… ## 6 1 Total, all countries … 2010 Popul… 11 &lt;NA&gt; Unite… ## # … with abbreviated variable name ¹​Footnotes colnames(df_un_pop) ## [1] &quot;Region/Country/Area&quot; &quot;...2&quot; &quot;Year&quot; ## [4] &quot;Series&quot; &quot;Value&quot; &quot;Footnotes&quot; ## [7] &quot;Source&quot; un_pop_tbl &lt;- df_un_pop %&gt;% select(num = &quot;Region/Country/Area&quot;, region = &quot;...2&quot;, year = &quot;Year&quot;, series = &quot;Series&quot;, value = &quot;Value&quot;) %&gt;% pivot_wider(names_from = series, values_from = value) head(un_pop_tbl) ## # A tibble: 6 × 11 ## num region year Popul…¹ Popul…² Popul…³ Sex r…⁴ Popul…⁵ Popul…⁶ Popul…⁷ ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Total, al… 2010 6957. 3508. 3449. 102. 27 11 53.5 ## 2 1 Total, al… 2015 7380. 3721. 3659. 102. 26.2 12.2 56.7 ## 3 1 Total, al… 2019 7713. 3889. 3824. 102. 25.6 13.2 59.3 ## 4 1 Total, al… 2021 7875. 3970. 3905. 102. 25.3 13.7 60.5 ## 5 2 Africa 2010 1039. 518. 521. 99.5 41.5 5.1 35.1 ## 6 2 Africa 2015 1182. 590. 592. 99.7 41.1 5.3 39.9 ## # … with 1 more variable: `Surface area (thousand km2)` &lt;dbl&gt;, and abbreviated ## # variable names ¹​`Population mid-year estimates (millions)`, ## # ²​`Population mid-year estimates for males (millions)`, ## # ³​`Population mid-year estimates for females (millions)`, ## # ⁴​`Sex ratio (males per 100 females)`, ## # ⁵​`Population aged 0 to 14 years old (percentage)`, ## # ⁶​`Population aged 60+ years old (percentage)`, ⁷​`Population density` colnames(un_pop_tbl) &lt;- c(&quot;num&quot;, &quot;region&quot;, &quot;year&quot;, &quot;total&quot;, &quot;male&quot;, &quot;female&quot;, &quot;ratio&quot;, &quot;0-14&quot;, &quot;60+&quot;, &quot;density&quot;, &quot;area&quot;) head(un_pop_tbl) ## # A tibble: 6 × 11 ## num region year total male female ratio `0-14` `60+` density area ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Total, all c… 2010 6957. 3508. 3449. 102. 27 11 53.5 NA ## 2 1 Total, all c… 2015 7380. 3721. 3659. 102. 26.2 12.2 56.7 136162 ## 3 1 Total, all c… 2019 7713. 3889. 3824. 102. 25.6 13.2 59.3 130094 ## 4 1 Total, all c… 2021 7875. 3970. 3905. 102. 25.3 13.7 60.5 NA ## 5 2 Africa 2010 1039. 518. 521. 99.5 41.5 5.1 35.1 NA ## 6 2 Africa 2015 1182. 590. 592. 99.7 41.1 5.3 39.9 30311 "],["model.html", "C Appendix C Modeling C.1 Exploration in Visualization C.2 Exploration in Visualization, IV C.3 Exploration and Data Modeling C.4 Examples", " C Appendix C Modeling Modeling using tidyverse EDA (A diagram from R4DS by H.W. and G.G.) EDA from r4ds Contents of EDA5 Data Science is empirical!? empirical: based on, concerned with, or verifiable by observation or experience rather than theory or pure logic Part I: Data Modeling Exploration in Visualization Modeling Scientific: Why? Prediction! - Do you support this? Evidence based! - What does it mean? What is regression, and why regression?  Linear Regression, ggplot2 Predictions and Residues Part II: Examples and Practicum C.1 Exploration in Visualization Reference: r4ds:EDA C.1.1 Exploration in Visualization, I EDA is an iterative cycle. You: Generate questions about your data. Search for answers by visualising, transforming, and modelling your data. - Methods in Data Science Use what you learn to refine your questions and/or generate new questions. “There are no routine statistical questions, only questionable statistical routines.” — Sir David Cox (1924.7.15-2022.1.18) Obituary: Sir David Cox, 1924-2022, Royal Statistical Society “Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.” — John Tukey (1915.6.16-2000.7.26) Exploratory Data Analysis cannot be done by statistical routines or a list of routine questions. C.1.2 Exploration in Visualization, II The very basic questions: What type of variation occurs within my variables? –one variable What type of covariation occurs between my variables? - two or more variables Typical values Which values are the most common? Why? median, mean, mode, etc. Which values are rare? Why? Does that match your expectations? outlier, exceptional Can you see any unusual patterns? What might explain them? C.1.3 Exploration in Visualization, III Clusters and Groups How are the observations within each cluster similar to each other? How are the observations in separate clusters different from each other? How can you explain or describe the clusters? Why might the appearance of clusters be misleading? Outliers and Unusual Values Sometimes outliers are data entry errors; other times outliers suggest important new science. C.2 Exploration in Visualization, IV Patterns and models Could this pattern be due to coincidence (i.e. random chance)? How can you describe the relationship implied by the pattern? How strong is the relationship implied by the pattern? What other variables might affect the relationship? Does the relationship change if you look at individual subgroups of the data? C.3 Exploration and Data Modeling Model is a simple summary of data Goal: A simple low-dimensional summary of a dataset. Ideally, the model will capture true “signals” (i.e. patterns generated by the phenomenon of interest), and ignore “noise” (i.e. random variation that you’re not interested in). “predictive” models: supervised “data discovery” models: unsupervised C.3.1 Hypothesis generation vs. hypothesis confirmation Each observation can either be used for exploration or confirmation, not both. You can use an observation as many times as you like for exploration, but you can only use it once for confirmation. As soon as you use an observation twice, you’ve switched from confirmation to exploration. If you are serious about doing an confirmatory analysis, one approach is to split your data into three pieces before you begin the analysis: 60% of your data goes into a training (or exploration) set. You’re allowed to do anything you like with this data: visualise it and fit tons of models to it. 20% goes into a query set. You can use this data to compare models or visualisations by hand, but you’re not allowed to use it as part of an automated process. 20% is held back for a test set. You can only use this data ONCE, to test your final model. C.3.2 Model Basics Reference: R4DS: Model Basics There are two parts to a model: First, you define a family of models that express a precise, but generic, pattern that you want to capture. For example, the pattern might be a straight line, or a quadratic curve. You will express the model family as an equation like \\(y = a_1 * x + a_2\\) or \\(y = a_1 * x ^ {a_2}\\). Here, \\(x\\) and \\(y\\) are known variables from your data, and \\(a_1\\) and \\(a_2\\) are parameters that can vary to capture different patterns. Next, you generate a fitted model by finding the model from the family that is the closest to your data. This takes the generic model family and makes it specific, like \\(y = 3 * x + 7\\) or \\(y = 9 * x ^ 2\\). It’s important to understand that a fitted model is just the closest model from a family of models. That implies that you have the “best” model (according to some criteria); it doesn’t imply that you have a good model and it certainly doesn’t imply that the model is “true”. All models are wrong, but some are useful. - George E.P. Box (1919-2013) Now it would be very remarkable if any system existing in the real world could be exactly represented by any simple model. However, cunningly chosen parsimonious models often do provide remarkably useful approximations. For example, the law PV = RT relating pressure P, volume V and temperature T of an “ideal” gas via a constant R is not exactly true for any real gas, but it frequently provides a useful approximation and furthermore its structure is informative since it springs from a physical view of the behavior of gas molecules. For such a model there is no need to ask the question “Is the model true?”. If “truth” is to be the “whole truth” the answer must be “No”. The only question of interest is “Is the model illuminating and useful?”. The goal of a model is not to uncover truth, but to discover a simple approximation that is still useful. C.3.2.1 Regression Analysis (wikipedia) In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (‘outcome variable’) and one or more independent variables (‘predictors’, ‘covariates’, or ‘features’). The most common form of regression analysis is linear regression, in which a researcher finds the line (or a more complex linear combination) that most closely fits the data according to a specific mathematical criterion. Two purposes of regression analysis: First, regression analysis is widely used for prediction and forecasting. Second, regression analysis can be used to infer causal relationships between the independent and dependent variables. Importantly, regressions by themselves only reveal relationships between a dependent variable and a collection of independent variables in a fixed dataset. To use regressions for prediction or to infer causal relationships, respectively, a researcher must carefully justify why existing relationships have predictive power for a new context or why a relationship between two variables has a causal interpretation. C.3.2.2 History of Regression Analysis (wikipedia) The earliest form of regression was the method of least squares, which was published by Legendre in 1805, and by Gauss in 1809. Legendre and Gauss both applied the method to the problem of determining, from astronomical observations, the orbits of bodies about the Sun (mostly comets, but also later the then newly discovered minor planets). Gauss published a further development of the theory of least squares in 1821, including a version of the Gauss–Markov theorem. The term “regression” was coined by Francis Galton in the nineteenth century to describe a biological phenomenon. The phenomenon was that the heights of descendants of tall ancestors tend to regress down towards a normal average (a phenomenon also known as regression toward the mean). For Galton, regression had only this biological meaning, but his work was later extended by Udny Yule and Karl Pearson to a more general statistical context. C.3.2.3 Galton’s Example We use HistData Package. See https://CRAN.R-project.org/package=HistData. library(tidyverse) library(modelr) library(datasets) library(HistData) Galton’s data on the heights of parents and their children, by child Description: This data set lists the individual observations for 934 children in 205 families on which Galton (1886) based his cross-tabulation. In addition to the question of the relation between heights of parents and their offspring, for which this data is mainly famous, Galton had another purpose which the data in this form allows to address: Does marriage selection indicate a relationship between the heights of husbands and wives, a topic he called assortative mating? Keen p. 297-298 provides a brief discussion of this topic. See Help: GaltonFamilies gf &lt;- as_tibble(GaltonFamilies) gf ## # A tibble: 934 × 8 ## family father mother midparentHeight children childNum gender childHeight ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 001 78.5 67 75.4 4 1 male 73.2 ## 2 001 78.5 67 75.4 4 2 female 69.2 ## 3 001 78.5 67 75.4 4 3 female 69 ## 4 001 78.5 67 75.4 4 4 female 69 ## 5 002 75.5 66.5 73.7 4 1 male 73.5 ## 6 002 75.5 66.5 73.7 4 2 male 72.5 ## 7 002 75.5 66.5 73.7 4 3 female 65.5 ## 8 002 75.5 66.5 73.7 4 4 female 65.5 ## 9 003 75 64 72.1 2 1 male 71 ## 10 003 75 64 72.1 2 2 female 68 ## # … with 924 more rows gf %&gt;% filter(gender == &quot;male&quot;) %&gt;% ggplot() + geom_point(aes(father, childHeight)) + labs(title = &quot;GaltonFamilies&quot;, x = &quot;father&#39;s height&quot;, y = &quot;son&#39;s height&quot;) gf %&gt;% filter(gender == &quot;female&quot;) %&gt;% ggplot() + geom_point(aes(mother, childHeight)) + labs(title = &quot;GaltonFamilies&quot;, x = &quot;mother&#39;s height&quot;, y = &quot;daughter&#39;s height&quot;) “The heights of descendants of tall ancestors tend to regress down towards a normal average (a phenomenon also known as regression toward the mean).” gf %&gt;% filter(gender == &quot;male&quot;) %&gt;% ggplot(aes(father, childHeight)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;GaltonFamilies&quot;, x = &quot;father&#39;s height&quot;, y = &quot;son&#39;s height&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; gf %&gt;% filter(gender == &quot;female&quot;) %&gt;% ggplot(aes(mother, childHeight)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(title = &quot;GaltonFamilies&quot;, x = &quot;mother&#39;s height&quot;, y = &quot;daughter&#39;s height&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; gf %&gt;% filter(gender == &quot;male&quot;) %&gt;% lm(childHeight ~ father, data = .) %&gt;% summary() ## ## Call: ## lm(formula = childHeight ~ father, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.3959 -1.5122 0.0413 1.6217 9.3808 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 38.36258 3.30837 11.596 &lt;2e-16 *** ## father 0.44652 0.04783 9.337 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.416 on 479 degrees of freedom ## Multiple R-squared: 0.154, Adjusted R-squared: 0.1522 ## F-statistic: 87.17 on 1 and 479 DF, p-value: &lt; 2.2e-16 gf %&gt;% filter(gender == &quot;female&quot;) %&gt;% lm(childHeight ~ mother, data = .) %&gt;% summary() ## ## Call: ## lm(formula = childHeight ~ mother, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.8749 -1.5340 0.0799 1.4434 6.7616 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 43.68897 3.00171 14.555 &lt; 2e-16 *** ## mother 0.31824 0.04676 6.805 3.22e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.246 on 451 degrees of freedom ## Multiple R-squared: 0.09313, Adjusted R-squared: 0.09111 ## F-statistic: 46.31 on 1 and 451 DF, p-value: 3.222e-11 midparentHeight: mid-parent height, calculated as (father + 1.08*mother)/2 gf %&gt;% filter(gender == &quot;male&quot;) %&gt;% lm(childHeight ~ midparentHeight, data = .) %&gt;% summary() ## ## Call: ## lm(formula = childHeight ~ midparentHeight, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.5431 -1.5160 0.1844 1.5082 9.0860 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 19.91346 4.08943 4.869 1.52e-06 *** ## midparentHeight 0.71327 0.05912 12.064 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.3 on 479 degrees of freedom ## Multiple R-squared: 0.2331, Adjusted R-squared: 0.2314 ## F-statistic: 145.6 on 1 and 479 DF, p-value: &lt; 2.2e-16 C.3.2.4 Summary of the Data gf %&gt;% summary() ## family father mother midparentHeight children ## 185 : 15 Min. :62.0 Min. :58.00 Min. :64.40 Min. : 1.000 ## 066 : 11 1st Qu.:68.0 1st Qu.:63.00 1st Qu.:68.14 1st Qu.: 4.000 ## 120 : 11 Median :69.0 Median :64.00 Median :69.25 Median : 6.000 ## 130 : 11 Mean :69.2 Mean :64.09 Mean :69.21 Mean : 6.171 ## 166 : 11 3rd Qu.:71.0 3rd Qu.:65.88 3rd Qu.:70.14 3rd Qu.: 8.000 ## 097 : 10 Max. :78.5 Max. :70.50 Max. :75.43 Max. :15.000 ## (Other):865 ## childNum gender childHeight ## Min. : 1.000 female:453 Min. :56.00 ## 1st Qu.: 2.000 male :481 1st Qu.:64.00 ## Median : 3.000 Median :66.50 ## Mean : 3.586 Mean :66.75 ## 3rd Qu.: 5.000 3rd Qu.:69.70 ## Max. :15.000 Max. :79.00 ## The following information can be found in summary above. gf %&gt;% summarize(n_father = sum(!is.na(father)), n_mother = sum(!is.na(mother)), n_male = sum(gender == &quot;male&quot;), n_female = sum(gender == &quot;female&quot;)) ## # A tibble: 1 × 4 ## n_father n_mother n_male n_female ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 934 934 481 453 C.3.2.5 What type of variation occurs within my variables? gf %&gt;% select(childHeight) %&gt;% summary() ## childHeight ## Min. :56.00 ## 1st Qu.:64.00 ## Median :66.50 ## Mean :66.75 ## 3rd Qu.:69.70 ## Max. :79.00 gf %&gt;% ggplot() + geom_boxplot(aes(x = childHeight)) geom_boxplot() gf %&gt;% ggplot() + geom_histogram(aes(x = childHeight)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. gf %&gt;% select(gender, childHeight) %&gt;% group_by(gender) %&gt;% summarize(q1 = quantile(childHeight, 0.25), med = median(childHeight), q3 = quantile(childHeight, 0.75), left = q1 - 1.5*(q3-q1), right = q3 + 1.5*(q3-q1)) ## # A tibble: 2 × 6 ## gender q1 med q3 left right ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 female 62.5 64 65.5 58 70 ## 2 male 67.5 69.2 71 62.2 76.2 gf %&gt;% ggplot() + geom_boxplot(aes(x = childHeight, y = gender)) gf %&gt;% ggplot() + geom_histogram(aes(x = childHeight, fill = gender), alpha = 0.7) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. gf %&gt;% ggplot() + geom_histogram(aes(x = childHeight, fill = gender)) + facet_wrap(vars(gender)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. mm &lt;- gf %&gt;% filter(gender == &quot;male&quot;) %&gt;% summarize(m = sum(!is.na(childHeight)), ave = mean(childHeight), se = sd(childHeight)) mm ## # A tibble: 1 × 3 ## m ave se ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 481 69.2 2.62 gf %&gt;% filter(gender == &quot;male&quot;) %&gt;% ggplot(aes(x = childHeight)) + geom_histogram() + geom_vline(xintercept = mm$ave, color = &quot;red&quot;) + geom_vline(xintercept = mm$ave - mm$se * 1.96, color = &quot;blue&quot;) + geom_vline(xintercept = mm$ave + mm$se * 1.96, color = &quot;blue&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. gf %&gt;% filter(gender == &quot;male&quot;) %&gt;% filter(childHeight &gt;= mm$ave - mm$se * 1.96) %&gt;% filter(childHeight &lt;= mm$ave + mm$se * 1.96) %&gt;% summarize(n = sum(!is.na(childHeight)), ratio = n/mm$m) ## # A tibble: 1 × 2 ## n ratio ## &lt;int&gt; &lt;dbl&gt; ## 1 458 0.952 gf %&gt;% filter(gender == &quot;male&quot;) %&gt;% ggplot(aes(sample = childHeight)) + stat_qq() + stat_qq_line(col = &quot;blue&quot;) gf %&gt;% filter(gender == &quot;female&quot;) %&gt;% ggplot(aes(sample = childHeight)) + stat_qq() + stat_qq_line(col = &quot;blue&quot;) gf %&gt;% ggplot(aes(sample = childHeight)) + stat_qq() + stat_qq_line(col = &quot;red&quot;) gf %&gt;% ggplot() + geom_freqpoly(aes(x = childHeight, color = gender)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. gf %&gt;% ggplot() + geom_histogram(aes(x = father), fill = &quot;blue&quot;, alpha = 0.5) + geom_histogram(aes(x = mother), fill = &quot;red&quot;, alpha = 0.5) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. gf %&gt;% ggplot() + geom_freqpoly(aes(x = father), color = &quot;blue&quot;) + geom_freqpoly(aes(x = mother), color = &quot;red&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. gf %&gt;% ggplot() + geom_boxplot(aes(x = gender, y = childHeight)) gf %&gt;% select(father, mother) %&gt;% pivot_longer(cols = everything(), names_to = &quot;parents&quot;, values_to = &quot;height&quot;) %&gt;% ggplot() + geom_boxplot(aes(x = parents, y = height)) ##### R commands on statistical measurements One Variable mean: average, i.e., the sum of values divided by the number of values. mean() or mean(x, na.rm = TRUE) median: middle value, i.e., the value separating the higher half from the lower half of a data sample. median() or median(x, na.rm = TRUE) quantile: quantile() or quantile(x, na.rm = TRUE) variance: spread from the mean i.e., the expectation of the squared standard deviation of a variable from its mean, or how much a set of numbers are spread out from the mean. var() or var(x, na.rm = TRUE) standard deviation: a measure of spread from the mean, i.e., a measure that is used to quantify the amount of variation or dispersion of a set of data value. The standard deviation is the square root of the variance. sd() More Than One variable covariance: a measure indicating the extent to which two random variables change in tandem (either positively or negatively). It’s unit is that of the variable. A large covariance can mean a strong relationship between variables. cov() correlation: a statistical measure that indicates how strongly two variables are related. It’s a scaled version of covariance. It’s value always lies between -1 and +1. cor(), cor(x, y, use = “everything”) A resulting value will be NA whenever one of its contributing observations is NA. `cor(x, y, use = “pairwise.complete.obs”) It uses all complete pairs of observations on those variables. summary() C.3.2.6 What type of covariation occurs between my variables? gf %&gt;% ggplot() + geom_point(aes(x = father, y = childHeight, color = gender)) gf %&gt;% ggplot() + geom_point(aes(x = father, y = childHeight, color = gender)) + facet_wrap(vars(gender)) gf %&gt;% select(father, mother, gender, childHeight) %&gt;% group_by(gender) %&gt;% summarize(&quot;cor_w/father&quot; = cor(father, childHeight), &quot;sq_w/father&quot; = cor(father, childHeight)^2, &quot;cor_w/mother&quot; = cor(mother, childHeight), &quot;sq_w/mother&quot; = cor(mother, childHeight)^2) ## # A tibble: 2 × 5 ## gender `cor_w/father` `sq_w/father` `cor_w/mother` `sq_w/mother` ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 female 0.428 0.184 0.305 0.0931 ## 2 male 0.392 0.154 0.323 0.104 Let \\(x = c(x_1, x_2, \\ldots, x_n)\\) be the independent variable, e.g., the height of fathers Let \\(y = c(y_1, y_2, \\ldots, y_n)\\) be the dependent variable, i.e., height of children Let \\(\\mbox{pred} = c(\\hat{y}_1, \\hat{y}_2, \\ldots, \\hat{y}_n)\\) be the predicted values by linear regression. \\[ \\begin{aligned} \\mbox{unbiased covariance: } cov(x,y) &amp;= \\frac{\\sum_{i=1}^n(x_i - mean(x))(y_i - mean(y))}{n-1}\\\\ \\mbox{unviased variance: } var(x) &amp;= cov(x,x) \\\\ \\mbox{correlation: } cor(x.y) &amp;= \\frac{cov(x,y)}{\\sqrt{var(x)var(y)}}\\\\ \\mbox{slope of the regression line} &amp;= \\frac{cov(x,y)}{var(x)} = \\frac{cor(x,y)\\sqrt{var(y)}}{\\sqrt{var(x)}}\\\\ \\mbox{total sum of squares} &amp;= SS_{tot} = \\sum_{i=1}^n((y_i-mean(y))^2)\\\\ \\mbox{residual sum of squares} &amp;= SS_{res} = \\sum_{i=1}^n((y_i-\\hat{y}_i)^2)\\\\ \\mbox{R squared} = R^2 &amp; = 1 - \\frac{SS_{res}}{SS_{tot}} = cor(x,y)^2 \\end{aligned} \\] gf %&gt;% ggplot(aes(father, childHeight, color = gender)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; gf %&gt;% ggplot(aes(father, childHeight, group = gender)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + facet_wrap(vars(gender)) ## `geom_smooth()` using formula = &#39;y ~ x&#39; mod_mf &lt;- gf %&gt;% filter(gender == &quot;male&quot;) %&gt;% lm(childHeight ~ father, .) summary(mod_mf) ## ## Call: ## lm(formula = childHeight ~ father, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.3959 -1.5122 0.0413 1.6217 9.3808 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 38.36258 3.30837 11.596 &lt;2e-16 *** ## father 0.44652 0.04783 9.337 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.416 on 479 degrees of freedom ## Multiple R-squared: 0.154, Adjusted R-squared: 0.1522 ## F-statistic: 87.17 on 1 and 479 DF, p-value: &lt; 2.2e-16 gf %&gt;% filter(gender == &quot;male&quot;) %&gt;% add_predictions(mod_mf) %&gt;% add_residuals(mod_mf) ## # A tibble: 481 × 10 ## family father mother midparentH…¹ child…² child…³ gender child…⁴ pred resid ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 001 78.5 67 75.4 4 1 male 73.2 73.4 -0.215 ## 2 002 75.5 66.5 73.7 4 1 male 73.5 72.1 1.42 ## 3 002 75.5 66.5 73.7 4 2 male 72.5 72.1 0.425 ## 4 003 75 64 72.1 2 1 male 71 71.9 -0.852 ## 5 004 75 64 72.1 5 1 male 70.5 71.9 -1.35 ## 6 004 75 64 72.1 5 2 male 68.5 71.9 -3.35 ## 7 005 75 58.5 69.1 6 1 male 72 71.9 0.148 ## 8 005 75 58.5 69.1 6 2 male 69 71.9 -2.85 ## 9 005 75 58.5 69.1 6 3 male 68 71.9 -3.85 ## 10 007 74 68 73.7 6 1 male 76.5 71.4 5.09 ## # … with 471 more rows, and abbreviated variable names ¹​midparentHeight, ## # ²​children, ³​childNum, ⁴​childHeight mod_ff &lt;- gf %&gt;% filter(gender == &quot;female&quot;) %&gt;% lm(childHeight ~ father, .) summary(mod_ff) ## ## Call: ## lm(formula = childHeight ~ father, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.6234 -1.5047 -0.0767 1.4953 6.7831 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.69497 2.62458 14.36 &lt;2e-16 *** ## father 0.38130 0.03787 10.07 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.131 on 451 degrees of freedom ## Multiple R-squared: 0.1836, Adjusted R-squared: 0.1817 ## F-statistic: 101.4 on 1 and 451 DF, p-value: &lt; 2.2e-16 gf %&gt;% filter(gender == &quot;female&quot;) %&gt;% add_predictions(mod_ff) %&gt;% add_residuals(mod_ff) ## # A tibble: 453 × 10 ## family father mother midparentH…¹ child…² child…³ gender child…⁴ pred resid ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 001 78.5 67 75.4 4 2 female 69.2 67.6 1.57 ## 2 001 78.5 67 75.4 4 3 female 69 67.6 1.37 ## 3 001 78.5 67 75.4 4 4 female 69 67.6 1.37 ## 4 002 75.5 66.5 73.7 4 3 female 65.5 66.5 -0.983 ## 5 002 75.5 66.5 73.7 4 4 female 65.5 66.5 -0.983 ## 6 003 75 64 72.1 2 2 female 68 66.3 1.71 ## 7 004 75 64 72.1 5 3 female 67 66.3 0.707 ## 8 004 75 64 72.1 5 4 female 64.5 66.3 -1.79 ## 9 004 75 64 72.1 5 5 female 63 66.3 -3.29 ## 10 005 75 58.5 69.1 6 4 female 66.5 66.3 0.207 ## # … with 443 more rows, and abbreviated variable names ¹​midparentHeight, ## # ²​children, ³​childNum, ⁴​childHeight C.3.2.6.1 Basic Facts \\(-1 \\leq cor(x,y) \\leq 1\\): positive (or negative) correlation If \\(cor(x,y)\\) is positive [resp. negative], the slope of the regression line is positive [resp. negative]. If the possibility of the slope of the regression line being zero is very low, \\(p\\)-value of the slope becomes very small. R squared is the square of \\(cor(x,y)\\) which is between 0 and 1. If the regression line fits well, R squared value is close to 1. What do you want to observe from this dataset? Reference: Introduction to Data Science by Rafael A. Irizarry, Chapter 18 Regression includes a Galton’s example. C.3.2.7 Visual Illustration C.3.2.7.1 Example 1. Strong positive correlation set.seed(12345) dt1 &lt;- as_tibble(MASS::mvrnorm(500, mu = c(0,0), Sigma = matrix(c(1, 0.8, 0.8, 1), 2,2))) ## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0. ## ℹ Using compatibility `.name_repair`. colnames(dt1) &lt;- c(&quot;V1&quot;, &quot;V2&quot;) dt1 %&gt;% ggplot() + geom_point(aes(x = V1, y = V2)) dt1 %&gt;% summarize(cor = cor(V1, V2), ave1 = mean(V1), ave2 = mean(V2)) ## # A tibble: 1 × 3 ## cor ave1 ave2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.794 0.0751 0.0814 dt1 %&gt;% ggplot(aes(V1, V2)) + geom_point() + geom_vline(xintercept = mean(dt1$V1), color = &quot;red&quot;) + geom_hline(yintercept = mean(dt1$V2), color = &quot;red&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; dt1 %&gt;% lm(V2~V1, .) %&gt;% summary() ## ## Call: ## lm(formula = V2 ~ V1, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.54654 -0.39681 0.01574 0.42024 2.27416 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.02194 0.02704 0.811 0.417 ## V1 0.79149 0.02717 29.133 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6028 on 498 degrees of freedom ## Multiple R-squared: 0.6302, Adjusted R-squared: 0.6295 ## F-statistic: 848.7 on 1 and 498 DF, p-value: &lt; 2.2e-16 C.3.2.7.2 Example 2. Weak positive correlation set.seed(12345) dt2 &lt;- as_tibble(MASS::mvrnorm(500, mu = c(0,0), Sigma = matrix(c(1, 0.2, 0.2, 1), 2,2))) colnames(dt2) &lt;- c(&quot;V1&quot;, &quot;V2&quot;) dt2 %&gt;% ggplot() + geom_point(aes(x = V1, y = V2)) dt2 %&gt;% summarize(cor = cor(V1, V2), ave1 = mean(V1), ave2 = mean(V2)) ## # A tibble: 1 × 3 ## cor ave1 ave2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.184 0.0576 0.0702 dt2 %&gt;% ggplot(aes(V1, V2)) + geom_point() + geom_vline(xintercept = mean(dt2$V1), color = &quot;red&quot;) + geom_hline(yintercept = mean(dt2$V2), color = &quot;red&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; dt2 %&gt;% lm(V2~V1, .) %&gt;% summary() ## ## Call: ## lm(formula = V2 ~ V1, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.7468 -0.6567 -0.0172 0.6584 3.9938 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.05962 0.04383 1.360 0.174 ## V1 0.18290 0.04384 4.173 3.56e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9785 on 498 degrees of freedom ## Multiple R-squared: 0.03378, Adjusted R-squared: 0.03184 ## F-statistic: 17.41 on 1 and 498 DF, p-value: 3.555e-05 C.3.2.7.3 Example 3. Strong negative correlation set.seed(12345) dt3 &lt;- as_tibble(MASS::mvrnorm(500, mu = c(0,0), Sigma = matrix(c(1, -0.8, -0.8, 1), 2,2))) colnames(dt3) &lt;- c(&quot;V1&quot;, &quot;V2&quot;) dt3 %&gt;% ggplot() + geom_point(aes(x = V1, y = V2)) dt3 %&gt;% summarize(cor = cor(V1, V2), ave1 = mean(V1), ave2 = mean(V2)) ## # A tibble: 1 × 3 ## cor ave1 ave2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.794 -0.0814 0.0751 dt3 %&gt;% ggplot(aes(V1, V2)) + geom_point() + geom_vline(xintercept = mean(dt1$V1), color = &quot;red&quot;) + geom_hline(yintercept = mean(dt1$V2), color = &quot;red&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; dt3 %&gt;% lm(V2~V1, .) %&gt;% summary() ## ## Call: ## lm(formula = V2 ~ V1, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.78510 -0.40850 0.00444 0.37938 1.84887 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.01030 0.02713 0.379 0.704 ## V1 -0.79624 0.02733 -29.133 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6046 on 498 degrees of freedom ## Multiple R-squared: 0.6302, Adjusted R-squared: 0.6295 ## F-statistic: 848.7 on 1 and 498 DF, p-value: &lt; 2.2e-16 C.3.2.7.4 Example 4. Weak negative correlation set.seed(12345) dt4 &lt;- as_tibble(MASS::mvrnorm(500, mu = c(0,0), Sigma = matrix(c(1, -0.2, -0.2, 1), 2,2))) colnames(dt4) &lt;- c(&quot;V1&quot;, &quot;V2&quot;) dt4 %&gt;% ggplot() + geom_point(aes(x = V1, y = V2)) dt4 %&gt;% summarize(cor = cor(V1, V2), ave1 = mean(V1), ave2 = mean(V2)) ## # A tibble: 1 × 3 ## cor ave1 ave2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.184 -0.0702 0.0576 dt4 %&gt;% ggplot(aes(V1, V2)) + geom_point() + geom_vline(xintercept = mean(dt1$V1), color = &quot;red&quot;) + geom_hline(yintercept = mean(dt1$V2), color = &quot;red&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; dt4 %&gt;% lm(V2~V1, .) %&gt;% summary() ## ## Call: ## lm(formula = V2 ~ V1, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.67763 -0.72072 -0.03253 0.67127 2.95515 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.04463 0.04408 1.012 0.312 ## V1 -0.18468 0.04426 -4.173 3.56e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9833 on 498 degrees of freedom ## Multiple R-squared: 0.03378, Adjusted R-squared: 0.03184 ## F-statistic: 17.41 on 1 and 498 DF, p-value: 3.555e-05 C.4 Examples C.4.1 cars data(cars) plot(cars) plot(cars) # cars: Speed and Stopping Distances of Cars abline(lm(cars$dist~cars$speed)) summary(lm(cars$dist~cars$speed)) ## ## Call: ## lm(formula = cars$dist ~ cars$speed) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.069 -9.525 -2.272 9.215 43.201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.5791 6.7584 -2.601 0.0123 * ## cars$speed 3.9324 0.4155 9.464 1.49e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.38 on 48 degrees of freedom ## Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 ## F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 C.4.1.1 Model Analysis with tidyverse: modelr Tidymodels: https://www.tidymodels.org modelr: https://modelr.tidyverse.org Compute model quality for a given dataset t_cars &lt;- as_tibble(cars) ggplot(t_cars) + geom_point(aes(speed, dist)) mod &lt;- lm(dist ~ speed, data = t_cars) mod ## ## Call: ## lm(formula = dist ~ speed, data = t_cars) ## ## Coefficients: ## (Intercept) speed ## -17.579 3.932 summary(mod) ## ## Call: ## lm(formula = dist ~ speed, data = t_cars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.069 -9.525 -2.272 9.215 43.201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.5791 6.7584 -2.601 0.0123 * ## speed 3.9324 0.4155 9.464 1.49e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.38 on 48 degrees of freedom ## Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 ## F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 Note that cor(speed, dist) = 0.8068949. ggplot(t_cars, aes(speed, dist)) + geom_point() + geom_abline(slope = coef(mod)[[2]], intercept = coef(mod)[[1]]) ggplot(t_cars, aes(speed, dist)) + geom_jitter() + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; C.4.1.2 Predictions \\(\\hat{y} = a_1 + a_2x\\) and Residuals \\(y - \\hat{y}\\) mod_table &lt;- t_cars %&gt;% add_predictions(mod) %&gt;% add_residuals(mod, var = &quot;resid&quot;) mod_table ## # A tibble: 50 × 4 ## speed dist pred resid ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 2 -1.85 3.85 ## 2 4 10 -1.85 11.8 ## 3 7 4 9.95 -5.95 ## 4 7 22 9.95 12.1 ## 5 8 16 13.9 2.12 ## 6 9 10 17.8 -7.81 ## 7 10 18 21.7 -3.74 ## 8 10 26 21.7 4.26 ## 9 10 34 21.7 12.3 ## 10 11 17 25.7 -8.68 ## # … with 40 more rows summary(mod_table) ## speed dist pred resid ## Min. : 4.0 Min. : 2.00 Min. :-1.849 Min. :-29.069 ## 1st Qu.:12.0 1st Qu.: 26.00 1st Qu.:29.610 1st Qu.: -9.525 ## Median :15.0 Median : 36.00 Median :41.407 Median : -2.272 ## Mean :15.4 Mean : 42.98 Mean :42.980 Mean : 0.000 ## 3rd Qu.:19.0 3rd Qu.: 56.00 3rd Qu.:57.137 3rd Qu.: 9.215 ## Max. :25.0 Max. :120.00 Max. :80.731 Max. : 43.201 mod_table %&gt;% ggplot() + geom_jitter(aes(speed, resid)) + geom_hline(yintercept = 0) geom_jitter() t_cars %&gt;% group_by(speed, dist) %&gt;% summarize(count = n()) %&gt;% arrange(desc(count)) ## `summarise()` has grouped output by &#39;speed&#39;. You can override using the ## `.groups` argument. ## # A tibble: 49 × 3 ## # Groups: speed [19] ## speed dist count ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 13 34 2 ## 2 4 2 1 ## 3 4 10 1 ## 4 7 4 1 ## 5 7 22 1 ## 6 8 16 1 ## 7 9 10 1 ## 8 10 18 1 ## 9 10 26 1 ## 10 10 34 1 ## # … with 39 more rows C.4.2 iris data(iris) t_iris &lt;- as_tibble(iris) t_iris ## # A tibble: 150 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # … with 140 more rows C.4.2.1 Linear Model: Sepal.Width ~ Sepal.Length lm(Sepal.Width ~ Sepal.Length, data = t_iris) ## ## Call: ## lm(formula = Sepal.Width ~ Sepal.Length, data = t_iris) ## ## Coefficients: ## (Intercept) Sepal.Length ## 3.41895 -0.06188 C.4.2.2 Linear Model: Petal.Width ~ Petal.Length lm(Petal.Width ~ Petal.Length, data = t_iris) ## ## Call: ## lm(formula = Petal.Width ~ Petal.Length, data = t_iris) ## ## Coefficients: ## (Intercept) Petal.Length ## -0.3631 0.4158 C.4.2.3 Correlation cor(t_iris[1:4]) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length 1.0000000 -0.1175698 0.8717538 0.8179411 ## Sepal.Width -0.1175698 1.0000000 -0.4284401 -0.3661259 ## Petal.Length 0.8717538 -0.4284401 1.0000000 0.9628654 ## Petal.Width 0.8179411 -0.3661259 0.9628654 1.0000000 C.4.2.4 filter(Species == \"setosa\") C.4.2.5 aes(Sepal.Length, Sepal.Width, color = Species) ggplot(t_iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; C.4.3 Model Analysis Warnings C.4.3.1 Linear Regression: aes(Sepal.Length, Sepal.Width, color = Species) C.4.3.2 aes(Sepal.Length, Sepal.Width, color = Species) C.4.3.3 Correlation and Covariance cor(t_iris[1:4]) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length 1.0000000 -0.1175698 0.8717538 0.8179411 ## Sepal.Width -0.1175698 1.0000000 -0.4284401 -0.3661259 ## Petal.Length 0.8717538 -0.4284401 1.0000000 0.9628654 ## Petal.Width 0.8179411 -0.3661259 0.9628654 1.0000000 cov(t_iris[1:4]) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length 0.6856935 -0.0424340 1.2743154 0.5162707 ## Sepal.Width -0.0424340 0.1899794 -0.3296564 -0.1216394 ## Petal.Length 1.2743154 -0.3296564 3.1162779 1.2956094 ## Petal.Width 0.5162707 -0.1216394 1.2956094 0.5810063 C.4.4 Regression Analyais: Summary R-Squared: Higher the better (&gt; 0.70) How well the prediction fit to the data. \\(0 \\leq R2 \\leq 1\\). For linear regression between two variables x and y, R-squared is the square of cor(x,y). R-Squared can be measured on any prediction model. t-statistic: The absolute value should be greater 1.96, and for p-value be less than 0.05 The model and the choice of variable(s) are suitable or not. p-value appears in Hypothesis testing (A/B test, sensitivity - specificity, etc.) C.4.4.1 The ASA Statement on p-Values: Context, Process, and Purpose P-values can indicate how incompatible the data are with a specified statistical model. P-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone. Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold. Proper inference requires full reporting and transparency. A p-value, or statistical significance, does not measure the size of an effect or the importance of a result. By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis. AMERICAN STATISTICAL ASSOCIATION RELEASES STATEMENT ON STATISTICAL SIGNIFICANCE AND P-VALUES C.4.4.2 Linear Regression Quick Reference Textbook: R for Data Science Introduction to Model: https://r4ds.had.co.nz/model-intro.html#model-intro Model Basics: https://r4ds.had.co.nz/model-basics.html#model-basics Introduction to Data Science by Rafael A. Irizarry, Chapters 18, 19: Regression: https://rafalab.github.io/dsbook/regression.html#regression Linear Models: https://rafalab.github.io/dsbook/linear-models.html#linear-models DataCamp: https://www.datacamp.com/community/tutorials/linear-regression-R What is a linear regression? Creating a linear regression in R. Learn the concepts of coefficients and residuals. How to test if your linear model has a good fit? Detecting influential points. DataCamp Top: https://www.datacamp.com - to be introduced r-statistics.co by Selva Prabhakaran: http://r-statistics.co/Linear-Regression.html The aim of linear regression is to model a continuous variable Y as a mathematical function of one or more X variable(s), so that we can use this regression model to predict the Y when only the X is known. This mathematical equation can be generalized as follows: \\(Y = \\beta_1 + \\beta_2 X + \\varepsilon\\) r-statistics.co Top: http://r-statistics.co An educational resource for those seeking knowledge related to machine learning and statistical computing in R. "],["coronavirus.html", "D Appendix D Coronavirus D.1 Introduction D.2 Exploration of Data with Base R D.3 Selection of Several Countries with ggplot2 D.4 Importing and Transforming Data with readr and dplyr in tidyverse Packages D.5 Data of Johns Hopkins Universiy and World Bank", " D Appendix D Coronavirus An example of an R Notebook, rendered at 2022-11-30 23:41:17 JST This version uses tidyverse packages for importing and transforming data. library(tidyverse) library(DT) D.1 Introduction The following site of Johns Hopkins University is famous: https://coronavirus.jhu.edu/map.html In this article, we study a coronavirus data collected by Johns Hopkins University called “JHU Covid-19 global time series data”. Since the original data requires reshaping, we use a data provided by RamiKrispin in the following site. https://github.com/RamiKrispin/coronavirus/tree/master/csv See also the R package coronavirus at https://CRAN.R-project.org/package=coronavirus For installation: `install.packages(“coronavirus”) To attach: library(coronavirus) We can directly download and read the data from: https://github.com/RamiKrispin/coronavirus/raw/master/csv/coronavirus.csv It is updated daily. In this note, we use the original JHU data and transform it using dplyr in the form similar to the Krispin’s. D.2 Exploration of Data with Base R D.2.1 Download and read csv (comma separated value) file coronavirus &lt;- read.csv(&quot;https://github.com/RamiKrispin/coronavirus/raw/master/csv/coronavirus.csv&quot;) D.2.2 Summaries and structures of the data head(coronavirus) ## date province country lat long type cases uid iso2 iso3 ## 1 2020-01-22 Alberta Canada 53.9333 -116.5765 confirmed 0 12401 CA CAN ## 2 2020-01-23 Alberta Canada 53.9333 -116.5765 confirmed 0 12401 CA CAN ## 3 2020-01-24 Alberta Canada 53.9333 -116.5765 confirmed 0 12401 CA CAN ## 4 2020-01-25 Alberta Canada 53.9333 -116.5765 confirmed 0 12401 CA CAN ## 5 2020-01-26 Alberta Canada 53.9333 -116.5765 confirmed 0 12401 CA CAN ## 6 2020-01-27 Alberta Canada 53.9333 -116.5765 confirmed 0 12401 CA CAN ## code3 combined_key population continent_name continent_code ## 1 124 Alberta, Canada 4413146 North America &lt;NA&gt; ## 2 124 Alberta, Canada 4413146 North America &lt;NA&gt; ## 3 124 Alberta, Canada 4413146 North America &lt;NA&gt; ## 4 124 Alberta, Canada 4413146 North America &lt;NA&gt; ## 5 124 Alberta, Canada 4413146 North America &lt;NA&gt; ## 6 124 Alberta, Canada 4413146 North America &lt;NA&gt; str(coronavirus) ## &#39;data.frame&#39;: 888636 obs. of 15 variables: ## $ date : chr &quot;2020-01-22&quot; &quot;2020-01-23&quot; &quot;2020-01-24&quot; &quot;2020-01-25&quot; ... ## $ province : chr &quot;Alberta&quot; &quot;Alberta&quot; &quot;Alberta&quot; &quot;Alberta&quot; ... ## $ country : chr &quot;Canada&quot; &quot;Canada&quot; &quot;Canada&quot; &quot;Canada&quot; ... ## $ lat : num 53.9 53.9 53.9 53.9 53.9 ... ## $ long : num -117 -117 -117 -117 -117 ... ## $ type : chr &quot;confirmed&quot; &quot;confirmed&quot; &quot;confirmed&quot; &quot;confirmed&quot; ... ## $ cases : int 0 0 0 0 0 0 0 0 0 0 ... ## $ uid : int 12401 12401 12401 12401 12401 12401 12401 12401 12401 12401 ... ## $ iso2 : chr &quot;CA&quot; &quot;CA&quot; &quot;CA&quot; &quot;CA&quot; ... ## $ iso3 : chr &quot;CAN&quot; &quot;CAN&quot; &quot;CAN&quot; &quot;CAN&quot; ... ## $ code3 : int 124 124 124 124 124 124 124 124 124 124 ... ## $ combined_key : chr &quot;Alberta, Canada&quot; &quot;Alberta, Canada&quot; &quot;Alberta, Canada&quot; &quot;Alberta, Canada&quot; ... ## $ population : num 4413146 4413146 4413146 4413146 4413146 ... ## $ continent_name: chr &quot;North America&quot; &quot;North America&quot; &quot;North America&quot; &quot;North America&quot; ... ## $ continent_code: chr NA NA NA NA ... Since the first column consists of date in character format, we change it to Date. coronavirus$date &lt;- as.Date(coronavirus$date) str(coronavirus) ## &#39;data.frame&#39;: 888636 obs. of 15 variables: ## $ date : Date, format: &quot;2020-01-22&quot; &quot;2020-01-23&quot; ... ## $ province : chr &quot;Alberta&quot; &quot;Alberta&quot; &quot;Alberta&quot; &quot;Alberta&quot; ... ## $ country : chr &quot;Canada&quot; &quot;Canada&quot; &quot;Canada&quot; &quot;Canada&quot; ... ## $ lat : num 53.9 53.9 53.9 53.9 53.9 ... ## $ long : num -117 -117 -117 -117 -117 ... ## $ type : chr &quot;confirmed&quot; &quot;confirmed&quot; &quot;confirmed&quot; &quot;confirmed&quot; ... ## $ cases : int 0 0 0 0 0 0 0 0 0 0 ... ## $ uid : int 12401 12401 12401 12401 12401 12401 12401 12401 12401 12401 ... ## $ iso2 : chr &quot;CA&quot; &quot;CA&quot; &quot;CA&quot; &quot;CA&quot; ... ## $ iso3 : chr &quot;CAN&quot; &quot;CAN&quot; &quot;CAN&quot; &quot;CAN&quot; ... ## $ code3 : int 124 124 124 124 124 124 124 124 124 124 ... ## $ combined_key : chr &quot;Alberta, Canada&quot; &quot;Alberta, Canada&quot; &quot;Alberta, Canada&quot; &quot;Alberta, Canada&quot; ... ## $ population : num 4413146 4413146 4413146 4413146 4413146 ... ## $ continent_name: chr &quot;North America&quot; &quot;North America&quot; &quot;North America&quot; &quot;North America&quot; ... ## $ continent_code: chr NA NA NA NA ... Now the first column is recognized as date. The range of dates covered can be seen by the following. range(coronavirus$date) ## [1] &quot;2020-01-22&quot; &quot;2022-11-29&quot; Let us check the countries by unique function. We choose a country from this list later. head(unique(coronavirus$country), 10) ## [1] &quot;Canada&quot; &quot;United Kingdom&quot; &quot;China&quot; &quot;Netherlands&quot; ## [5] &quot;Australia&quot; &quot;New Zealand&quot; &quot;Denmark&quot; &quot;France&quot; ## [9] &quot;Afghanistan&quot; &quot;Albania&quot; Now check the type column similarly. unique(coronavirus$type) ## [1] &quot;confirmed&quot; &quot;death&quot; &quot;recovery&quot; D.2.3 Set a Country As a test we choose “Japan” as a country. Please check the country list above. We apply a filter country == COUNTRY to the country column. COUNTRY &lt;- &quot;Japan&quot; df0 &lt;- coronavirus[coronavirus$country == COUNTRY,] Let us check the subset of our data and see if the population column changes over time. head(df0) ## date province country lat long type cases uid iso2 ## 183569 2020-01-22 &lt;NA&gt; Japan 36.20482 138.2529 confirmed 2 392 JP ## 183570 2020-01-23 &lt;NA&gt; Japan 36.20482 138.2529 confirmed 0 392 JP ## 183571 2020-01-24 &lt;NA&gt; Japan 36.20482 138.2529 confirmed 0 392 JP ## 183572 2020-01-25 &lt;NA&gt; Japan 36.20482 138.2529 confirmed 0 392 JP ## 183573 2020-01-26 &lt;NA&gt; Japan 36.20482 138.2529 confirmed 2 392 JP ## 183574 2020-01-27 &lt;NA&gt; Japan 36.20482 138.2529 confirmed 0 392 JP ## iso3 code3 combined_key population continent_name continent_code ## 183569 JPN 392 Japan 126476458 Asia AS ## 183570 JPN 392 Japan 126476458 Asia AS ## 183571 JPN 392 Japan 126476458 Asia AS ## 183572 JPN 392 Japan 126476458 Asia AS ## 183573 JPN 392 Japan 126476458 Asia AS ## 183574 JPN 392 Japan 126476458 Asia AS tail(df0) ## date province country lat long type cases uid iso2 ## 771815 2022-11-24 &lt;NA&gt; Japan 36.20482 138.2529 recovery 0 392 JP ## 771816 2022-11-25 &lt;NA&gt; Japan 36.20482 138.2529 recovery 0 392 JP ## 771817 2022-11-26 &lt;NA&gt; Japan 36.20482 138.2529 recovery 0 392 JP ## 771818 2022-11-27 &lt;NA&gt; Japan 36.20482 138.2529 recovery 0 392 JP ## 771819 2022-11-28 &lt;NA&gt; Japan 36.20482 138.2529 recovery 0 392 JP ## 771820 2022-11-29 &lt;NA&gt; Japan 36.20482 138.2529 recovery 0 392 JP ## iso3 code3 combined_key population continent_name continent_code ## 771815 JPN 392 Japan 126476458 Asia AS ## 771816 JPN 392 Japan 126476458 Asia AS ## 771817 JPN 392 Japan 126476458 Asia AS ## 771818 JPN 392 Japan 126476458 Asia AS ## 771819 JPN 392 Japan 126476458 Asia AS ## 771820 JPN 392 Japan 126476458 Asia AS Since the population on the first day and the last day are equal, you can set one as the population of the country. (pop &lt;- df0$population[1]) ## [1] 126476458 We need only the first, the sixth, the seventh and the thirteenth column, namely, “date”, “type”, “cases”, “population”, we create a new data frame called df by the following. df &lt;- df0[c(1,6,7,13)] str(df) ## &#39;data.frame&#39;: 3129 obs. of 4 variables: ## $ date : Date, format: &quot;2020-01-22&quot; &quot;2020-01-23&quot; ... ## $ type : chr &quot;confirmed&quot; &quot;confirmed&quot; &quot;confirmed&quot; &quot;confirmed&quot; ... ## $ cases : int 2 0 0 0 2 0 3 0 4 4 ... ## $ population: num 1.26e+08 1.26e+08 1.26e+08 1.26e+08 1.26e+08 ... head(df) ## date type cases population ## 183569 2020-01-22 confirmed 2 126476458 ## 183570 2020-01-23 confirmed 0 126476458 ## 183571 2020-01-24 confirmed 0 126476458 ## 183572 2020-01-25 confirmed 0 126476458 ## 183573 2020-01-26 confirmed 2 126476458 ## 183574 2020-01-27 confirmed 0 126476458 Alternatively, df0[c(1,6,7,13)] can be replaced by df0[c(\"date\", \"type\", \"cases\", \"population\")]. head(df0[c(&quot;date&quot;, &quot;type&quot;, &quot;cases&quot;, &quot;population&quot;)]) ## date type cases population ## 183569 2020-01-22 confirmed 2 126476458 ## 183570 2020-01-23 confirmed 0 126476458 ## 183571 2020-01-24 confirmed 0 126476458 ## 183572 2020-01-25 confirmed 0 126476458 ## 183573 2020-01-26 confirmed 2 126476458 ## 183574 2020-01-27 confirmed 0 126476458 D.2.4 Types: “confirmed” “death” and “recovery” Let us check each type as follows. df_confirmed &lt;- df[df$type == &quot;confirmed&quot;,] df_death &lt;- df[df$type == &quot;death&quot;,] df_recovery &lt;- df[df$data_type == &quot;recovery&quot;,] head(df_confirmed) ## date type cases population ## 183569 2020-01-22 confirmed 2 126476458 ## 183570 2020-01-23 confirmed 0 126476458 ## 183571 2020-01-24 confirmed 0 126476458 ## 183572 2020-01-25 confirmed 0 126476458 ## 183573 2020-01-26 confirmed 2 126476458 ## 183574 2020-01-27 confirmed 0 126476458 head(df_death) ## date type cases population ## 484996 2020-01-22 death 0 126476458 ## 484997 2020-01-23 death 0 126476458 ## 484998 2020-01-24 death 0 126476458 ## 484999 2020-01-25 death 0 126476458 ## 485000 2020-01-26 death 0 126476458 ## 485001 2020-01-27 death 0 126476458 head(df_recovery) ## [1] date type cases population ## &lt;0 行&gt; (または長さ 0 の row.names) Notice that “recovery” data is empty. D.2.5 Data Visualization D.2.5.1 Histogram for One Numerical Data plot(df_confirmed$date, df_confirmed$cases, type = &quot;h&quot;) plot(df_death$date, df_death$cases, type = &quot;h&quot;) # plot(df_recovered$date, df_recovered$cases, type = &quot;h&quot;) # no data for recovery D.2.6 Scatter Plot and Correlation for Two Numerial Data plot(df_confirmed$cases, df_death$cases, type = &quot;p&quot;) cor(df_confirmed$cases, df_death$cases) ## [1] 0.716229 D.2.6.1 Extra It is better to add the title of the graph and the labels of x-axis and y-axis. Here we used paste(\"Comfirmed Cases in\",COUNTRY) to automatically paste the chosen country name. plot(df_confirmed$date, df_confirmed$cases, type = &quot;h&quot;, main = paste(&quot;Comfirmed Cases in&quot;,COUNTRY), xlab = &quot;Date&quot;, ylab = &quot;Number of Cases&quot;) D.2.7 In Addition Set a Period start_date &lt;- as.Date(&quot;2021-07-01&quot;) end_date &lt;- Sys.Date() df_date &lt;- df[df$date &gt;=start_date &amp; df$date &lt;= end_date,] Apply the same operations on this subset. D.2.7.1 Setting types df_date_confirmed &lt;- df_date[df_date$type == &quot;confirmed&quot;,] df_date_death &lt;- df_date[df_date$type == &quot;death&quot;,] df_date_recovery &lt;- df_date[df_date$data_type == &quot;recovery&quot;,] head(df_date_confirmed) ## date type cases population ## 184095 2021-07-01 confirmed 1754 126476458 ## 184096 2021-07-02 confirmed 1775 126476458 ## 184097 2021-07-03 confirmed 1878 126476458 ## 184098 2021-07-04 confirmed 1485 126476458 ## 184099 2021-07-05 confirmed 1029 126476458 ## 184100 2021-07-06 confirmed 1668 126476458 head(df_date_death) ## date type cases population ## 485522 2021-07-01 death 24 126476458 ## 485523 2021-07-02 death 25 126476458 ## 485524 2021-07-03 death 9 126476458 ## 485525 2021-07-04 death 6 126476458 ## 485526 2021-07-05 death 19 126476458 ## 485527 2021-07-06 death 22 126476458 head(df_date_recovery) ## [1] date type cases population ## &lt;0 行&gt; (または長さ 0 の row.names) D.2.8 Histograms plot(df_date_confirmed$date, df_date_confirmed$cases, type = &quot;h&quot;) plot(df_date_death$date, df_date_death$cases, type = &quot;h&quot;) # plot(df_date_recovered$date, df_date_recovered$cases, type = &quot;h&quot;) # no data for recovery D.2.8.1 Scatter Plot and Correlation plot(df_date_confirmed$cases, df_date_death$cases, type = &quot;p&quot;) cor(df_date_confirmed$cases, df_date_death$cases) ## [1] 0.7289401 D.2.9 List Observations and Questions for Further Exploration Q0. Change the values of the location and the period and see the outcomes. Q1. What is the correlation between df_confirmed\\(cases and df_death\\)cases? Q2. Do we have a larger correlation value if we shift the dates to implement the time-lag? Q3. Do you have any other questions to explore? D.3 Selection of Several Countries with ggplot2 Let us choose “US”, “Germany”, “India”, “South Africa”, “Korea, South” and “Japan” Check whether province part is valid in these countries. df0 &lt;- coronavirus[coronavirus$country %in% c(&quot;US&quot;, &quot;Germany&quot;, &quot;India&quot;, &quot;South Africa&quot;,&quot;Korea, South&quot;, &quot;Japan&quot;),] unique(df0$province) ## [1] NA We keep the country name. df &lt;- df0[c(1,3,6,7,13)] str(df) ## &#39;data.frame&#39;: 18774 obs. of 5 variables: ## $ date : Date, format: &quot;2020-01-22&quot; &quot;2020-01-23&quot; ... ## $ country : chr &quot;Germany&quot; &quot;Germany&quot; &quot;Germany&quot; &quot;Germany&quot; ... ## $ type : chr &quot;confirmed&quot; &quot;confirmed&quot; &quot;confirmed&quot; &quot;confirmed&quot; ... ## $ cases : int 0 0 0 0 0 1 3 0 0 1 ... ## $ population: num 83783945 83783945 83783945 83783945 83783945 ... head(df) ## date country type cases population ## 161666 2020-01-22 Germany confirmed 0 83783945 ## 161667 2020-01-23 Germany confirmed 0 83783945 ## 161668 2020-01-24 Germany confirmed 0 83783945 ## 161669 2020-01-25 Germany confirmed 0 83783945 ## 161670 2020-01-26 Germany confirmed 0 83783945 ## 161671 2020-01-27 Germany confirmed 1 83783945 D.3.1 Set types df_confirmed &lt;- df[df$type == &quot;confirmed&quot;,] df_death &lt;- df[df$type == &quot;death&quot;,] df_recovery &lt;- df[df$data_type == &quot;recovery&quot;,] head(df_confirmed) ## date country type cases population ## 161666 2020-01-22 Germany confirmed 0 83783945 ## 161667 2020-01-23 Germany confirmed 0 83783945 ## 161668 2020-01-24 Germany confirmed 0 83783945 ## 161669 2020-01-25 Germany confirmed 0 83783945 ## 161670 2020-01-26 Germany confirmed 0 83783945 ## 161671 2020-01-27 Germany confirmed 1 83783945 head(df_death) ## date country type cases population ## 463093 2020-01-22 Germany death 0 83783945 ## 463094 2020-01-23 Germany death 0 83783945 ## 463095 2020-01-24 Germany death 0 83783945 ## 463096 2020-01-25 Germany death 0 83783945 ## 463097 2020-01-26 Germany death 0 83783945 ## 463098 2020-01-27 Germany death 0 83783945 head(df_recovery) ## [1] date country type cases population ## &lt;0 行&gt; (または長さ 0 の row.names) unique(df_confirmed$population) ## [1] 83783945 1380004385 126476458 51269183 59308690 329466283 D.3.2 Visualization using ggplot2 package library(ggplot2) ggplot(df_confirmed) + geom_line(aes(x = date, y = cases, color = country)) + labs(x = &quot;Date&quot;, y = &quot;Number of Confirmed Cases&quot;, title = &quot;Number of Confirmed Cases&quot;) ggplot(df_confirmed) + geom_line(aes(x = date, y = cases, color = country)) + facet_wrap(vars(country)) + labs(x = &quot;Date&quot;, y = &quot;Number of Confirmed Cases&quot;, title = &quot;Number of Confirmed Cases&quot;) ggplot(df_confirmed) + geom_boxplot(aes(x = country, y = cases)) + labs(x = &quot;Date&quot;, y = &quot;Number of Confirmed Cases&quot;, title = &quot;Number of Confirmed Cases&quot;) ggplot(df_confirmed) + geom_line(aes(x = date, y = (cases*100000/population), color = country)) + labs(x = &quot;Date&quot;, y = &quot;Number of Confirmed Cases per 100,000&quot;, title = &quot;Number of Confirmed Cases per 100,000&quot;) ggplot(df_confirmed) + geom_line(aes(x = date, y = (cases*100000/population), color = country)) + facet_wrap(vars(country)) + labs(x = &quot;Date&quot;, y = &quot;Number of Confirmed Cases per 100,000&quot;, title = &quot;Number of Confirmed Cases per 100,000&quot;) ggplot(df_confirmed) + geom_boxplot(aes(x = country, y = (cases*100000)/population)) + labs(x = &quot;Date&quot;, y = &quot;Number of Confirmed Cases per 100,000&quot;, title = &quot;Number of Confirmed Cases per 100,000&quot;) ggplot(df_confirmed) + geom_boxplot(aes(x = country, y = (cases*100000)/population)) + scale_y_continuous(trans=&#39;log10&#39;) + labs(x = &quot;Date&quot;, y = &quot;Number of Confirmed Cases in log10 per 100,000&quot;, title = &quot;Number of Confirmed Cases in log10 Scale per 100,000&quot;) ## Warning in self$trans$transform(x): 計算結果が NaN になりました ## Warning: Transformation introduced infinite values in continuous y-axis ## Warning: Removed 265 rows containing non-finite values (`stat_boxplot()`). ggplot(df_death) + geom_line(aes(x = date, y = cases, color = country)) + labs(x = &quot;Date&quot;, y = &quot;Number of Deaths&quot;, title = &quot;Number of Deaths&quot;) ggplot(df_death) + geom_line(aes(x = date, y = (cases*100000/population), color = country)) + labs(x = &quot;Date&quot;, y = &quot;Number of Deaths per 100,000&quot;, title = &quot;Number of Deaths per 100,000&quot;) ggplot(df_death) + geom_line(aes(x = date, y = (cases*100000/population), color = country)) + facet_wrap(vars(country)) + labs(x = &quot;Date&quot;, y = &quot;Number of Deaths per 100,000&quot;, title = &quot;Number of Deaths per 100,000&quot;) ggplot(df_death) + geom_boxplot(aes(x = country, y = (cases*100000)/population)) + scale_y_continuous(trans=&#39;log10&#39;) + labs(x = &quot;Date&quot;, y = &quot;Number of Deaths in log10 per 100,000&quot;, title = &quot;Number of Deaths in log10 Scale per 100,000&quot;) ## Warning in self$trans$transform(x): 計算結果が NaN になりました ## Warning: Transformation introduced infinite values in continuous y-axis ## Warning: Removed 635 rows containing non-finite values (`stat_boxplot()`). D.3.3 Setting a Period start_date &lt;- as.Date(&quot;2021-07-01&quot;) end_date &lt;- Sys.Date() df_date &lt;- df[df$date &gt;=start_date &amp; df$date &lt;= end_date,] df_date_confirmed &lt;- df_date[df_date$type == &quot;confirmed&quot;,] df_date_death &lt;- df_date[df_date$type == &quot;death&quot;,] ggplot(df_date_confirmed) + geom_line(aes(x = date, y = (cases*100000/population), color = country)) ggplot(df_date_confirmed) + geom_line(aes(x = date, y = (cases*100000/population), color = country)) + facet_wrap(vars(country)) start_date &lt;- as.Date(&quot;2021-11-20&quot;) end_date &lt;- Sys.Date() df_date &lt;- df[df$date &gt;=start_date &amp; df$date &lt;= end_date &amp; df$country %in% c(&quot;Germany&quot;, &quot;South Africa&quot;, &quot;US&quot;),] df_date_confirmed &lt;- df_date[df_date$type == &quot;confirmed&quot;,] df_date_death &lt;- df_date[df_date$type == &quot;death&quot;,] ggplot(df_date_confirmed) + geom_line(aes(x = date, y = (cases*100000/population), color = country)) The number of deaths in 100,000. ggplot(df_date_death) + geom_line(aes(x = date, y = (cases*100000/population), color = country)) D.4 Importing and Transforming Data with readr and dplyr in tidyverse Packages D.4.1 Review Attaching neccessary packages Importing data Glimpsing data with head(), str() and changing types of a column, i.e.,characters to date Selecting columns of data Filtering rows of data Mutating data Visualizing data by ggplot() library(ggplot2) coronavirus &lt;- read.csv(&quot;https://github.com/RamiKrispin/coronavirus/raw/master/csv/coronavirus.csv&quot;) coronavirus$date &lt;- as.Date(coronavirus$date) df &lt;- coronavirus[c(1,3,6,7,13)] COUNTRIES &lt;- c(&quot;US&quot;, &quot;Germany&quot;, &quot;India&quot;, &quot;South Africa&quot;,&quot;Korea, South&quot;, &quot;Japan&quot;) start_date &lt;- as.Date(&quot;2021-07-01&quot;) end_date &lt;- Sys.Date() df0 &lt;- coronavirus[df$country %in% COUNTRIES,] df1 &lt;- df0[df0$date &gt;=start_date &amp; df0$date &lt;= end_date,] df1_confirmed &lt;- df1[df1$type == &quot;confirmed&quot;,] ggplot(df1_confirmed) + geom_line(aes(x = date, y = (cases*100000/population), color = country)) + labs(x = &quot;Date&quot;, y = &quot;Number of Confirmed Cases per 100,000&quot;, title = &quot;Number of Confirmed Cases per 100,000&quot;) ### library: Loading/Attaching Packages To use packages, Install packages by install.packages() only once, e.g., install.packages(\"tidyverse\"). Load and attach packages by library() at each session, e.g., library(tidyverse). For library(), see https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/library. library(tidyverse) D.4.2 Importing data by readr in tidyverse See https://readr.tidyverse.org The goal of readr is to provide a fast and friendly way to read rectangular data (like csv, tsv, and fwf). It is designed to flexibly parse many types of data found in the wild, while still cleanly failing when data unexpectedly changes. If you are new to readr, the best place to start is the data import chapter in R for data science. To accurately read a rectangular dataset with readr you combine two pieces: a function that parses the overall file, and a column specification. The column specification describes how each column should be converted from a character vector to the most appropriate data type, and in most cases it’s not necessary because readr will guess it for you automatically. readr supports seven file formats with seven read_ functions: read_csv(): comma separated (CSV) files read_tsv(): tab separated files read_delim(): general delimited files read_fwf(): fixed width files read_table(): tabular files where columns are separated by white-space. read_log(): web log files coronavirus_tv &lt;- read_csv(&quot;https://github.com/RamiKrispin/coronavirus/raw/master/csv/coronavirus.csv&quot;) ## Rows: 888636 Columns: 15 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (8): province, country, type, iso2, iso3, combined_key, continent_name,... ## dbl (6): lat, long, cases, uid, code3, population ## date (1): date ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. coronavirus_tv ## # A tibble: 888,636 × 15 ## date province country lat long type cases uid iso2 iso3 code3 ## &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2020-01-22 Alberta Canada 53.9 -117. confir… 0 12401 CA CAN 124 ## 2 2020-01-23 Alberta Canada 53.9 -117. confir… 0 12401 CA CAN 124 ## 3 2020-01-24 Alberta Canada 53.9 -117. confir… 0 12401 CA CAN 124 ## 4 2020-01-25 Alberta Canada 53.9 -117. confir… 0 12401 CA CAN 124 ## 5 2020-01-26 Alberta Canada 53.9 -117. confir… 0 12401 CA CAN 124 ## 6 2020-01-27 Alberta Canada 53.9 -117. confir… 0 12401 CA CAN 124 ## 7 2020-01-28 Alberta Canada 53.9 -117. confir… 0 12401 CA CAN 124 ## 8 2020-01-29 Alberta Canada 53.9 -117. confir… 0 12401 CA CAN 124 ## 9 2020-01-30 Alberta Canada 53.9 -117. confir… 0 12401 CA CAN 124 ## 10 2020-01-31 Alberta Canada 53.9 -117. confir… 0 12401 CA CAN 124 ## # … with 888,626 more rows, and 4 more variables: combined_key &lt;chr&gt;, ## # population &lt;dbl&gt;, continent_name &lt;chr&gt;, continent_code &lt;chr&gt; # print(coronavirus_tv) # same as above Data is in tibble, see https://tibble.tidyverse.org. See thatthe first column is already recognized as date. glimpse is similar to str and is like a transposed version of print() glimpse(coronavirus_tv) ## Rows: 888,636 ## Columns: 15 ## $ date &lt;date&gt; 2020-01-22, 2020-01-23, 2020-01-24, 2020-01-25, 2020-0… ## $ province &lt;chr&gt; &quot;Alberta&quot;, &quot;Alberta&quot;, &quot;Alberta&quot;, &quot;Alberta&quot;, &quot;Alberta&quot;, … ## $ country &lt;chr&gt; &quot;Canada&quot;, &quot;Canada&quot;, &quot;Canada&quot;, &quot;Canada&quot;, &quot;Canada&quot;, &quot;Cana… ## $ lat &lt;dbl&gt; 53.9333, 53.9333, 53.9333, 53.9333, 53.9333, 53.9333, 5… ## $ long &lt;dbl&gt; -116.5765, -116.5765, -116.5765, -116.5765, -116.5765, … ## $ type &lt;chr&gt; &quot;confirmed&quot;, &quot;confirmed&quot;, &quot;confirmed&quot;, &quot;confirmed&quot;, &quot;co… ## $ cases &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ uid &lt;dbl&gt; 12401, 12401, 12401, 12401, 12401, 12401, 12401, 12401,… ## $ iso2 &lt;chr&gt; &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;CA&quot;, &quot;… ## $ iso3 &lt;chr&gt; &quot;CAN&quot;, &quot;CAN&quot;, &quot;CAN&quot;, &quot;CAN&quot;, &quot;CAN&quot;, &quot;CAN&quot;, &quot;CAN&quot;, &quot;CAN&quot;,… ## $ code3 &lt;dbl&gt; 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, … ## $ combined_key &lt;chr&gt; &quot;Alberta, Canada&quot;, &quot;Alberta, Canada&quot;, &quot;Alberta, Canada&quot;… ## $ population &lt;dbl&gt; 4413146, 4413146, 4413146, 4413146, 4413146, 4413146, 4… ## $ continent_name &lt;chr&gt; &quot;North America&quot;, &quot;North America&quot;, &quot;North America&quot;, &quot;Nor… ## $ continent_code &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… download.file() and write_csv() # Don&#39;t run repeatedly DLURL &lt;- &quot;https://github.com/RamiKrispin/coronavirus/raw/master/csv/coronavirus.csv&quot; DLDATE &lt;- paste0(&quot;coronavirus&quot;, Sys.Date(), &quot;.csv&quot;) download.file(DLURL, destfile = DLDATE) # Don&#39;t run repeatedly WRITEDATE &lt;- paste0(&quot;covid19_&quot;, Sys.Date(), &quot;.csv&quot;) write_csv(coronavirus, WRITEDATE) D.4.3 Transforming data by dplyr in tidyverse See https://dplyr.tidyverse.org. dplyr is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges: mutate() adds new variables that are functions of existing variables select() picks variables based on their names. filter() picks cases based on their values. summarise() reduces multiple values down to a single summary. arrange() changes the ordering of the rows. These all combine naturally with group_by() which allows you to perform any operation “by group”. You can learn more about them in vignette(“dplyr”). As well as these single-table verbs, dplyr also provides a variety of two-table verbs, which you can learn about in vignette(\"two-table\"). If you are new to dplyr, the best place to start is the data transformation chapter in R for data science. D.4.3.1 slice(): Subset rows using their positions See https://dplyr.tidyverse.org/reference/slice.html slice() lets you index rows by their (integer) locations. It allows you to select, remove, and duplicate rows. It is accompanied by a number of helpers for common use cases: slice_head() and slice_tail() select the first or last rows. slice_sample() randomly selects rows. slice_min() and slice_max() select rows with highest or lowest values of a variable. The following is similar to head() but it does much more. slice(coronavirus_tv, 6) ## # A tibble: 1 × 15 ## date province country lat long type cases uid iso2 iso3 code3 ## &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2020-01-27 Alberta Canada 53.9 -117. confirm… 0 12401 CA CAN 124 ## # … with 4 more variables: combined_key &lt;chr&gt;, population &lt;dbl&gt;, ## # continent_name &lt;chr&gt;, continent_code &lt;chr&gt; D.4.3.2 select() Subset columns using their names and types See https://dplyr.tidyverse.org/reference/select.html Select (and optionally rename) variables in a data frame, using a concise mini-language that makes it easy to refer to variables based on their name (e.g. a:f selects all columns from a on the left to f on the right). You can also use predicate functions like is.numeric to select variables based on their properties. Helper Function Use Example - Columns except select(babynames, -prop) : Columns between (inclusive) select(babynames, year:n) contains() Columns that contains a string select(babynames, contains(“n”)) ends_with() Columns that ends with a string select(babynames, ends_with(“n”)) matches() Columns that matches a regex select(babynames, matches(“n”)) num_range() Columns with a numerical suffix in the range Not applicable with babynames one_of() Columns whose name appear in the given set select(babynames, one_of(c(“sex”, “gender”))) starts_with() Columns that starts with a string select(babynames, starts_with(“n”)) df_tv &lt;- select(coronavirus_tv, c(date, country, type, cases, population)) df_tv ## # A tibble: 888,636 × 5 ## date country type cases population ## &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2020-01-22 Canada confirmed 0 4413146 ## 2 2020-01-23 Canada confirmed 0 4413146 ## 3 2020-01-24 Canada confirmed 0 4413146 ## 4 2020-01-25 Canada confirmed 0 4413146 ## 5 2020-01-26 Canada confirmed 0 4413146 ## 6 2020-01-27 Canada confirmed 0 4413146 ## 7 2020-01-28 Canada confirmed 0 4413146 ## 8 2020-01-29 Canada confirmed 0 4413146 ## 9 2020-01-30 Canada confirmed 0 4413146 ## 10 2020-01-31 Canada confirmed 0 4413146 ## # … with 888,626 more rows identical(df_tv, select(coronavirus_tv, date, country, type, cases, population)) ## [1] TRUE identical(df_tv, select(coronavirus_tv, 1, 3, 6, 7, 13)) ## [1] TRUE identical(df_tv, select(coronavirus_tv, &quot;date&quot;, &quot;country&quot;, &quot;type&quot;, &quot;cases&quot;, &quot;population&quot;)) ## [1] TRUE D.4.3.3 filter() Subset rows using column values See https://dplyr.tidyverse.org/reference/filter.html The filter() function is used to subset a data frame, retaining all rows that satisfy your conditions. To be retained, the row must produce a value of TRUE for all conditions. Note that when a condition evaluates to NA the row will be dropped, unlike base subsetting with [. Logical operator tests Example &gt; Is x greater than y? x &gt; y &gt;= Is x greater than or equal to y? x &gt;= y &lt; Is x less than y? x &lt; y &lt;= Is x less than or equal to y? x &lt;= y == Is x equal to y? x == y != Is x not equal to y? x != y is.na() Is x an NA? is.na(x) !is.na() Is x not an NA? !is.na(x) COUNTRIES &lt;- c(&quot;US&quot;, &quot;Germany&quot;, &quot;India&quot;, &quot;South Africa&quot;,&quot;Korea, South&quot;, &quot;Japan&quot;) start_date &lt;- as.Date(&quot;2021-07-01&quot;) end_date &lt;- Sys.Date() df_tv0 &lt;- filter(df_tv, country %in% COUNTRIES) df_tv1 &lt;- filter(df_tv0, date &gt;=start_date &amp; df_tv0$date &lt;= end_date) df_tv1_confirmed &lt;- filter(df_tv1, type == &quot;confirmed&quot;) identical(df_tv1_confirmed, filter(df_tv, (country %in% COUNTRIES) &amp; (date &gt;=start_date &amp; date &lt;= end_date) &amp; (type == &quot;confirmed&quot;))) ## [1] TRUE Advanced method using piping df_tv %&gt;% filter(country %in% COUNTRIES) %&gt;% filter(date &gt;=start_date &amp; df_tv0$date &lt;= end_date) %&gt;% filter(type == &quot;confirmed&quot;) %&gt;% identical(df_tv1_confirmed) ## [1] TRUE D.4.3.4 mutate(): Create, modify, and delete columns See https://dplyr.tidyverse.org/reference/mutate.html mutate() adds new variables and preserves existing ones; transmute() adds new variables and drops existing ones. New variables overwrite existing variables of the same name. Variables can be removed by setting their value to NULL. df_tv1_confirmed_pp &lt;- mutate(df_tv1_confirmed, confirmed_pp = cases*100000/population) D.4.3.5 ggplot(): Plotting See https://ggplot2.tidyverse.org ggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details. ggplot(df_tv1_confirmed_pp) + geom_line(aes(x = date, y = confirmed_pp, color = country)) + labs(x = &quot;Date&quot;, y = &quot;Number of Confirmed Cases per 100,000&quot;, title = &quot;Number of Confirmed Cases per 100,000&quot;) D.4.3.6 Summary library(tidyverse) coronavirus_tv &lt;- read_csv(&quot;https://github.com/RamiKrispin/coronavirus/raw/master/csv/coronavirus.csv&quot;) ## Rows: 888636 Columns: 15 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (8): province, country, type, iso2, iso3, combined_key, continent_name,... ## dbl (6): lat, long, cases, uid, code3, population ## date (1): date ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. COUNTRIES &lt;- c(&quot;US&quot;, &quot;Germany&quot;, &quot;India&quot;, &quot;South Africa&quot;,&quot;Korea, South&quot;, &quot;Japan&quot;) start_date &lt;- as.Date(&quot;2021-07-01&quot;) end_date &lt;- Sys.Date() df_tv &lt;- select(coronavirus_tv, c(date, country, type, cases, population)) df_tv0 &lt;- filter(df_tv, country %in% COUNTRIES) df_tv1 &lt;- filter(df_tv0, date &gt;=start_date &amp; df_tv0$date &lt;= end_date) df_tv1_confirmed &lt;- filter(df_tv1, type == &quot;confirmed&quot;) df_tv1_confirmed_pp &lt;- mutate(df_tv1_confirmed, confirmed_pp = cases*100000/population) ggplot(df_tv1_confirmed_pp) + geom_line(aes(x = date, y = confirmed_pp, color = country)) + labs(x = &quot;Date&quot;, y = &quot;Number of Confirmed Cases per 100,000&quot;, title = &quot;Number of Confirmed Cases per 100,000&quot;) D.4.3.7 Pipes After importing data and setting parameters; COUNTRIES, start_date and end_date, we can simplify the code block as follows. coronavirus_tv %&gt;% select(date, country, type, cases, population) %&gt;% filter(country %in% COUNTRIES) %&gt;% filter(date &gt;=start_date &amp; df_tv0$date &lt;= end_date) %&gt;% filter(type == &quot;confirmed&quot;) %&gt;% mutate(confirmed_pp = cases*100000/population) %&gt;% ggplot() + geom_line(aes(x = date, y = confirmed_pp, color = country)) + labs(x = &quot;Date&quot;, y = &quot;Number of Confirmed Cases per 100,000&quot;, title = &quot;Number of Confirmed Cases per 100,000&quot;) D.5 Data of Johns Hopkins Universiy and World Bank D.5.1 Importing Raw Data We import the original Johns Hopkins Github data. COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University: https://github.com/CSSEGISandData/COVID-19 We use time series data # IMPORT RAW DATA: Johns Hopkins Github data confirmedraw &lt;- read_csv(&quot;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv&quot;) ## Rows: 289 Columns: 1047 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): Province/State, Country/Region ## dbl (1045): Lat, Long, 1/22/20, 1/23/20, 1/24/20, 1/25/20, 1/26/20, 1/27/20,... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. glimpse(confirmedraw) ## Rows: 289 ## Columns: 1,047 ## $ `Province/State` &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;Australian Capit… ## $ `Country/Region` &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Albania&quot;, &quot;Algeria&quot;, &quot;Andorra&quot;, &quot;Ango… ## $ Lat &lt;dbl&gt; 33.93911, 41.15330, 28.03390, 42.50630, -11.20270, -7… ## $ Long &lt;dbl&gt; 67.709953, 20.168300, 1.659600, 1.521800, 17.873900, … ## $ `1/22/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ `1/23/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ `1/24/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ `1/25/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ `1/26/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0,… ## $ `1/27/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 1, 0, 0,… ## $ `1/28/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 1, 0, 0,… ## $ `1/29/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 1, 0, 0, 1, 0, 0,… ## $ `1/30/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 3, 0, 0, 2, 0, 0,… ## $ `1/31/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 2, 0, 0, 3, 0, 0,… ## $ `2/1/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 3, 1, 0, 4, 0, 0,… ## $ `2/2/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 2, 2, 0, 4, 0, 0,… ## $ `2/3/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 2, 2, 0, 4, 0, 0,… ## $ `2/4/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 3, 2, 0, 4, 0, 0,… ## $ `2/5/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 3, 2, 0, 4, 0, 0,… ## $ `2/6/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 4, 2, 0, 4, 0, 0,… ## $ `2/7/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 5, 2, 0, 4, 0, 0,… ## $ `2/8/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 5, 2, 0, 4, 0, 0,… ## $ `2/9/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 5, 2, 0, 4, 0, 0,… ## $ `2/10/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 5, 2, 0, 4, 0, 0,… ## $ `2/11/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 5, 2, 0, 4, 0, 0,… ## $ `2/12/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 5, 2, 0, 4, 0, 0,… ## $ `2/13/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 5, 2, 0, 4, 0, 0,… ## $ `2/14/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 5, 2, 0, 4, 0, 0,… ## $ `2/15/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 5, 2, 0, 4, 0, 0,… ## $ `2/16/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 5, 2, 0, 4, 0, 0,… ## $ `2/17/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 5, 2, 0, 4, 0, 0,… ## $ `2/18/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 5, 2, 0, 4, 0, 0,… ## $ `2/19/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 5, 2, 0, 4, 0, 0,… ## $ `2/20/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 5, 2, 0, 4, 0, 0,… ## $ `2/21/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 5, 2, 0, 4, 0, 0,… ## $ `2/22/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 5, 2, 0, 4, 0, 0,… ## $ `2/23/20` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 5, 2, 0, 4, 0, 0,… ## $ `2/24/20` &lt;dbl&gt; 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 5, 2, 0, 4, 0, 0,… ## $ `2/25/20` &lt;dbl&gt; 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 4, 0, 5, 2, 0, 4, 0, 2,… ## $ `2/26/20` &lt;dbl&gt; 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 4, 0, 5, 2, 0, 4, 0, 1,… ## $ `2/27/20` &lt;dbl&gt; 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 4, 0, 5, 2, 0, 4, 0, 1,… ## $ `2/28/20` &lt;dbl&gt; 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 4, 0, 5, 2, 0, 4, 0, 1,… ## $ `2/29/20` &lt;dbl&gt; 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 4, 0, 9, 3, 0, 7, 2, 3,… ## $ `3/1/20` &lt;dbl&gt; 5, 0, 1, 0, 0, 0, 0, 0, 1, 0, 6, 0, 9, 3, 0, 7, 2, 7,… ## $ `3/2/20` &lt;dbl&gt; 5, 0, 3, 1, 0, 0, 0, 0, 1, 0, 6, 0, 9, 3, 1, 9, 2, 8,… ## $ `3/3/20` &lt;dbl&gt; 5, 0, 5, 1, 0, 0, 0, 1, 1, 0, 13, 0, 11, 3, 1, 9, 2, … ## $ `3/4/20` &lt;dbl&gt; 5, 0, 12, 1, 0, 0, 0, 1, 1, 0, 22, 1, 11, 5, 1, 10, 2… ## $ `3/5/20` &lt;dbl&gt; 5, 0, 12, 1, 0, 0, 0, 1, 1, 0, 22, 1, 13, 5, 1, 10, 3… ## $ `3/6/20` &lt;dbl&gt; 5, 0, 17, 1, 0, 0, 0, 2, 1, 0, 26, 0, 13, 7, 1, 10, 3… ## $ `3/7/20` &lt;dbl&gt; 8, 0, 17, 1, 0, 0, 0, 8, 1, 0, 28, 0, 13, 7, 1, 11, 3… ## $ `3/8/20` &lt;dbl&gt; 8, 0, 19, 1, 0, 0, 0, 12, 1, 0, 38, 0, 15, 7, 2, 11, … ## $ `3/9/20` &lt;dbl&gt; 8, 2, 20, 1, 0, 0, 0, 12, 1, 0, 48, 0, 15, 7, 2, 15, … ## $ `3/10/20` &lt;dbl&gt; 8, 10, 20, 1, 0, 0, 0, 17, 1, 0, 55, 1, 18, 7, 2, 18,… ## $ `3/11/20` &lt;dbl&gt; 11, 12, 20, 1, 0, 0, 0, 19, 1, 0, 65, 1, 20, 9, 3, 21… ## $ `3/12/20` &lt;dbl&gt; 11, 23, 24, 1, 0, 0, 0, 19, 4, 0, 65, 1, 20, 9, 3, 21… ## $ `3/13/20` &lt;dbl&gt; 11, 33, 26, 1, 0, 0, 1, 31, 8, 1, 92, 1, 35, 16, 5, 3… ## $ `3/14/20` &lt;dbl&gt; 14, 38, 37, 1, 0, 0, 1, 34, 18, 1, 112, 1, 46, 19, 5,… ## $ `3/15/20` &lt;dbl&gt; 20, 42, 48, 1, 0, 0, 1, 45, 26, 1, 134, 1, 61, 20, 6,… ## $ `3/16/20` &lt;dbl&gt; 25, 51, 54, 2, 0, 0, 1, 56, 52, 2, 171, 1, 68, 29, 7,… ## $ `3/17/20` &lt;dbl&gt; 26, 55, 60, 39, 0, 0, 1, 68, 78, 2, 210, 1, 78, 29, 7… ## $ `3/18/20` &lt;dbl&gt; 26, 59, 74, 39, 0, 0, 1, 79, 84, 3, 267, 1, 94, 37, 1… ## $ `3/19/20` &lt;dbl&gt; 26, 64, 87, 53, 0, 0, 1, 97, 115, 4, 307, 1, 144, 42,… ## $ `3/20/20` &lt;dbl&gt; 24, 70, 90, 75, 1, 0, 1, 128, 136, 6, 353, 3, 184, 50… ## $ `3/21/20` &lt;dbl&gt; 24, 76, 139, 88, 2, 0, 1, 158, 160, 9, 436, 3, 221, 6… ## $ `3/22/20` &lt;dbl&gt; 34, 89, 201, 113, 2, 0, 1, 266, 194, 19, 669, 5, 259,… ## $ `3/23/20` &lt;dbl&gt; 40, 104, 230, 133, 3, 0, 3, 301, 235, 32, 669, 5, 319… ## $ `3/24/20` &lt;dbl&gt; 42, 123, 264, 164, 3, 0, 3, 387, 249, 39, 818, 6, 397… ## $ `3/25/20` &lt;dbl&gt; 74, 146, 302, 188, 3, 0, 3, 387, 265, 39, 1029, 6, 44… ## $ `3/26/20` &lt;dbl&gt; 80, 174, 367, 224, 4, 0, 7, 502, 290, 53, 1219, 12, 4… ## $ `3/27/20` &lt;dbl&gt; 91, 186, 409, 267, 4, 0, 7, 589, 329, 62, 1405, 12, 5… ## $ `3/28/20` &lt;dbl&gt; 106, 197, 454, 308, 5, 0, 7, 690, 407, 71, 1617, 15, … ## $ `3/29/20` &lt;dbl&gt; 114, 212, 511, 334, 7, 0, 7, 745, 424, 77, 1791, 15, … ## $ `3/30/20` &lt;dbl&gt; 114, 223, 584, 370, 7, 0, 7, 820, 482, 78, 2032, 15, … ## $ `3/31/20` &lt;dbl&gt; 166, 243, 716, 376, 7, 0, 7, 1054, 532, 80, 2032, 17,… ## $ `4/1/20` &lt;dbl&gt; 192, 259, 847, 390, 8, 0, 7, 1054, 571, 84, 2182, 19,… ## $ `4/2/20` &lt;dbl&gt; 235, 277, 986, 428, 8, 0, 9, 1133, 663, 87, 2298, 21,… ## $ `4/3/20` &lt;dbl&gt; 269, 304, 1171, 439, 8, 0, 15, 1265, 736, 91, 2389, 2… ## $ `4/4/20` &lt;dbl&gt; 270, 333, 1251, 466, 10, 0, 15, 1451, 770, 93, 2493, … ## $ `4/5/20` &lt;dbl&gt; 299, 361, 1320, 501, 14, 0, 15, 1451, 822, 96, 2580, … ## $ `4/6/20` &lt;dbl&gt; 337, 377, 1423, 525, 16, 0, 15, 1554, 833, 96, 2637, … ## $ `4/7/20` &lt;dbl&gt; 367, 383, 1468, 545, 17, 0, 19, 1628, 853, 96, 2686, … ## $ `4/8/20` &lt;dbl&gt; 423, 400, 1572, 564, 19, 0, 19, 1715, 881, 99, 2734, … ## $ `4/9/20` &lt;dbl&gt; 444, 409, 1666, 583, 19, 0, 19, 1795, 921, 100, 2773,… ## $ `4/10/20` &lt;dbl&gt; 521, 416, 1761, 601, 19, 0, 19, 1975, 937, 103, 2822,… ## $ `4/11/20` &lt;dbl&gt; 521, 433, 1825, 601, 19, 0, 21, 1975, 967, 103, 2857,… ## $ `4/12/20` &lt;dbl&gt; 555, 446, 1914, 638, 19, 0, 21, 2142, 1013, 103, 2857… ## $ `4/13/20` &lt;dbl&gt; 607, 467, 1983, 646, 19, 0, 23, 2208, 1039, 102, 2863… ## $ `4/14/20` &lt;dbl&gt; 665, 475, 2070, 659, 19, 0, 23, 2277, 1067, 103, 2870… ## $ `4/15/20` &lt;dbl&gt; 770, 494, 2160, 673, 19, 0, 23, 2443, 1111, 103, 2886… ## $ `4/16/20` &lt;dbl&gt; 794, 518, 2268, 673, 19, 0, 23, 2571, 1159, 103, 2897… ## $ `4/17/20` &lt;dbl&gt; 845, 539, 2418, 696, 19, 0, 23, 2669, 1201, 103, 2926… ## $ `4/18/20` &lt;dbl&gt; 908, 548, 2534, 704, 24, 0, 23, 2758, 1248, 103, 2936… ## $ `4/19/20` &lt;dbl&gt; 933, 562, 2629, 713, 24, 0, 23, 2839, 1291, 103, 2957… ## $ `4/20/20` &lt;dbl&gt; 996, 584, 2718, 717, 24, 0, 23, 2941, 1339, 104, 2963… ## $ `4/21/20` &lt;dbl&gt; 1026, 609, 2811, 717, 24, 0, 23, 3031, 1401, 104, 296… ## $ `4/22/20` &lt;dbl&gt; 1092, 634, 2910, 723, 25, 0, 24, 3144, 1473, 104, 297… ## $ `4/23/20` &lt;dbl&gt; 1176, 663, 3007, 723, 25, 0, 24, 3435, 1523, 104, 297… ## $ `4/24/20` &lt;dbl&gt; 1226, 678, 3127, 731, 25, 0, 24, 3607, 1596, 105, 298… ## $ `4/25/20` &lt;dbl&gt; 1330, 712, 3256, 738, 25, 0, 24, 3780, 1677, 106, 299… ## $ `4/26/20` &lt;dbl&gt; 1463, 726, 3382, 738, 26, 0, 24, 3892, 1746, 106, 300… ## $ `4/27/20` &lt;dbl&gt; 1531, 736, 3517, 743, 27, 0, 24, 4003, 1808, 106, 300… ## $ `4/28/20` &lt;dbl&gt; 1703, 750, 3649, 743, 27, 0, 24, 4127, 1867, 106, 301… ## $ `4/29/20` &lt;dbl&gt; 1827, 766, 3848, 743, 27, 0, 24, 4285, 1932, 106, 301… ## $ `4/30/20` &lt;dbl&gt; 1827, 773, 4006, 745, 27, 0, 24, 4428, 2066, 106, 302… ## $ `5/1/20` &lt;dbl&gt; 2171, 782, 4154, 745, 30, 0, 25, 4532, 2148, 106, 303… ## $ `5/2/20` &lt;dbl&gt; 2469, 789, 4295, 747, 35, 0, 25, 4681, 2273, 106, 303… ## $ `5/3/20` &lt;dbl&gt; 2469, 795, 4474, 748, 35, 0, 25, 4783, 2386, 106, 303… ## $ `5/4/20` &lt;dbl&gt; 2469, 803, 4648, 750, 35, 0, 25, 4887, 2507, 107, 303… ## $ `5/5/20` &lt;dbl&gt; 2469, 820, 4838, 751, 36, 0, 25, 5020, 2619, 107, 304… ## $ `5/6/20` &lt;dbl&gt; 3224, 832, 4997, 751, 36, 0, 25, 5208, 2782, 107, 304… ## $ `5/7/20` &lt;dbl&gt; 3392, 842, 5182, 752, 36, 0, 25, 5371, 2884, 107, 304… ## $ `5/8/20` &lt;dbl&gt; 3563, 850, 5369, 752, 43, 0, 25, 5611, 3029, 107, 305… ## $ `5/9/20` &lt;dbl&gt; 3563, 856, 5558, 754, 43, 0, 25, 5776, 3175, 107, 305… ## $ `5/10/20` &lt;dbl&gt; 4402, 868, 5723, 755, 45, 0, 25, 6034, 3313, 107, 305… ## $ `5/11/20` &lt;dbl&gt; 4664, 872, 5891, 755, 45, 0, 25, 6278, 3392, 107, 305… ## $ `5/12/20` &lt;dbl&gt; 4967, 876, 6067, 758, 45, 0, 25, 6563, 3538, 107, 305… ## $ `5/13/20` &lt;dbl&gt; 4967, 880, 6253, 760, 45, 0, 25, 6879, 3718, 107, 306… ## $ `5/14/20` &lt;dbl&gt; 5339, 898, 6442, 761, 48, 0, 25, 7134, 3860, 107, 307… ## $ `5/15/20` &lt;dbl&gt; 6053, 916, 6629, 761, 48, 0, 25, 7479, 4044, 107, 307… ## $ `5/16/20` &lt;dbl&gt; 6402, 933, 6821, 761, 48, 0, 25, 7805, 4283, 107, 307… ## $ `5/17/20` &lt;dbl&gt; 6635, 946, 7019, 761, 48, 0, 25, 8068, 4472, 107, 307… ## $ `5/18/20` &lt;dbl&gt; 7072, 948, 7201, 761, 50, 0, 25, 8371, 4823, 107, 307… ## $ `5/19/20` &lt;dbl&gt; 7655, 949, 7377, 761, 52, 0, 25, 8809, 5041, 107, 308… ## $ `5/20/20` &lt;dbl&gt; 8145, 964, 7542, 762, 52, 0, 25, 9283, 5271, 107, 308… ## $ `5/21/20` &lt;dbl&gt; 8676, 969, 7728, 762, 58, 0, 25, 9931, 5606, 107, 308… ## $ `5/22/20` &lt;dbl&gt; 9216, 981, 7918, 762, 60, 0, 25, 10649, 5928, 107, 30… ## $ `5/23/20` &lt;dbl&gt; 9952, 989, 8113, 762, 61, 0, 25, 11353, 6302, 107, 30… ## $ `5/24/20` &lt;dbl&gt; 10668, 998, 8306, 762, 69, 0, 25, 12076, 6661, 107, 3… ## $ `5/25/20` &lt;dbl&gt; 11180, 1004, 8503, 763, 70, 0, 25, 12628, 7113, 107, … ## $ `5/26/20` &lt;dbl&gt; 11917, 1029, 8697, 763, 70, 0, 25, 13228, 7402, 107, … ## $ `5/27/20` &lt;dbl&gt; 12465, 1050, 8857, 763, 71, 0, 25, 13933, 7774, 107, … ## $ `5/28/20` &lt;dbl&gt; 13102, 1076, 8997, 763, 74, 0, 25, 14702, 8216, 107, … ## $ `5/29/20` &lt;dbl&gt; 13745, 1099, 9134, 764, 81, 0, 25, 15419, 8676, 107, … ## $ `5/30/20` &lt;dbl&gt; 14529, 1122, 9267, 764, 84, 0, 25, 16214, 8927, 107, … ## $ `5/31/20` &lt;dbl&gt; 15180, 1137, 9394, 764, 86, 0, 26, 16851, 9282, 107, … ## $ `6/1/20` &lt;dbl&gt; 15836, 1143, 9513, 765, 86, 0, 26, 17415, 9492, 107, … ## $ `6/2/20` &lt;dbl&gt; 16578, 1164, 9626, 844, 86, 0, 26, 18319, 10009, 107,… ## $ `6/3/20` &lt;dbl&gt; 17353, 1184, 9733, 851, 86, 0, 26, 19268, 10524, 107,… ## $ `6/4/20` &lt;dbl&gt; 17977, 1197, 9831, 852, 86, 0, 26, 20197, 11221, 107,… ## $ `6/5/20` &lt;dbl&gt; 19055, 1212, 9935, 852, 86, 0, 26, 21037, 11817, 107,… ## $ `6/6/20` &lt;dbl&gt; 19637, 1232, 10050, 852, 88, 0, 26, 22020, 12364, 108… ## $ `6/7/20` &lt;dbl&gt; 20428, 1246, 10154, 852, 91, 0, 26, 22794, 13130, 108… ## $ `6/8/20` &lt;dbl&gt; 21003, 1263, 10265, 852, 92, 0, 26, 23620, 13325, 108… ## $ `6/9/20` &lt;dbl&gt; 21308, 1299, 10382, 852, 96, 0, 26, 24761, 13675, 108… ## $ `6/10/20` &lt;dbl&gt; 22228, 1341, 10484, 852, 113, 0, 26, 25987, 14103, 10… ## $ `6/11/20` &lt;dbl&gt; 22976, 1385, 10589, 852, 118, 0, 26, 27373, 14669, 10… ## $ `6/12/20` &lt;dbl&gt; 23632, 1416, 10698, 853, 130, 0, 26, 28764, 15281, 10… ## $ `6/13/20` &lt;dbl&gt; 24188, 1464, 10810, 853, 138, 0, 26, 30295, 16004, 10… ## $ `6/14/20` &lt;dbl&gt; 24852, 1521, 10919, 853, 140, 0, 26, 31577, 16667, 10… ## $ `6/15/20` &lt;dbl&gt; 25613, 1590, 11031, 853, 142, 0, 26, 32785, 17064, 10… ## $ `6/16/20` &lt;dbl&gt; 25719, 1672, 11147, 854, 148, 0, 26, 34159, 17489, 10… ## $ `6/17/20` &lt;dbl&gt; 26960, 1722, 11268, 854, 155, 0, 26, 35552, 18033, 10… ## $ `6/18/20` &lt;dbl&gt; 27423, 1788, 11385, 855, 166, 0, 26, 37510, 18698, 10… ## $ `6/19/20` &lt;dbl&gt; 27964, 1838, 11504, 855, 172, 0, 26, 39570, 19157, 10… ## $ `6/20/20` &lt;dbl&gt; 28383, 1891, 11631, 855, 176, 0, 26, 41204, 19708, 10… ## $ `6/21/20` &lt;dbl&gt; 28919, 1962, 11771, 855, 183, 0, 26, 42785, 20268, 10… ## $ `6/22/20` &lt;dbl&gt; 29229, 1995, 11920, 855, 186, 0, 26, 44931, 20588, 10… ## $ `6/23/20` &lt;dbl&gt; 29567, 2047, 12076, 855, 189, 0, 26, 47203, 21006, 10… ## $ `6/24/20` &lt;dbl&gt; 29726, 2114, 12248, 855, 197, 0, 26, 49851, 21717, 10… ## $ `6/25/20` &lt;dbl&gt; 30261, 2192, 12445, 855, 212, 0, 65, 52457, 22488, 10… ## $ `6/26/20` &lt;dbl&gt; 30346, 2269, 12685, 855, 212, 0, 65, 55343, 23247, 10… ## $ `6/27/20` &lt;dbl&gt; 30702, 2330, 12968, 855, 259, 0, 65, 57744, 23909, 10… ## $ `6/28/20` &lt;dbl&gt; 31053, 2402, 13273, 855, 267, 0, 69, 59933, 24645, 10… ## $ `6/29/20` &lt;dbl&gt; 31324, 2466, 13571, 855, 276, 0, 69, 62268, 25127, 10… ## $ `6/30/20` &lt;dbl&gt; 31445, 2535, 13907, 855, 284, 0, 69, 64530, 25542, 10… ## $ `7/1/20` &lt;dbl&gt; 31848, 2580, 14272, 855, 291, 0, 69, 67197, 26065, 10… ## $ `7/2/20` &lt;dbl&gt; 32108, 2662, 14657, 855, 315, 0, 69, 69941, 26658, 10… ## $ `7/3/20` &lt;dbl&gt; 32410, 2752, 15070, 855, 328, 0, 68, 72786, 27320, 10… ## $ `7/4/20` &lt;dbl&gt; 32758, 2819, 15500, 855, 346, 0, 68, 75376, 27900, 10… ## $ `7/5/20` &lt;dbl&gt; 33037, 2893, 15941, 855, 346, 0, 68, 77815, 28606, 10… ## $ `7/6/20` &lt;dbl&gt; 33150, 2964, 16404, 855, 346, 0, 70, 80447, 28936, 10… ## $ `7/7/20` &lt;dbl&gt; 33470, 3038, 16879, 855, 386, 0, 70, 83426, 29285, 11… ## $ `7/8/20` &lt;dbl&gt; 33680, 3106, 17348, 855, 386, 0, 70, 87030, 29820, 11… ## $ `7/9/20` &lt;dbl&gt; 33739, 3188, 17808, 855, 396, 0, 73, 90693, 30346, 11… ## $ `7/10/20` &lt;dbl&gt; 34280, 3278, 18242, 855, 458, 0, 74, 94060, 30903, 11… ## $ `7/11/20` &lt;dbl&gt; 34437, 3371, 18712, 855, 462, 0, 74, 97509, 31392, 11… ## $ `7/12/20` &lt;dbl&gt; 34537, 3454, 19195, 855, 506, 0, 74, 100166, 31969, 1… ## $ `7/13/20` &lt;dbl&gt; 34541, 3571, 19689, 858, 525, 0, 74, 103265, 32151, 1… ## $ `7/14/20` &lt;dbl&gt; 34826, 3667, 20216, 861, 541, 0, 74, 106910, 32490, 1… ## $ `7/15/20` &lt;dbl&gt; 35026, 3752, 20770, 862, 576, 0, 74, 111146, 33005, 1… ## $ `7/16/20` &lt;dbl&gt; 35156, 3851, 21355, 877, 607, 0, 74, 114783, 33559, 1… ## $ `7/17/20` &lt;dbl&gt; 35315, 3906, 21948, 880, 638, 0, 76, 119301, 34001, 1… ## $ `7/18/20` &lt;dbl&gt; 35375, 4008, 22549, 880, 687, 0, 76, 122524, 34462, 1… ## $ `7/19/20` &lt;dbl&gt; 35561, 4090, 23084, 880, 705, 0, 76, 126755, 34877, 1… ## $ `7/20/20` &lt;dbl&gt; 35595, 4171, 23691, 884, 749, 0, 76, 130774, 34981, 1… ## $ `7/21/20` &lt;dbl&gt; 35701, 4290, 24278, 884, 779, 0, 76, 136118, 35254, 1… ## $ `7/22/20` &lt;dbl&gt; 35813, 4358, 24872, 889, 812, 0, 76, 141900, 35693, 1… ## $ `7/23/20` &lt;dbl&gt; 36001, 4466, 25484, 889, 851, 0, 76, 148027, 36162, 1… ## $ `7/24/20` &lt;dbl&gt; 36067, 4570, 26159, 897, 880, 0, 82, 153520, 36613, 1… ## $ `7/25/20` &lt;dbl&gt; 36122, 4637, 26764, 897, 916, 0, 82, 158334, 36996, 1… ## $ `7/26/20` &lt;dbl&gt; 36243, 4763, 27357, 897, 932, 0, 82, 162526, 37317, 1… ## $ `7/27/20` &lt;dbl&gt; 36349, 4880, 27973, 907, 950, 0, 86, 167416, 37390, 1… ## $ `7/28/20` &lt;dbl&gt; 36454, 4997, 28615, 907, 1000, 0, 86, 173355, 37629, … ## $ `7/29/20` &lt;dbl&gt; 36557, 5105, 29229, 918, 1078, 0, 91, 178996, 37937, … ## $ `7/30/20` &lt;dbl&gt; 36628, 5197, 29831, 922, 1109, 0, 91, 185373, 38196, … ## $ `7/31/20` &lt;dbl&gt; 36628, 5276, 30394, 925, 1148, 0, 91, 191302, 38550, … ## $ `8/1/20` &lt;dbl&gt; 36796, 5396, 30950, 925, 1164, 0, 91, 196543, 38841, … ## $ `8/2/20` &lt;dbl&gt; 36796, 5519, 31465, 925, 1199, 0, 91, 201919, 39050, … ## $ `8/3/20` &lt;dbl&gt; 36796, 5620, 31972, 937, 1280, 0, 92, 206743, 39102, … ## $ `8/4/20` &lt;dbl&gt; 36833, 5750, 32504, 939, 1344, 0, 92, 213535, 39298, … ## $ `8/5/20` &lt;dbl&gt; 36915, 5889, 33055, 939, 1395, 0, 92, 220682, 39586, … ## $ `8/6/20` &lt;dbl&gt; 36982, 6016, 33626, 944, 1483, 0, 92, 228195, 39819, … ## $ `8/7/20` &lt;dbl&gt; 37023, 6151, 34155, 955, 1538, 0, 92, 235677, 39985, … ## $ `8/8/20` &lt;dbl&gt; 37101, 6275, 34693, 955, 1572, 0, 92, 241811, 40185, … ## $ `8/9/20` &lt;dbl&gt; 37140, 6411, 35160, 955, 1672, 0, 92, 246499, 40410, … ## $ `8/10/20` &lt;dbl&gt; 37140, 6536, 35712, 963, 1679, 0, 92, 253868, 40433, … ## $ `8/11/20` &lt;dbl&gt; 37355, 6676, 36204, 963, 1735, 0, 92, 260911, 40593, … ## $ `8/12/20` &lt;dbl&gt; 37431, 6817, 36699, 977, 1762, 0, 92, 268574, 40794, … ## $ `8/13/20` &lt;dbl&gt; 37510, 6971, 37187, 981, 1815, 0, 92, 276072, 41023, … ## $ `8/14/20` &lt;dbl&gt; 37517, 7117, 37664, 989, 1852, 0, 93, 282437, 41299, … ## $ `8/15/20` &lt;dbl&gt; 37637, 7260, 38133, 989, 1879, 0, 93, 289100, 41495, … ## $ `8/16/20` &lt;dbl&gt; 37682, 7380, 38583, 989, 1906, 0, 93, 294569, 41663, … ## $ `8/17/20` &lt;dbl&gt; 37682, 7499, 39025, 1005, 1935, 0, 93, 299126, 41701,… ## $ `8/18/20` &lt;dbl&gt; 37685, 7654, 39444, 1005, 1966, 0, 93, 305966, 41846,… ## $ `8/19/20` &lt;dbl&gt; 37685, 7812, 39847, 1024, 2015, 0, 94, 312659, 42056,… ## $ `8/20/20` &lt;dbl&gt; 37845, 7967, 40258, 1024, 2044, 0, 94, 320884, 42319,… ## $ `8/21/20` &lt;dbl&gt; 37942, 8119, 40667, 1045, 2068, 0, 94, 329043, 42477,… ## $ `8/22/20` &lt;dbl&gt; 37980, 8275, 41068, 1045, 2134, 0, 94, 336802, 42616,… ## $ `8/23/20` &lt;dbl&gt; 38039, 8427, 41460, 1045, 2171, 0, 94, 342154, 42792,… ## $ `8/24/20` &lt;dbl&gt; 38085, 8605, 41858, 1060, 2222, 0, 94, 350867, 42825,… ## $ `8/25/20` &lt;dbl&gt; 38156, 8759, 42228, 1060, 2283, 0, 94, 359638, 42936,… ## $ `8/26/20` &lt;dbl&gt; 38199, 8927, 42619, 1098, 2332, 0, 94, 370188, 43067,… ## $ `8/27/20` &lt;dbl&gt; 38215, 9083, 43016, 1098, 2415, 0, 94, 380292, 43270,… ## $ `8/28/20` &lt;dbl&gt; 38226, 9195, 43403, 1124, 2471, 0, 94, 392009, 43451,… ## $ `8/29/20` &lt;dbl&gt; 38229, 9279, 43781, 1124, 2551, 0, 94, 401239, 43626,… ## $ `8/30/20` &lt;dbl&gt; 38229, 9380, 44146, 1124, 2624, 0, 94, 408426, 43750,… ## $ `8/31/20` &lt;dbl&gt; 38248, 9513, 44494, 1176, 2654, 0, 94, 417735, 43781,… ## $ `9/1/20` &lt;dbl&gt; 38282, 9606, 44833, 1184, 2729, 0, 94, 428239, 43878,… ## $ `9/2/20` &lt;dbl&gt; 38329, 9728, 45158, 1199, 2777, 0, 94, 439172, 44075,… ## $ `9/3/20` &lt;dbl&gt; 38374, 9844, 45469, 1199, 2805, 0, 95, 451198, 44271,… ## $ `9/4/20` &lt;dbl&gt; 38374, 9967, 45773, 1215, 2876, 0, 95, 461882, 44461,… ## $ `9/5/20` &lt;dbl&gt; 38390, 10102, 46071, 1215, 2935, 0, 95, 471806, 44649… ## $ `9/6/20` &lt;dbl&gt; 38484, 10255, 46364, 1215, 2965, 0, 95, 478792, 44783… ## $ `9/7/20` &lt;dbl&gt; 38580, 10406, 46653, 1261, 2981, 0, 95, 488007, 44845… ## $ `9/8/20` &lt;dbl&gt; 38606, 10553, 46938, 1261, 3033, 0, 95, 500034, 44953… ## $ `9/9/20` &lt;dbl&gt; 38630, 10704, 47216, 1301, 3092, 0, 95, 512293, 45152… ## $ `9/10/20` &lt;dbl&gt; 38658, 10860, 47488, 1301, 3217, 0, 95, 524198, 45326… ## $ `9/11/20` &lt;dbl&gt; 38692, 11021, 47752, 1344, 3279, 0, 95, 535705, 45503… ## $ `9/12/20` &lt;dbl&gt; 38727, 11185, 48007, 1344, 3335, 0, 95, 546481, 45675… ## $ `9/13/20` &lt;dbl&gt; 38802, 11353, 48254, 1344, 3388, 0, 95, 555537, 45862… ## $ `9/14/20` &lt;dbl&gt; 38858, 11520, 48496, 1438, 3439, 0, 95, 565446, 45969… ## $ `9/15/20` &lt;dbl&gt; 38901, 11672, 48734, 1438, 3569, 0, 95, 577338, 46119… ## $ `9/16/20` &lt;dbl&gt; 38941, 11816, 48966, 1483, 3675, 0, 95, 589012, 46376… ## $ `9/17/20` &lt;dbl&gt; 38958, 11948, 49194, 1483, 3789, 0, 95, 601713, 46671… ## $ `9/18/20` &lt;dbl&gt; 38969, 12073, 49413, 1564, 3848, 0, 95, 613658, 46910… ## $ `9/19/20` &lt;dbl&gt; 39005, 12226, 49623, 1564, 3901, 0, 96, 622934, 47154… ## $ `9/20/20` &lt;dbl&gt; 39130, 12385, 49826, 1564, 3991, 0, 96, 631365, 47431… ## $ `9/21/20` &lt;dbl&gt; 39160, 12535, 50023, 1681, 4117, 0, 96, 640147, 47552… ## $ `9/22/20` &lt;dbl&gt; 39182, 12666, 50214, 1681, 4236, 0, 96, 652174, 47667… ## $ `9/23/20` &lt;dbl&gt; 39231, 12787, 50400, 1753, 4363, 0, 97, 664799, 47877… ## $ `9/24/20` &lt;dbl&gt; 39256, 12921, 50579, 1753, 4475, 0, 97, 678266, 48251… ## $ `9/25/20` &lt;dbl&gt; 39272, 13045, 50754, 1836, 4590, 0, 98, 691235, 48643… ## $ `9/26/20` &lt;dbl&gt; 39278, 13153, 50914, 1836, 4672, 0, 98, 702484, 49072… ## $ `9/27/20` &lt;dbl&gt; 39313, 13259, 51067, 1836, 4718, 0, 101, 711325, 4940… ## $ `9/28/20` &lt;dbl&gt; 39325, 13391, 51213, 1966, 4797, 0, 101, 723132, 4957… ## $ `9/29/20` &lt;dbl&gt; 39340, 13518, 51368, 1966, 4905, 0, 101, 736609, 4990… ## $ `9/30/20` &lt;dbl&gt; 39354, 13649, 51530, 2050, 4972, 0, 101, 751001, 5035… ## $ `10/1/20` &lt;dbl&gt; 39371, 13806, 51690, 2050, 5114, 0, 101, 765002, 5085… ## $ `10/2/20` &lt;dbl&gt; 39376, 13965, 51847, 2110, 5211, 0, 106, 779689, 5138… ## $ `10/3/20` &lt;dbl&gt; 39383, 14117, 51995, 2110, 5370, 0, 107, 790818, 5192… ## $ `10/4/20` &lt;dbl&gt; 39427, 14266, 52136, 2110, 5402, 0, 107, 798486, 5249… ## $ `10/5/20` &lt;dbl&gt; 39508, 14410, 52270, 2370, 5530, 0, 107, 809728, 5267… ## $ `10/6/20` &lt;dbl&gt; 39572, 14568, 52399, 2370, 5725, 0, 107, 824468, 5308… ## $ `10/7/20` &lt;dbl&gt; 39634, 14730, 52520, 2568, 5725, 0, 108, 840915, 5375… ## $ `10/8/20` &lt;dbl&gt; 39702, 14899, 52658, 2568, 5958, 0, 111, 856369, 5447… ## $ `10/9/20` &lt;dbl&gt; 39779, 15066, 52804, 2696, 6031, 0, 111, 871468, 5508… ## $ `10/10/20` &lt;dbl&gt; 39789, 15231, 52940, 2696, 6246, 0, 111, 883882, 5573… ## $ `10/11/20` &lt;dbl&gt; 39885, 15399, 53072, 2696, 6366, 0, 111, 894206, 5645… ## $ `10/12/20` &lt;dbl&gt; 39956, 15570, 53325, 2995, 6488, 0, 111, 903730, 5682… ## $ `10/13/20` &lt;dbl&gt; 40014, 15752, 53399, 2995, 6680, 0, 111, 917035, 5756… ## $ `10/14/20` &lt;dbl&gt; 40080, 15955, 53584, 3190, 6846, 0, 112, 931967, 5862… ## $ `10/15/20` &lt;dbl&gt; 40112, 16212, 53777, 3190, 7096, 0, 112, 949063, 5999… ## $ `10/16/20` &lt;dbl&gt; 40159, 16501, 53998, 3377, 7222, 0, 112, 965609, 6146… ## $ `10/17/20` &lt;dbl&gt; 40227, 16774, 54203, 3377, 7462, 0, 119, 979119, 6300… ## $ `10/18/20` &lt;dbl&gt; 40286, 17055, 54402, 3377, 7622, 0, 119, 989680, 6469… ## $ `10/19/20` &lt;dbl&gt; 40373, 17350, 54616, 3623, 7829, 0, 119, 1002662, 654… ## $ `10/20/20` &lt;dbl&gt; 40461, 17651, 54829, 3623, 8049, 0, 119, 1018999, 666… ## $ `10/21/20` &lt;dbl&gt; 40461, 17948, 55081, 3811, 8338, 0, 122, 1037325, 685… ## $ `10/22/20` &lt;dbl&gt; 40510, 18250, 55357, 3811, 8582, 0, 122, 1053650, 708… ## $ `10/23/20` &lt;dbl&gt; 40626, 18556, 55630, 4038, 8829, 0, 122, 1069368, 733… ## $ `10/24/20` &lt;dbl&gt; 40687, 18858, 55880, 4038, 9026, 0, 124, 1081336, 755… ## $ `10/25/20` &lt;dbl&gt; 40768, 19157, 56143, 4038, 9381, 0, 124, 1090589, 778… ## $ `10/26/20` &lt;dbl&gt; 40833, 19445, 56419, 4325, 9644, 0, 124, 1102301, 788… ## $ `10/27/20` &lt;dbl&gt; 40937, 19729, 56706, 4410, 9871, 0, 124, 1116609, 804… ## $ `10/28/20` &lt;dbl&gt; 41032, 20040, 57026, 4517, 10074, 0, 124, 1130533, 82… ## $ `10/29/20` &lt;dbl&gt; 41145, 20315, 57332, 4567, 10269, 0, 124, 1143800, 85… ## $ `10/30/20` &lt;dbl&gt; 41268, 20634, 57651, 4665, 10558, 0, 127, 1157179, 87… ## $ `10/31/20` &lt;dbl&gt; 41334, 20875, 57942, 4756, 10805, 0, 128, 1166924, 89… ## $ `11/1/20` &lt;dbl&gt; 41425, 21202, 58272, 4825, 11035, 0, 128, 1173533, 92… ## $ `11/2/20` &lt;dbl&gt; 41501, 21523, 58574, 4888, 11228, 0, 128, 1183131, 93… ## $ `11/3/20` &lt;dbl&gt; 41633, 21904, 58979, 4910, 11577, 0, 128, 1195276, 94… ## $ `11/4/20` &lt;dbl&gt; 41728, 22300, 59527, 5045, 11813, 0, 130, 1205928, 97… ## $ `11/5/20` &lt;dbl&gt; 41814, 22721, 60169, 5135, 12102, 0, 130, 1217028, 99… ## $ `11/6/20` &lt;dbl&gt; 41935, 23210, 60800, 5135, 12223, 0, 130, 1228814, 10… ## $ `11/7/20` &lt;dbl&gt; 41975, 23705, 61381, 5319, 12335, 0, 131, 1236851, 10… ## $ `11/8/20` &lt;dbl&gt; 42033, 24206, 62051, 5383, 12433, 0, 131, 1242182, 10… ## $ `11/9/20` &lt;dbl&gt; 42159, 24731, 62693, 5437, 12680, 0, 131, 1250499, 10… ## $ `11/10/20` &lt;dbl&gt; 42297, 25294, 63446, 5477, 12816, 0, 131, 1262476, 10… ## $ `11/11/20` &lt;dbl&gt; 42463, 25801, 64257, 5567, 12953, 0, 131, 1273356, 11… ## $ `11/12/20` &lt;dbl&gt; 42609, 26211, 65108, 5616, 13053, 0, 131, 1284519, 11… ## $ `11/13/20` &lt;dbl&gt; 42795, 26701, 65975, 5725, 13228, 0, 133, 1296378, 11… ## $ `11/14/20` &lt;dbl&gt; 42969, 27233, 66819, 5725, 13374, 0, 134, 1304846, 11… ## $ `11/15/20` &lt;dbl&gt; 43035, 27830, 67679, 5872, 13451, 0, 134, 1310491, 11… ## $ `11/16/20` &lt;dbl&gt; 43240, 28432, 68589, 5914, 13615, 0, 134, 1318384, 11… ## $ `11/17/20` &lt;dbl&gt; 43403, 29126, 69591, 5951, 13818, 0, 134, 1329005, 11… ## $ `11/18/20` &lt;dbl&gt; 43628, 29837, 70629, 6018, 13922, 0, 139, 1339337, 12… ## $ `11/19/20` &lt;dbl&gt; 43851, 30623, 71652, 6066, 14134, 0, 139, 1349434, 12… ## $ `11/20/20` &lt;dbl&gt; 44228, 31459, 72755, 6142, 14267, 0, 139, 1359042, 12… ## $ `11/21/20` &lt;dbl&gt; 44443, 32196, 73774, 6207, 14413, 0, 139, 1366182, 12… ## $ `11/22/20` &lt;dbl&gt; 44503, 32761, 74862, 6256, 14493, 0, 139, 1370366, 12… ## $ `11/23/20` &lt;dbl&gt; 44706, 33556, 75867, 6304, 14634, 0, 139, 1374631, 12… ## $ `11/24/20` &lt;dbl&gt; 44988, 34300, 77000, 6351, 14742, 0, 139, 1381795, 12… ## $ `11/25/20` &lt;dbl&gt; 45278, 34944, 78025, 6428, 14821, 0, 140, 1390388, 12… ## $ `11/26/20` &lt;dbl&gt; 45490, 35600, 79110, 6534, 14920, 0, 141, 1399431, 13… ## $ `11/27/20` &lt;dbl&gt; 45716, 36245, 80168, 6610, 15008, 0, 141, 1407277, 13… ## $ `11/28/20` &lt;dbl&gt; 45839, 36790, 81212, 6610, 15087, 0, 141, 1413375, 13… ## $ `11/29/20` &lt;dbl&gt; 45966, 37625, 82221, 6712, 15103, 0, 141, 1418807, 13… ## $ `11/30/20` &lt;dbl&gt; 46215, 38182, 83199, 6745, 15139, 0, 141, 1424533, 13… ## $ `12/1/20` &lt;dbl&gt; 46498, 39014, 84152, 6790, 15251, 0, 142, 1432570, 13… ## $ `12/2/20` &lt;dbl&gt; 46717, 39719, 85084, 6842, 15319, 0, 144, 1440103, 13… ## $ `12/3/20` &lt;dbl&gt; 46980, 40501, 85927, 6904, 15361, 0, 144, 1447732, 13… ## $ `12/4/20` &lt;dbl&gt; 47258, 41302, 86730, 6955, 15493, 0, 144, 1454631, 13… ## $ `12/5/20` &lt;dbl&gt; 47388, 42148, 87502, 7005, 15536, 0, 144, 1459832, 14… ## $ `12/6/20` &lt;dbl&gt; 47641, 42988, 88252, 7050, 15591, 0, 144, 1463110, 14… ## $ `12/7/20` &lt;dbl&gt; 47901, 43683, 88825, 7084, 15648, 0, 146, 1466309, 14… ## $ `12/8/20` &lt;dbl&gt; 48136, 44436, 89416, 7127, 15729, 0, 146, 1469919, 14… ## $ `12/9/20` &lt;dbl&gt; 48366, 45188, 90014, 7162, 15804, 0, 146, 1475222, 14… ## $ `12/10/20` &lt;dbl&gt; 48540, 46061, 90579, 7190, 15925, 0, 146, 1482216, 14… ## $ `12/11/20` &lt;dbl&gt; 48753, 46863, 91121, 7236, 16061, 0, 147, 1489328, 14… ## $ `12/12/20` &lt;dbl&gt; 48826, 47742, 91638, 7288, 16161, 0, 148, 1494602, 14… ## $ `12/13/20` &lt;dbl&gt; 48952, 48530, 92102, 7338, 16188, 0, 148, 1498160, 14… ## $ `12/14/20` &lt;dbl&gt; 49273, 49191, 92597, 7382, 16277, 0, 148, 1503222, 14… ## $ `12/15/20` &lt;dbl&gt; 49484, 50000, 93065, 7382, 16362, 0, 148, 1510203, 14… ## $ `12/16/20` &lt;dbl&gt; 49703, 50637, 93507, 7446, 16407, 0, 151, 1517046, 15… ## $ `12/17/20` &lt;dbl&gt; 49927, 51424, 93933, 7466, 16484, 0, 151, 1524372, 15… ## $ `12/18/20` &lt;dbl&gt; 50202, 52004, 94371, 7519, 16562, 0, 152, 1531374, 15… ## $ `12/19/20` &lt;dbl&gt; 50456, 52542, 94781, 7560, 16626, 0, 152, 1537169, 15… ## $ `12/20/20` &lt;dbl&gt; 50536, 53003, 95203, 7577, 16644, 0, 153, 1541285, 15… ## $ `12/21/20` &lt;dbl&gt; 50678, 53425, 95659, 7602, 16686, 0, 153, 1547138, 15… ## $ `12/22/20` &lt;dbl&gt; 50888, 53814, 96069, 7633, 16802, 0, 153, 1555279, 15… ## $ `12/23/20` &lt;dbl&gt; 51070, 54317, 96549, 7669, 16931, 0, 154, 1563865, 15… ## $ `12/24/20` &lt;dbl&gt; 51357, 54827, 97007, 7699, 17029, 0, 154, 1563865, 15… ## $ `12/25/20` &lt;dbl&gt; 51595, 55380, 97441, 7756, 17099, 0, 155, 1574554, 15… ## $ `12/26/20` &lt;dbl&gt; 51764, 55755, 97857, 7806, 17149, 0, 155, 1578267, 15… ## $ `12/27/20` &lt;dbl&gt; 51848, 56254, 98249, 7821, 17240, 0, 155, 1583297, 15… ## $ `12/28/20` &lt;dbl&gt; 52007, 56572, 98631, 7875, 17296, 0, 158, 1590513, 15… ## $ `12/29/20` &lt;dbl&gt; 52147, 57146, 98988, 7919, 17371, 0, 158, 1602163, 15… ## $ `12/30/20` &lt;dbl&gt; 52330, 57727, 99311, 7983, 17433, 0, 158, 1613928, 15… ## $ `12/31/20` &lt;dbl&gt; 52330, 58316, 99610, 8049, 17553, 0, 159, 1625514, 15… ## $ `1/1/21` &lt;dbl&gt; 52513, 58316, 99897, 8117, 17568, 0, 159, 1629594, 15… ## $ `1/2/21` &lt;dbl&gt; 52586, 58991, 100159, 8166, 17608, 0, 159, 1634834, 1… ## $ `1/3/21` &lt;dbl&gt; 52709, 59438, 100408, 8192, 17642, 0, 160, 1640718, 1… ## $ `1/4/21` &lt;dbl&gt; 52909, 59623, 100645, 8249, 17684, 0, 160, 1648940, 1… ## $ `1/5/21` &lt;dbl&gt; 53011, 60283, 100873, 8308, 17756, 0, 160, 1662730, 1… ## $ `1/6/21` &lt;dbl&gt; 53105, 61008, 101120, 8348, 17864, 0, 163, 1676171, 1… ## $ `1/7/21` &lt;dbl&gt; 53207, 61705, 101382, 8348, 17974, 0, 163, 1690006, 1… ## $ `1/8/21` &lt;dbl&gt; 53332, 62378, 101657, 8489, 18066, 0, 167, 1703352, 1… ## $ `1/9/21` &lt;dbl&gt; 53400, 63033, 101913, 8586, 18156, 0, 169, 1714409, 1… ## $ `1/10/21` &lt;dbl&gt; 53489, 63595, 102144, 8586, 18193, 0, 176, 1722217, 1… ## $ `1/11/21` &lt;dbl&gt; 53538, 63971, 102369, 8586, 18254, 0, 176, 1730921, 1… ## $ `1/12/21` &lt;dbl&gt; 53584, 64627, 102641, 8682, 18343, 0, 176, 1744704, 1… ## $ `1/13/21` &lt;dbl&gt; 53690, 65334, 102860, 8818, 18425, 0, 176, 1757429, 1… ## $ `1/14/21` &lt;dbl&gt; 53775, 65994, 103127, 8868, 18613, 0, 184, 1770715, 1… ## $ `1/15/21` &lt;dbl&gt; 53831, 66635, 103381, 8946, 18679, 0, 184, 1783047, 1… ## $ `1/16/21` &lt;dbl&gt; 53938, 67216, 103611, 9038, 18765, 0, 187, 1791979, 1… ## $ `1/17/21` &lt;dbl&gt; 53984, 67690, 103833, 9083, 18875, 0, 189, 1799243, 1… ## $ `1/18/21` &lt;dbl&gt; 54062, 67982, 104092, 9083, 18926, 0, 189, 1807428, 1… ## $ `1/19/21` &lt;dbl&gt; 54141, 68568, 104341, 9194, 19011, 0, 190, 1819569, 1… ## $ `1/20/21` &lt;dbl&gt; 54278, 69238, 104606, 9308, 19093, 0, 190, 1831681, 1… ## $ `1/21/21` &lt;dbl&gt; 54403, 69916, 104852, 9379, 19177, 0, 192, 1843077, 1… ## $ `1/22/21` &lt;dbl&gt; 54483, 70655, 105124, 9416, 19269, 0, 195, 1853830, 1… ## $ `1/23/21` &lt;dbl&gt; 54559, 71441, 105369, 9499, 19367, 0, 195, 1862192, 1… ## $ `1/24/21` &lt;dbl&gt; 54595, 72274, 105596, 9549, 19399, 0, 198, 1867223, 1… ## $ `1/25/21` &lt;dbl&gt; 54672, 72812, 105854, 9596, 19476, 0, 201, 1874801, 1… ## $ `1/26/21` &lt;dbl&gt; 54750, 73691, 106097, 9638, 19553, 0, 201, 1885210, 1… ## $ `1/27/21` &lt;dbl&gt; 54854, 74567, 106359, 9716, 19580, 0, 215, 1896053, 1… ## $ `1/28/21` &lt;dbl&gt; 54891, 75454, 106610, 9779, 19672, 0, 215, 1905524, 1… ## $ `1/29/21` &lt;dbl&gt; 54939, 76350, 106887, 9837, 19723, 0, 218, 1915362, 1… ## $ `1/30/21` &lt;dbl&gt; 55008, 77251, 107122, 9885, 19782, 0, 218, 1922264, 1… ## $ `1/31/21` &lt;dbl&gt; 55023, 78127, 107339, 9937, 19796, 0, 234, 1927239, 1… ## $ `2/1/21` &lt;dbl&gt; 55059, 78992, 107578, 9972, 19829, 0, 234, 1933853, 1… ## $ `2/2/21` &lt;dbl&gt; 55121, 79934, 107841, 10017, 19900, 0, 249, 1943548, … ## $ `2/3/21` &lt;dbl&gt; 55174, 80941, 108116, 10070, 19937, 0, 249, 1952744, … ## $ `2/4/21` &lt;dbl&gt; 55231, 81993, 108381, 10137, 19996, 0, 268, 1961635, … ## $ `2/5/21` &lt;dbl&gt; 55265, 83082, 108629, 10172, 20030, 0, 277, 1970009, … ## $ `2/6/21` &lt;dbl&gt; 55330, 84212, 108629, 10206, 20062, 0, 288, 1976689, … ## $ `2/7/21` &lt;dbl&gt; 55335, 85336, 109088, 10251, 20086, 0, 299, 1980347, … ## $ `2/8/21` &lt;dbl&gt; 55359, 86289, 109313, 10275, 20112, 0, 316, 1985501, … ## $ `2/9/21` &lt;dbl&gt; 55384, 87528, 109559, 10312, 20163, 0, 316, 1993295, … ## $ `2/10/21` &lt;dbl&gt; 55402, 88671, 109782, 10352, 20210, 0, 350, 2001034, … ## $ `2/11/21` &lt;dbl&gt; 55420, 89776, 110049, 10391, 20261, 0, 381, 2008345, … ## $ `2/12/21` &lt;dbl&gt; 55445, 90835, 110303, 10427, 20294, 0, 419, 2015496, … ## $ `2/13/21` &lt;dbl&gt; 55473, 91987, 110513, 10463, 20329, 0, 427, 2021553, … ## $ `2/14/21` &lt;dbl&gt; 55492, 93075, 110711, 10503, 20366, 0, 427, 2025798, … ## $ `2/15/21` &lt;dbl&gt; 55514, 93850, 110894, 10538, 20381, 0, 443, 2029057, … ## $ `2/16/21` &lt;dbl&gt; 55518, 94651, 111069, 10555, 20389, 0, 443, 2033060, … ## $ `2/17/21` &lt;dbl&gt; 55540, 95726, 111247, 10583, 20400, 0, 525, 2039124, … ## $ `2/18/21` &lt;dbl&gt; 55557, 96838, 111418, 10610, 20452, 0, 548, 2046795, … ## $ `2/19/21` &lt;dbl&gt; 55575, 97909, 111600, 10645, 20478, 0, 548, 2054681, … ## $ `2/20/21` &lt;dbl&gt; 55580, 99062, 111764, 10672, 20499, 0, 598, 2060625, … ## $ `2/21/21` &lt;dbl&gt; 55604, 100246, 111917, 10699, 20519, 0, 598, 2064334,… ## $ `2/22/21` &lt;dbl&gt; 55617, 101285, 112094, 10712, 20548, 0, 614, 2069751,… ## $ `2/23/21` &lt;dbl&gt; 55646, 102306, 112279, 10739, 20584, 0, 636, 2077228,… ## $ `2/24/21` &lt;dbl&gt; 55664, 103327, 112461, 10775, 20640, 0, 646, 2085411,… ## $ `2/25/21` &lt;dbl&gt; 55680, 104313, 112622, 10799, 20695, 0, 701, 2093645,… ## $ `2/26/21` &lt;dbl&gt; 55696, 105229, 112805, 10822, 20759, 0, 701, 2098728,… ## $ `2/27/21` &lt;dbl&gt; 55707, 106215, 112960, 10849, 20782, 0, 726, 2104197,… ## $ `2/28/21` &lt;dbl&gt; 55714, 107167, 113092, 10866, 20807, 0, 730, 2107365,… ## $ `3/1/21` &lt;dbl&gt; 55733, 107931, 113255, 10889, 20854, 0, 769, 2112023,… ## $ `3/2/21` &lt;dbl&gt; 55759, 108823, 113430, 10908, 20882, 0, 769, 2118676,… ## $ `3/3/21` &lt;dbl&gt; 55770, 109674, 113593, 10948, 20923, 0, 769, 2126531,… ## $ `3/4/21` &lt;dbl&gt; 55775, 110521, 113761, 10976, 20981, 0, 813, 2133963,… ## $ `3/5/21` &lt;dbl&gt; 55827, 111301, 113948, 10998, 21026, 0, 813, 2141854,… ## $ `3/6/21` &lt;dbl&gt; 55840, 112078, 114104, 11019, 21055, 0, 813, 2146714,… ## $ `3/7/21` &lt;dbl&gt; 55847, 112897, 114234, 11042, 21086, 0, 848, 2149636,… ## $ `3/8/21` &lt;dbl&gt; 55876, 113580, 114382, 11069, 21108, 0, 848, 2154694,… ## $ `3/9/21` &lt;dbl&gt; 55876, 114209, 114543, 11089, 21114, 0, 862, 2162001,… ## $ `3/10/21` &lt;dbl&gt; 55894, 114840, 114681, 11130, 21161, 0, 882, 2169694,… ## $ `3/11/21` &lt;dbl&gt; 55917, 115442, 114851, 11130, 21205, 0, 882, 2177898,… ## $ `3/12/21` &lt;dbl&gt; 55959, 116123, 115008, 11199, 21265, 0, 945, 2185747,… ## $ `3/13/21` &lt;dbl&gt; 55959, 116821, 115143, 11228, 21323, 0, 962, 2192025,… ## $ `3/14/21` &lt;dbl&gt; 55985, 117474, 115265, 11266, 21380, 0, 963, 2195722,… ## $ `3/15/21` &lt;dbl&gt; 55985, 118017, 115410, 11289, 21407, 0, 963, 2201886,… ## $ `3/16/21` &lt;dbl&gt; 55995, 118492, 115540, 11319, 21446, 0, 992, 2210121,… ## $ `3/17/21` &lt;dbl&gt; 56016, 118938, 115688, 11360, 21489, 0, 992, 2218425,… ## $ `3/18/21` &lt;dbl&gt; 56044, 119528, 115842, 11393, 21558, 0, 1008, 2226753… ## $ `3/19/21` &lt;dbl&gt; 56069, 120022, 115970, 11431, 21642, 0, 1011, 2234913… ## $ `3/20/21` &lt;dbl&gt; 56093, 120541, 116066, 11481, 21696, 0, 1033, 2241739… ## $ `3/21/21` &lt;dbl&gt; 56103, 121200, 116157, 11517, 21733, 0, 1033, 2245771… ## $ `3/22/21` &lt;dbl&gt; 56153, 121544, 116255, 11545, 21757, 0, 1072, 2252172… ## $ `3/23/21` &lt;dbl&gt; 56177, 121847, 116349, 11591, 21774, 0, 1080, 2261577… ## $ `3/24/21` &lt;dbl&gt; 56192, 122295, 116438, 11638, 21836, 0, 1080, 2269877… ## $ `3/25/21` &lt;dbl&gt; 56226, 122767, 116543, 11687, 21914, 0, 1103, 2278115… ## $ `3/26/21` &lt;dbl&gt; 56254, 123216, 116657, 11732, 21961, 0, 1122, 2291051… ## $ `3/27/21` &lt;dbl&gt; 56290, 123641, 116750, 11809, 22031, 0, 1122, 2301389… ## $ `3/28/21` &lt;dbl&gt; 56294, 124134, 116836, 11850, 22063, 0, 1128, 2308597… ## $ `3/29/21` &lt;dbl&gt; 56322, 124419, 116946, 11888, 22132, 0, 1136, 2322611… ## $ `3/30/21` &lt;dbl&gt; 56384, 124723, 117061, 11944, 22182, 0, 1136, 2332765… ## $ `3/31/21` &lt;dbl&gt; 56454, 125157, 117192, 12010, 22311, 0, 1136, 2348821… ## $ `4/1/21` &lt;dbl&gt; 56517, 125506, 117304, 12053, 22399, 0, 1147, 2363251… ## $ `4/2/21` &lt;dbl&gt; 56572, 125842, 117429, 12115, 22467, 0, 1152, 2373153… ## $ `4/3/21` &lt;dbl&gt; 56595, 126183, 117524, 12174, 22579, 0, 1170, 2383537… ## $ `4/4/21` &lt;dbl&gt; 56676, 126531, 117622, 12231, 22631, 0, 1170, 2393492… ## $ `4/5/21` &lt;dbl&gt; 56717, 126795, 117739, 12286, 22717, 0, 1173, 2407159… ## $ `4/6/21` &lt;dbl&gt; 56779, 126936, 117879, 12328, 22885, 0, 1173, 2428029… ## $ `4/7/21` &lt;dbl&gt; 56873, 127192, 118004, 12363, 23010, 0, 1177, 2450068… ## $ `4/8/21` &lt;dbl&gt; 56943, 127509, 118116, 12409, 23108, 0, 1180, 2473751… ## $ `4/9/21` &lt;dbl&gt; 57019, 127795, 118251, 12456, 23242, 0, 1182, 2497881… ## $ `4/10/21` &lt;dbl&gt; 57144, 128155, 118378, 12497, 23331, 0, 1197, 2517300… ## $ `4/11/21` &lt;dbl&gt; 57160, 128393, 118516, 12545, 23457, 0, 1198, 2532562… ## $ `4/12/21` &lt;dbl&gt; 57242, 128518, 118645, 12581, 23549, 0, 1198, 2551999… ## $ `4/13/21` &lt;dbl&gt; 57364, 128752, 118799, 12614, 23697, 0, 1201, 2579000… ## $ `4/14/21` &lt;dbl&gt; 57492, 128959, 118975, 12641, 23841, 0, 1201, 2604157… ## $ `4/15/21` &lt;dbl&gt; 57534, 129128, 119142, 12641, 23951, 0, 1209, 2629156… ## $ `4/16/21` &lt;dbl&gt; 57612, 129307, 119323, 12712, 24122, 0, 1213, 2658628… ## $ `4/17/21` &lt;dbl&gt; 57721, 129456, 119486, 12771, 24300, 0, 1216, 2677747… ## $ `4/18/21` &lt;dbl&gt; 57793, 129594, 119642, 12805, 24389, 0, 1216, 2694014… ## $ `4/19/21` &lt;dbl&gt; 57898, 129694, 119805, 12805, 24518, 0, 1217, 2714475… ## $ `4/20/21` &lt;dbl&gt; 58037, 129842, 119992, 12874, 24661, 0, 1217, 2743620… ## $ `4/21/21` &lt;dbl&gt; 58214, 129980, 120174, 12917, 24883, 0, 1217, 2769552… ## $ `4/22/21` &lt;dbl&gt; 58312, 130114, 120363, 12942, 25051, 0, 1217, 2796768… ## $ `4/23/21` &lt;dbl&gt; 58542, 130270, 120562, 13007, 25279, 0, 1222, 2824652… ## $ `4/24/21` &lt;dbl&gt; 58730, 130409, 120736, 13024, 25492, 0, 1227, 2845872… ## $ `4/25/21` &lt;dbl&gt; 58843, 130537, 120922, 13060, 25609, 0, 1227, 2860884… ## $ `4/26/21` &lt;dbl&gt; 59015, 130606, 121112, 13083, 25710, 0, 1228, 2879677… ## $ `4/27/21` &lt;dbl&gt; 59225, 130736, 121344, 13121, 25942, 0, 1232, 2905172… ## $ `4/28/21` &lt;dbl&gt; 59370, 130859, 121580, 13148, 26168, 0, 1232, 2928890… ## $ `4/29/21` &lt;dbl&gt; 59576, 130977, 121866, 13198, 26431, 0, 1232, 2954943… ## $ `4/30/21` &lt;dbl&gt; 59745, 131085, 122108, 13232, 26652, 0, 1232, 2977363… ## $ `5/1/21` &lt;dbl&gt; 59939, 131185, 122311, 13232, 26815, 0, 1232, 2993865… ## $ `5/2/21` &lt;dbl&gt; 60122, 131238, 122522, 13282, 26993, 0, 1232, 3005259… ## $ `5/3/21` &lt;dbl&gt; 60300, 131276, 122717, 13295, 27133, 0, 1232, 3021179… ## $ `5/4/21` &lt;dbl&gt; 60563, 131327, 122999, 13316, 27284, 0, 1232, 3047417… ## $ `5/5/21` &lt;dbl&gt; 60797, 131419, 123272, 13340, 27529, 0, 1232, 3071496… ## $ `5/6/21` &lt;dbl&gt; 61162, 131510, 123473, 13363, 27921, 0, 1232, 3095582… ## $ `5/7/21` &lt;dbl&gt; 61455, 131577, 123692, 13390, 28201, 0, 1232, 3118134… ## $ `5/8/21` &lt;dbl&gt; 61755, 131666, 123900, 13406, 28477, 0, 1232, 3136158… ## $ `5/9/21` &lt;dbl&gt; 61842, 131723, 124104, 13423, 28740, 0, 1231, 3147740… ## $ `5/10/21` &lt;dbl&gt; 62063, 131753, 124288, 13429, 28875, 0, 1237, 3165121… ## $ `5/11/21` &lt;dbl&gt; 62403, 131803, 124483, 13447, 29146, 0, 1238, 3191097… ## $ `5/12/21` &lt;dbl&gt; 62718, 131845, 124682, 13470, 29405, 0, 1240, 3215572… ## $ `5/13/21` &lt;dbl&gt; 63045, 131890, 124889, 13470, 29695, 0, 1240, 3242103… ## $ `5/14/21` &lt;dbl&gt; 63355, 131939, 125059, 13510, 30030, 0, 1240, 3269466… ## $ `5/15/21` &lt;dbl&gt; 63412, 131978, 125194, 13510, 30354, 0, 1241, 3290935… ## $ `5/16/21` &lt;dbl&gt; 63484, 132015, 125311, 13510, 30637, 0, 1241, 3307285… ## $ `5/17/21` &lt;dbl&gt; 63598, 132032, 125485, 13555, 30787, 0, 1251, 3335965… ## $ `5/18/21` &lt;dbl&gt; 63819, 132071, 125693, 13569, 31045, 0, 1251, 3371508… ## $ `5/19/21` &lt;dbl&gt; 64122, 132095, 125896, 13569, 31438, 0, 1252, 3411160… ## $ `5/20/21` &lt;dbl&gt; 64575, 132118, 126156, 13569, 31661, 0, 1255, 3447044… ## $ `5/21/21` &lt;dbl&gt; 65080, 132153, 126434, 13569, 31909, 0, 1255, 3482512… ## $ `5/22/21` &lt;dbl&gt; 65486, 132176, 126651, 13569, 32149, 0, 1257, 3514683… ## $ `5/23/21` &lt;dbl&gt; 65728, 132209, 126860, 13569, 32441, 0, 1257, 3539484… ## $ `5/24/21` &lt;dbl&gt; 66275, 132215, 127107, 13569, 32623, 0, 1258, 3562135… ## $ `5/25/21` &lt;dbl&gt; 66903, 132229, 127361, 13664, 32933, 0, 1258, 3586736… ## $ `5/26/21` &lt;dbl&gt; 67743, 132244, 127646, 13671, 33338, 0, 1258, 3622135… ## $ `5/27/21` &lt;dbl&gt; 68366, 132264, 127926, 13682, 33607, 0, 1258, 3663215… ## $ `5/28/21` &lt;dbl&gt; 69130, 132285, 128198, 13693, 33944, 0, 1259, 3702422… ## $ `5/29/21` &lt;dbl&gt; 70111, 132297, 128456, 13693, 34180, 0, 1259, 3732263… ## $ `5/30/21` &lt;dbl&gt; 70761, 132309, 128725, 13693, 34366, 0, 1259, 3753609… ## $ `5/31/21` &lt;dbl&gt; 71838, 132315, 128913, 13727, 34551, 0, 1260, 3781784… ## $ `6/1/21` &lt;dbl&gt; 72977, 132337, 129218, 13729, 34752, 0, 1260, 3817139… ## $ `6/2/21` &lt;dbl&gt; 74026, 132351, 129640, 13744, 34960, 0, 1262, 3852156… ## $ `6/3/21` &lt;dbl&gt; 75119, 132360, 129976, 13752, 35140, 0, 1262, 3884447… ## $ `6/4/21` &lt;dbl&gt; 76628, 132372, 130361, 13758, 35307, 0, 1263, 3915397… ## $ `6/5/21` &lt;dbl&gt; 77963, 132374, 130681, 13758, 35594, 0, 1263, 3939024… ## $ `6/6/21` &lt;dbl&gt; 79224, 132379, 130958, 13758, 35772, 0, 1263, 3955439… ## $ `6/7/21` &lt;dbl&gt; 80841, 132384, 131283, 13777, 35854, 0, 1263, 3977634… ## $ `6/8/21` &lt;dbl&gt; 82326, 132397, 131647, 13781, 36004, 0, 1263, 4008771… ## $ `6/9/21` &lt;dbl&gt; 84050, 132415, 132034, 13791, 36115, 0, 1263, 4038528… ## $ `6/10/21` &lt;dbl&gt; 85892, 132426, 132355, 13805, 36325, 0, 1263, 4066156… ## $ `6/11/21` &lt;dbl&gt; 87716, 132437, 132727, 13813, 36455, 0, 1263, 4093090… ## $ `6/12/21` &lt;dbl&gt; 88740, 132449, 133070, 13813, 36600, 0, 1263, 4111147… ## $ `6/13/21` &lt;dbl&gt; 89861, 132459, 133388, 13813, 36705, 0, 1263, 4124190… ## $ `6/14/21` &lt;dbl&gt; 91458, 132461, 133742, 13826, 36790, 0, 1263, 4145482… ## $ `6/15/21` &lt;dbl&gt; 93272, 132469, 134115, 13828, 36921, 0, 1263, 4172742… ## $ `6/16/21` &lt;dbl&gt; 93288, 132476, 134458, 13836, 37094, 0, 1263, 4198620… ## $ `6/17/21` &lt;dbl&gt; 96531, 132481, 134840, 13839, 37289, 0, 1263, 4222400… ## $ `6/18/21` &lt;dbl&gt; 98734, 132484, 135219, 13842, 37467, 0, 1263, 4242763… ## $ `6/19/21` &lt;dbl&gt; 100521, 132488, 135586, 13842, 37604, 0, 1263, 425839… ## $ `6/20/21` &lt;dbl&gt; 101906, 132490, 135821, 13842, 37678, 0, 1263, 426878… ## $ `6/21/21` &lt;dbl&gt; 103902, 132490, 136294, 13864, 37748, 0, 1263, 427739… ## $ `6/22/21` &lt;dbl&gt; 105749, 132496, 136679, 13864, 37874, 0, 1263, 429878… ## $ `6/23/21` &lt;dbl&gt; 107957, 132497, 137049, 13873, 38002, 0, 1263, 432610… ## $ `6/24/21` &lt;dbl&gt; 109532, 132499, 137403, 13877, 38091, 0, 1263, 435056… ## $ `6/25/21` &lt;dbl&gt; 111592, 132506, 137772, 13882, 38371, 0, 1263, 437458… ## $ `6/26/21` &lt;dbl&gt; 113124, 132509, 138113, 13882, 38528, 0, 1263, 439314… ## $ `6/27/21` &lt;dbl&gt; 114220, 132512, 138465, 13882, 38556, 0, 1263, 440524… ## $ `6/28/21` &lt;dbl&gt; 115751, 132513, 138840, 13882, 38613, 0, 1263, 442363… ## $ `6/29/21` &lt;dbl&gt; 117158, 132514, 139229, 13900, 38682, 0, 1263, 444770… ## $ `6/30/21` &lt;dbl&gt; 118659, 132521, 139626, 13911, 38849, 0, 1263, 447037… ## $ `7/1/21` &lt;dbl&gt; 120216, 132523, 140075, 13918, 38965, 0, 1264, 449155… ## $ `7/2/21` &lt;dbl&gt; 122156, 132526, 140550, 13918, 39089, 0, 1264, 451243… ## $ `7/3/21` &lt;dbl&gt; 123485, 132534, 141007, 13918, 39172, 0, 1264, 452647… ## $ `7/4/21` &lt;dbl&gt; 124748, 132535, 141471, 13918, 39230, 0, 1264, 453547… ## $ `7/5/21` &lt;dbl&gt; 125937, 132537, 141966, 13918, 39300, 0, 1264, 455275… ## $ `7/6/21` &lt;dbl&gt; 127464, 132544, 142447, 13991, 39375, 0, 1265, 457434… ## $ `7/7/21` &lt;dbl&gt; 129021, 132557, 143032, 14021, 39491, 0, 1265, 459376… ## $ `7/8/21` &lt;dbl&gt; 130113, 132565, 143652, 14050, 39593, 0, 1266, 461301… ## $ `7/9/21` &lt;dbl&gt; 131586, 132580, 144483, 14075, 39791, 0, 1266, 462753… ## $ `7/10/21` &lt;dbl&gt; 132777, 132587, 145296, 14075, 39881, 0, 1266, 463909… ## $ `7/11/21` &lt;dbl&gt; 133578, 132592, 146064, 14075, 39958, 0, 1266, 464794… ## $ `7/12/21` &lt;dbl&gt; 134653, 132597, 146942, 14155, 40055, 0, 1266, 466293… ## $ `7/13/21` &lt;dbl&gt; 135889, 132608, 147883, 14167, 40138, 0, 1266, 468296… ## $ `7/14/21` &lt;dbl&gt; 136643, 132616, 148797, 14167, 40327, 0, 1267, 470265… ## $ `7/15/21` &lt;dbl&gt; 137853, 132629, 149906, 14239, 40530, 0, 1267, 471995… ## $ `7/16/21` &lt;dbl&gt; 139051, 132647, 151103, 14273, 40631, 0, 1268, 473721… ## $ `7/17/21` &lt;dbl&gt; 140224, 132665, 152210, 14273, 40707, 0, 1268, 474944… ## $ `7/18/21` &lt;dbl&gt; 140602, 132686, 153309, 14273, 40805, 0, 1268, 475637… ## $ `7/19/21` &lt;dbl&gt; 141499, 132697, 154486, 14359, 40906, 0, 1275, 476914… ## $ `7/20/21` &lt;dbl&gt; 142414, 132740, 155784, 14379, 41061, 0, 1275, 478421… ## $ `7/21/21` &lt;dbl&gt; 142762, 132763, 157005, 14379, 41227, 0, 1275, 479885… ## $ `7/22/21` &lt;dbl&gt; 143183, 132797, 158213, 14464, 41405, 0, 1277, 481235… ## $ `7/23/21` &lt;dbl&gt; 143439, 132828, 159563, 14498, 41629, 0, 1277, 482797… ## $ `7/24/21` &lt;dbl&gt; 143666, 132853, 160868, 14498, 41736, 0, 1280, 483910… ## $ `7/25/21` &lt;dbl&gt; 143871, 132875, 162155, 14498, 41780, 0, 1280, 484661… ## $ `7/26/21` &lt;dbl&gt; 144285, 132891, 163660, 14577, 41879, 0, 1280, 485917… ## $ `7/27/21` &lt;dbl&gt; 145008, 132922, 165204, 14586, 42110, 0, 1288, 487592… ## $ `7/28/21` &lt;dbl&gt; 145552, 132952, 167131, 14586, 42288, 0, 1288, 489181… ## $ `7/29/21` &lt;dbl&gt; 145996, 132999, 168668, 14655, 42486, 0, 1295, 490592… ## $ `7/30/21` &lt;dbl&gt; 146523, 133036, 170189, 14678, 42646, 0, 1303, 491940… ## $ `7/31/21` &lt;dbl&gt; 147154, 133081, 171392, 14678, 42777, 0, 1303, 492976… ## $ `8/1/21` &lt;dbl&gt; 147501, 133121, 172564, 14678, 42815, 0, 1303, 493584… ## $ `8/2/21` &lt;dbl&gt; 147985, 133146, 173922, 14747, 42970, 0, 1303, 494703… ## $ `8/3/21` &lt;dbl&gt; 148572, 133211, 175229, 14766, 43070, 0, 1303, 496188… ## $ `8/4/21` &lt;dbl&gt; 148933, 133310, 176724, 14797, 43158, 0, 1311, 497561… ## $ `8/5/21` &lt;dbl&gt; 149361, 133442, 178013, 14809, 43269, 0, 1320, 498940… ## $ `8/6/21` &lt;dbl&gt; 149810, 133591, 179216, 14836, 43487, 0, 1328, 500295… ## $ `8/7/21` &lt;dbl&gt; 150240, 133730, 180356, 14836, 43592, 0, 1328, 501275… ## $ `8/8/21` &lt;dbl&gt; 150458, 133912, 181376, 14836, 43662, 0, 1338, 501889… ## $ `8/9/21` &lt;dbl&gt; 150778, 133981, 182368, 14836, 43747, 0, 1348, 502907… ## $ `8/10/21` &lt;dbl&gt; 151013, 134201, 183347, 14873, 43890, 0, 1348, 504148… ## $ `8/11/21` &lt;dbl&gt; 151291, 134487, 184191, 14891, 43998, 0, 1372, 505288… ## $ `8/12/21` &lt;dbl&gt; 151563, 134761, 185042, 14908, 44174, 0, 1372, 506625… ## $ `8/13/21` &lt;dbl&gt; 151770, 135140, 185902, 14924, 44328, 0, 1372, 507472… ## $ `8/14/21` &lt;dbl&gt; 151941, 135550, 186655, 14924, 44534, 0, 1378, 508090… ## $ `8/15/21` &lt;dbl&gt; 152033, 135947, 187258, 14924, 44617, 0, 1397, 508463… ## $ `8/16/21` &lt;dbl&gt; 152142, 136147, 187968, 14954, 44739, 0, 1397, 508827… ## $ `8/17/21` &lt;dbl&gt; 152243, 136598, 188663, 14960, 44972, 0, 1397, 509644… ## $ `8/18/21` &lt;dbl&gt; 152363, 137075, 189384, 14976, 45175, 0, 1421, 510620… ## $ `8/19/21` &lt;dbl&gt; 152411, 137597, 190078, 14981, 45325, 0, 1421, 511680… ## $ `8/20/21` &lt;dbl&gt; 152448, 138132, 190656, 14988, 45583, 0, 1447, 512496… ## $ `8/21/21` &lt;dbl&gt; 152497, 138790, 191171, 14988, 45817, 0, 1490, 513085… ## $ `8/22/21` &lt;dbl&gt; 152511, 139324, 191583, 14988, 45945, 0, 1490, 513383… ## $ `8/23/21` &lt;dbl&gt; 152583, 139721, 192089, 15002, 46076, 0, 1540, 513996… ## $ `8/24/21` &lt;dbl&gt; 152660, 140521, 192626, 15003, 46340, 0, 1540, 514808… ## $ `8/25/21` &lt;dbl&gt; 152722, 141365, 193171, 15014, 46539, 0, 1598, 515507… ## $ `8/26/21` &lt;dbl&gt; 152822, 142253, 193674, 15016, 46726, 0, 1598, 516192… ## $ `8/27/21` &lt;dbl&gt; 152960, 143174, 194186, 15025, 46929, 0, 1598, 516773… ## $ `8/28/21` &lt;dbl&gt; 153007, 144079, 194671, 15025, 47079, 0, 1638, 517145… ## $ `8/29/21` &lt;dbl&gt; 153033, 144847, 195162, 15025, 47168, 0, 1651, 517353… ## $ `8/30/21` &lt;dbl&gt; 153148, 145333, 195574, 15032, 47331, 0, 1713, 517888… ## $ `8/31/21` &lt;dbl&gt; 153220, 146387, 196080, 15033, 47544, 0, 1715, 518562… ## $ `9/1/21` &lt;dbl&gt; 153260, 147369, 196527, 15046, 47781, 0, 1719, 519094… ## $ `9/2/21` &lt;dbl&gt; 153306, 148222, 196915, 15052, 48004, 0, 1742, 519560… ## $ `9/3/21` &lt;dbl&gt; 153375, 149117, 197308, 15055, 48261, 0, 1750, 519991… ## $ `9/4/21` &lt;dbl&gt; 153395, 150101, 197659, 15055, 48475, 0, 1759, 520240… ## $ `9/5/21` &lt;dbl&gt; 153423, 150997, 198004, 15055, 48656, 0, 1870, 520380… ## $ `9/6/21` &lt;dbl&gt; 153534, 151499, 198313, 15069, 48790, 0, 1878, 520769… ## $ `9/7/21` &lt;dbl&gt; 153626, 152239, 198645, 15070, 49114, 0, 1960, 521180… ## $ `9/8/21` &lt;dbl&gt; 153736, 153318, 198962, 15070, 49349, 0, 1974, 521533… ## $ `9/9/21` &lt;dbl&gt; 153840, 154316, 199275, 15078, 49628, 0, 2059, 521899… ## $ `9/10/21` &lt;dbl&gt; 153962, 155293, 199560, 15083, 49943, 0, 2059, 522180… ## $ `9/11/21` &lt;dbl&gt; 153982, 156162, 199822, 15083, 50348, 0, 2166, 522360… ## $ `9/12/21` &lt;dbl&gt; 153990, 157026, 200068, 15083, 50446, 0, 2166, 522453… ## $ `9/13/21` &lt;dbl&gt; 154094, 157436, 200301, 15096, 50738, 0, 2297, 522683… ## $ `9/14/21` &lt;dbl&gt; 154180, 158431, 200528, 15099, 51047, 0, 2304, 522984… ## $ `9/15/21` &lt;dbl&gt; 154283, 159423, 200770, 15108, 51407, 0, 2304, 523235… ## $ `9/16/21` &lt;dbl&gt; 154361, 160365, 200989, 15113, 51827, 0, 2304, 523485… ## $ `9/17/21` &lt;dbl&gt; 154487, 161324, 201224, 15124, 52208, 0, 2603, 523715… ## $ `9/18/21` &lt;dbl&gt; 154487, 162173, 201425, 15124, 52307, 0, 2603, 523861… ## $ `9/19/21` &lt;dbl&gt; 154487, 162953, 201600, 15124, 52307, 0, 2603, 523923… ## $ `9/20/21` &lt;dbl&gt; 154585, 163404, 201766, 15140, 52644, 0, 2603, 524139… ## $ `9/21/21` &lt;dbl&gt; 154712, 164276, 201948, 15140, 52968, 0, 2603, 524323… ## $ `9/22/21` &lt;dbl&gt; 154757, 165096, 202122, 15153, 53387, 0, 2625, 524526… ## $ `9/23/21` &lt;dbl&gt; 154800, 165864, 202283, 15156, 53840, 0, 2815, 524699… ## $ `9/24/21` &lt;dbl&gt; 154960, 166690, 202449, 15167, 54280, 0, 2815, 524884… ## $ `9/25/21` &lt;dbl&gt; 154960, 167354, 202574, 15167, 54795, 0, 2902, 524984… ## $ `9/26/21` &lt;dbl&gt; 154960, 167893, 202722, 15167, 55121, 0, 2923, 525040… ## $ `9/27/21` &lt;dbl&gt; 155072, 168188, 202877, 15189, 55583, 0, 2923, 525194… ## $ `9/28/21` &lt;dbl&gt; 155093, 168782, 203045, 15192, 56040, 0, 3160, 525376… ## $ `9/29/21` &lt;dbl&gt; 155128, 169462, 203198, 15209, 56583, 0, 3188, 525526… ## $ `9/30/21` &lt;dbl&gt; 155174, 170131, 203359, 15222, 56583, 0, 3231, 525690… ## $ `10/1/21` &lt;dbl&gt; 155191, 170778, 203517, 15222, 58076, 0, 3336, 525846… ## $ `10/2/21` &lt;dbl&gt; 155191, 171327, 203657, 15222, 58603, 0, 3403, 525935… ## $ `10/3/21` &lt;dbl&gt; 155191, 171794, 203789, 15222, 58943, 0, 3503, 525973… ## $ `10/4/21` &lt;dbl&gt; 155287, 171794, 203915, 15267, 58943, 0, 3503, 526071… ## $ `10/5/21` &lt;dbl&gt; 155309, 172618, 204046, 15271, 59895, 0, 3518, 526193… ## $ `10/6/21` &lt;dbl&gt; 155380, 173190, 204171, 15284, 60448, 0, 3581, 526321… ## $ `10/7/21` &lt;dbl&gt; 155429, 173723, 204276, 15288, 60803, 0, 3663, 526430… ## $ `10/8/21` &lt;dbl&gt; 155448, 174168, 204388, 15291, 61023, 0, 3678, 526505… ## $ `10/9/21` &lt;dbl&gt; 155466, 174643, 204490, 15291, 61245, 0, 3738, 526552… ## $ `10/10/21` &lt;dbl&gt; 155508, 174968, 204597, 15291, 61378, 0, 3750, 526585… ## $ `10/11/21` &lt;dbl&gt; 155540, 175163, 204695, 15307, 61580, 0, 3750, 526627… ## $ `10/12/21` &lt;dbl&gt; 155599, 175664, 204790, 15307, 61794, 0, 3772, 526733… ## $ `10/13/21` &lt;dbl&gt; 155627, 176172, 204900, 15314, 62143, 0, 3817, 526865… ## $ `10/14/21` &lt;dbl&gt; 155682, 176667, 205005, 15326, 62385, 0, 3830, 527000… ## $ `10/15/21` &lt;dbl&gt; 155688, 177108, 205106, 15338, 62606, 0, 3858, 527136… ## $ `10/16/21` &lt;dbl&gt; 155739, 177536, 205199, 15338, 62789, 0, 3888, 527215… ## $ `10/17/21` &lt;dbl&gt; 155764, 177971, 205286, 15338, 62842, 0, 3888, 527255… ## $ `10/18/21` &lt;dbl&gt; 155776, 178188, 205364, 15367, 63012, 0, 3918, 527346… ## $ `10/19/21` &lt;dbl&gt; 155801, 178804, 205453, 15369, 63197, 0, 3918, 527476… ## $ `10/20/21` &lt;dbl&gt; 155859, 179463, 205529, 15382, 63340, 0, 3939, 527598… ## $ `10/21/21` &lt;dbl&gt; 155891, 180029, 205599, 15382, 63567, 0, 3984, 527752… ## $ `10/22/21` &lt;dbl&gt; 155931, 180623, 205683, 15404, 63691, 0, 3994, 527891… ## $ `10/23/21` &lt;dbl&gt; 155940, 181252, 205750, 15404, 63775, 0, 4019, 527981… ## $ `10/24/21` &lt;dbl&gt; 155944, 181696, 205822, 15404, 63861, 0, 4019, 528035… ## $ `10/25/21` &lt;dbl&gt; 156040, 181960, 205903, 15425, 63930, 0, 4031, 528158… ## $ `10/26/21` &lt;dbl&gt; 156071, 182610, 205990, 15425, 64033, 0, 4031, 528300… ## $ `10/27/21` &lt;dbl&gt; 156124, 183282, 206069, 15462, 64126, 0, 4036, 528448… ## $ `10/28/21` &lt;dbl&gt; 156166, 183873, 206160, 15505, 64226, 0, 4040, 528607… ## $ `10/29/21` &lt;dbl&gt; 156196, 184340, 206270, 15516, 64301, 0, 4040, 528744… ## $ `10/30/21` &lt;dbl&gt; 156210, 184887, 206358, 15516, 64374, 0, 4058, 528825… ## $ `10/31/21` &lt;dbl&gt; 156250, 185300, 206452, 15516, 64433, 0, 4058, 528880… ## $ `11/1/21` &lt;dbl&gt; 156284, 185497, 206566, 15516, 64458, 0, 4062, 528994… ## $ `11/2/21` &lt;dbl&gt; 156307, 186222, 206649, 15516, 64487, 0, 4069, 529128… ## $ `11/3/21` &lt;dbl&gt; 156323, 186793, 206754, 15572, 64533, 0, 4069, 529254… ## $ `11/4/21` &lt;dbl&gt; 156363, 187363, 206878, 15618, 64583, 0, 4072, 529398… ## $ `11/5/21` &lt;dbl&gt; 156392, 187994, 206995, 15618, 64612, 0, 4078, 529526… ## $ `11/6/21` &lt;dbl&gt; 156397, 187994, 207079, 15618, 64654, 0, 4091, 529618… ## $ `11/7/21` &lt;dbl&gt; 156397, 189125, 207156, 15618, 64674, 0, 4091, 529678… ## $ `11/8/21` &lt;dbl&gt; 156397, 189355, 207254, 15705, 64724, 0, 4091, 529806… ## $ `11/9/21` &lt;dbl&gt; 156397, 190125, 207385, 15717, 64762, 0, 4091, 529941… ## $ `11/10/21` &lt;dbl&gt; 156414, 190815, 207509, 15744, 64815, 0, 4102, 530098… ## $ `11/11/21` &lt;dbl&gt; 156456, 191440, 207624, 15744, 64857, 0, 4102, 530244… ## $ `11/12/21` &lt;dbl&gt; 156487, 192013, 207764, 15819, 64875, 0, 4106, 530405… ## $ `11/13/21` &lt;dbl&gt; 156510, 192600, 207873, 15819, 64899, 0, 4118, 530515… ## $ `11/14/21` &lt;dbl&gt; 156552, 193075, 207970, 15819, 64913, 0, 4118, 530574… ## $ `11/15/21` &lt;dbl&gt; 156610, 193269, 208104, 15907, 64913, 0, 4118, 530715… ## $ `11/16/21` &lt;dbl&gt; 156649, 193856, 208245, 15929, 64940, 0, 4122, 530878… ## $ `11/17/21` &lt;dbl&gt; 156739, 194472, 208380, 15972, 64968, 0, 4129, 531033… ## $ `11/18/21` &lt;dbl&gt; 156739, 195021, 208532, 16035, 64985, 0, 4129, 531208… ## $ `11/19/21` &lt;dbl&gt; 156812, 195523, 208695, 16086, 64997, 0, 4131, 531360… ## $ `11/20/21` &lt;dbl&gt; 156864, 195988, 208839, 16086, 65011, 0, 4135, 531470… ## $ `11/21/21` &lt;dbl&gt; 156896, 195988, 208952, 16086, 65024, 0, 4135, 531534… ## $ `11/22/21` &lt;dbl&gt; 156911, 196611, 209111, 16299, 65033, 0, 4136, 531598… ## $ `11/23/21` &lt;dbl&gt; 157015, 197167, 209283, 16342, 65061, 0, 4138, 531763… ## $ `11/24/21` &lt;dbl&gt; 157032, 197776, 209463, 16426, 65080, 0, 4138, 531986… ## $ `11/25/21` &lt;dbl&gt; 157144, 198292, 209624, 16566, 65105, 0, 4141, 532212… ## $ `11/26/21` &lt;dbl&gt; 157171, 198732, 209817, 16712, 65130, 0, 4141, 532403… ## $ `11/27/21` &lt;dbl&gt; 157190, 199137, 209980, 16712, 65139, 0, 4141, 532556… ## $ `11/28/21` &lt;dbl&gt; 157218, 199555, 210152, 16712, 65144, 0, 4141, 532644… ## $ `11/29/21` &lt;dbl&gt; 157260, 199750, 210344, 16712, 65155, 0, 4141, 532841… ## $ `11/30/21` &lt;dbl&gt; 157289, 199945, 210531, 17115, 65168, 0, 4141, 533074… ## $ `12/1/21` &lt;dbl&gt; 157359, 200173, 210723, 17426, 65183, 0, 4141, 533262… ## $ `12/2/21` &lt;dbl&gt; 157387, 200639, 210921, 17658, 65208, 0, 4141, 533531… ## $ `12/3/21` &lt;dbl&gt; 157412, 201045, 211112, 18010, 65223, 0, 4146, 533769… ## $ `12/4/21` &lt;dbl&gt; 157431, 201402, 211297, 18010, 65244, 0, 4147, 533938… ## $ `12/5/21` &lt;dbl&gt; 157454, 201730, 211469, 18010, 65259, 0, 4147, 534067… ## $ `12/6/21` &lt;dbl&gt; 157499, 201902, 211662, 18631, 65259, 0, 4148, 534315… ## $ `12/7/21` &lt;dbl&gt; 157508, 202295, 211859, 18815, 65301, 0, 4148, 534624… ## $ `12/8/21` &lt;dbl&gt; 157542, 202641, 212047, 18815, 65332, 0, 4151, 534812… ## $ `12/9/21` &lt;dbl&gt; 157585, 202863, 212224, 19272, 65346, 0, 4151, 535086… ## $ `12/10/21` &lt;dbl&gt; 157603, 203215, 212434, 19440, 65371, 0, 4159, 535444… ## $ `12/11/21` &lt;dbl&gt; 157611, 203524, 212652, 19440, 65397, 0, 4159, 535688… ## $ `12/12/21` &lt;dbl&gt; 157633, 203787, 212848, 19440, 65404, 0, 4162, 535845… ## $ `12/13/21` &lt;dbl&gt; 157648, 203925, 213058, 19440, 65404, 0, 4162, 536196… ## $ `12/14/21` &lt;dbl&gt; 157660, 204301, 213288, 20136, 65431, 11, 4177, 53665… ## $ `12/15/21` &lt;dbl&gt; 157665, 204627, 213533, 20136, 65565, 11, 4177, 53713… ## $ `12/16/21` &lt;dbl&gt; 157725, 204928, 213745, 20549, 65648, 11, 4178, 53766… ## $ `12/17/21` &lt;dbl&gt; 157734, 205224, 214044, 20549, 65760, 11, 4186, 53822… ## $ `12/18/21` &lt;dbl&gt; 157745, 205549, 214330, 20549, 65868, 11, 4198, 53864… ## $ `12/19/21` &lt;dbl&gt; 157787, 205777, 214592, 20549, 65938, 11, 4198, 53897… ## $ `12/20/21` &lt;dbl&gt; 157797, 205897, 214835, 21062, 66086, 11, 4198, 53950… ## $ `12/21/21` &lt;dbl&gt; 157816, 206273, 215145, 21062, 66566, 11, 4201, 54043… ## $ `12/22/21` &lt;dbl&gt; 157841, 206616, 215430, 21372, 67199, 11, 4205, 54155… ## $ `12/23/21` &lt;dbl&gt; 157878, 206935, 215723, 21571, 68362, 11, 4216, 54289… ## $ `12/24/21` &lt;dbl&gt; 157887, 207221, 216098, 21730, 70221, 11, 4216, 54452… ## $ `12/25/21` &lt;dbl&gt; 157895, 207542, 216376, 21730, 71142, 11, 4236, 54524… ## $ `12/26/21` &lt;dbl&gt; 157951, 207709, 216637, 21730, 71752, 11, 4236, 54600… ## $ `12/27/21` &lt;dbl&gt; 157967, 207709, 216930, 22332, 71752, 11, 4259, 54803… ## $ `12/28/21` &lt;dbl&gt; 157998, 208352, 217265, 22540, 76787, 11, 4259, 55142… ## $ `12/29/21` &lt;dbl&gt; 158037, 208899, 217647, 22823, 78475, 11, 4259, 55562… ## $ `12/30/21` &lt;dbl&gt; 158056, 208899, 218037, 23122, 79871, 11, 4283, 56067… ## $ `12/31/21` &lt;dbl&gt; 158084, 210224, 218432, 23740, 81593, 11, 4283, 56544… ## $ `1/1/22` &lt;dbl&gt; 158107, 210224, 218818, 23740, 82398, 11, 4283, 56744… ## $ `1/2/22` &lt;dbl&gt; 158189, 210885, 219159, 23740, 82920, 11, 4283, 56949… ## $ `1/3/22` &lt;dbl&gt; 158183, 210885, 219532, 24502, 83764, 11, 4283, 57393… ## $ `1/4/22` &lt;dbl&gt; 158205, 212021, 219953, 24802, 84666, 11, 4486, 58205… ## $ `1/5/22` &lt;dbl&gt; 158245, 212021, 220415, 25289, 86636, 11, 4486, 59156… ## $ `1/6/22` &lt;dbl&gt; 158275, 213257, 220825, 25289, 87625, 11, 4715, 60253… ## $ `1/7/22` &lt;dbl&gt; 158300, 214905, 221316, 26408, 88775, 11, 4715, 61358… ## $ `1/8/22` &lt;dbl&gt; 158309, 214905, 221742, 26408, 89251, 11, 4844, 62375… ## $ `1/9/22` &lt;dbl&gt; 158381, 219694, 222157, 26408, 89718, 11, 5058, 63108… ## $ `1/10/22` &lt;dbl&gt; 158394, 220487, 222639, 27983, 90316, 11, 5058, 63991… ## $ `1/11/22` &lt;dbl&gt; 158471, 222664, 223196, 28542, 91148, 11, 5058, 65336… ## $ `1/12/22` &lt;dbl&gt; 158511, 224569, 223806, 28899, 91907, 11, 5214, 66647… ## $ `1/13/22` &lt;dbl&gt; 158602, 226598, 224383, 28899, 92581, 11, 5214, 67931… ## $ `1/14/22` &lt;dbl&gt; 158639, 228777, 224979, 29888, 93302, 11, 5246, 69329… ## $ `1/15/22` &lt;dbl&gt; 158678, 230940, 225484, 29888, 93524, 11, 5321, 70296… ## $ `1/16/22` &lt;dbl&gt; 158717, 232637, 226057, 29888, 93694, 11, 5321, 70948… ## $ `1/17/22` &lt;dbl&gt; 158826, 233654, 226749, 29888, 93974, 11, 5321, 71973… ## $ `1/18/22` &lt;dbl&gt; 158974, 236486, 227559, 29888, 94275, 11, 5346, 73183… ## $ `1/19/22` &lt;dbl&gt; 159070, 239129, 228918, 29888, 94779, 11, 5741, 74466… ## $ `1/20/22` &lt;dbl&gt; 159303, 241512, 230470, 32201, 95220, 11, 5741, 75763… ## $ `1/21/22` &lt;dbl&gt; 159516, 244182, 232325, 33025, 95676, 11, 5815, 76945… ## $ `1/22/22` &lt;dbl&gt; 159548, 246412, 234536, 33025, 95902, 11, 5931, 77926… ## $ `1/23/22` &lt;dbl&gt; 159649, 248070, 236670, 33025, 96582, 11, 5931, 78625… ## $ `1/24/22` &lt;dbl&gt; 159896, 248070, 238885, 33025, 97263, 11, 6023, 79406… ## $ `1/25/22` &lt;dbl&gt; 160252, 248859, 241406, 34701, 97594, 11, 6023, 80415… ## $ `1/26/22` &lt;dbl&gt; 160692, 251015, 243568, 35028, 97812, 11, 6442, 81300… ## $ `1/27/22` &lt;dbl&gt; 161004, 252577, 245698, 35028, 97901, 11, 6524, 82077… ## $ `1/28/22` &lt;dbl&gt; 161057, 254126, 247568, 35556, 98029, 11, 6558, 82716… ## $ `1/29/22` &lt;dbl&gt; 161290, 254126, 249310, 35556, 98057, 11, 6558, 83136… ## $ `1/30/22` &lt;dbl&gt; 162111, 255741, 250774, 35556, 98076, 11, 6558, 83351… ## $ `1/31/22` &lt;dbl&gt; 162926, 258543, 252117, 35958, 98116, 11, 6627, 83786… ## $ `2/1/22` &lt;dbl&gt; 163555, 258543, 253520, 35958, 98226, 11, 6627, 84277… ## $ `2/2/22` &lt;dbl&gt; 164190, 261240, 254885, 36315, 98267, 11, 6732, 84728… ## $ `2/3/22` &lt;dbl&gt; 164727, 261240, 255836, 36470, 98319, 11, 6732, 85152… ## $ `2/4/22` &lt;dbl&gt; 165358, 263172, 256806, 36599, 98340, 11, 6732, 85553… ## $ `2/5/22` &lt;dbl&gt; 165711, 263172, 257598, 36599, 98351, 11, 6732, 85772… ## $ `2/6/22` &lt;dbl&gt; 166191, 264624, 257976, 36599, 98364, 11, 6853, 85898… ## $ `2/7/22` &lt;dbl&gt; 166924, 264875, 258478, 36808, 98409, 11, 6853, 86152… ## $ `2/8/22` &lt;dbl&gt; 167739, 265716, 259088, 36808, 98424, 11, 6853, 86480… ## $ `2/9/22` &lt;dbl&gt; 168550, 266416, 259673, 36989, 98453, 11, 6853, 86753… ## $ `2/10/22` &lt;dbl&gt; 169448, 267020, 260191, 37074, 98474, 11, 7321, 87004… ## $ `2/11/22` &lt;dbl&gt; 169940, 267020, 260723, 37140, 98501, 11, 7331, 87169… ## $ `2/12/22` &lt;dbl&gt; 170152, 267551, 261226, 37140, 98514, 11, 7331, 87282… ## $ `2/13/22` &lt;dbl&gt; 170604, 268008, 261752, 37140, 98514, 11, 7331, 87345… ## $ `2/14/22` &lt;dbl&gt; 171246, 268304, 262165, 37277, 98514, 11, 7331, 87476… ## $ `2/15/22` &lt;dbl&gt; 171422, 268491, 262570, 37361, 98555, 11, 7342, 87661… ## $ `2/16/22` &lt;dbl&gt; 171519, 268940, 262994, 37452, 98568, 11, 7395, 87832… ## $ `2/17/22` &lt;dbl&gt; 171673, 269301, 263369, 37522, 98585, 11, 7395, 87998… ## $ `2/18/22` &lt;dbl&gt; 171857, 269601, 263685, 37589, 98605, 11, 7400, 88152… ## $ `2/19/22` &lt;dbl&gt; 171931, 269904, 263936, 37589, 98617, 11, 7408, 88230… ## $ `2/20/22` &lt;dbl&gt; 172205, 270164, 264054, 37589, 98638, 11, 7408, 88275… ## $ `2/21/22` &lt;dbl&gt; 172441, 270370, 264201, 37589, 98658, 11, 7408, 88386… ## $ `2/22/22` &lt;dbl&gt; 172716, 270455, 264365, 37820, 98671, 11, 7408, 88556… ## $ `2/23/22` &lt;dbl&gt; 172901, 270734, 264488, 37901, 98698, 11, 7429, 88681… ## $ `2/24/22` &lt;dbl&gt; 173047, 270947, 264603, 37958, 98701, 11, 7435, 88784… ## $ `2/25/22` &lt;dbl&gt; 173084, 271141, 264706, 37999, 98701, 11, 7437, 88879… ## $ `2/26/22` &lt;dbl&gt; 173146, 271141, 264778, 37999, 98701, 11, 7437, 88935… ## $ `2/27/22` &lt;dbl&gt; 173395, 271527, 264855, 37999, 98701, 11, 7437, 88971… ## $ `2/28/22` &lt;dbl&gt; 173659, 271563, 264936, 37999, 98741, 11, 7437, 89006… ## $ `3/1/22` &lt;dbl&gt; 173879, 271702, 265010, 38165, 98746, 11, 7447, 89041… ## $ `3/2/22` &lt;dbl&gt; 174073, 271825, 265079, 38249, 98746, 11, 7449, 89123… ## $ `3/3/22` &lt;dbl&gt; 174214, 271825, 265130, 38342, 98746, 11, 7449, 89215… ## $ `3/4/22` &lt;dbl&gt; 174214, 272030, 265186, 38434, 98796, 11, 7449, 89298… ## $ `3/5/22` &lt;dbl&gt; 174331, 272030, 265227, 38434, 98796, 11, 7455, 89343… ## $ `3/6/22` &lt;dbl&gt; 174582, 272210, 265265, 38434, 98806, 11, 7455, 89366… ## $ `3/7/22` &lt;dbl&gt; 175000, 272250, 265297, 38620, 98806, 11, 7455, 89428… ## $ `3/8/22` &lt;dbl&gt; 175353, 272337, 265323, 38710, 98829, 11, 7455, 89493… ## $ `3/9/22` &lt;dbl&gt; 175525, 272412, 265346, 38794, 98855, 11, 7461, 89554… ## $ `3/10/22` &lt;dbl&gt; 175893, 272479, 265366, 38794, 98855, 11, 7461, 89615… ## $ `3/11/22` &lt;dbl&gt; 175974, 272552, 265391, 38794, 98855, 11, 7466, 89672… ## $ `3/12/22` &lt;dbl&gt; 176039, 272621, 265410, 38794, 98909, 11, 7466, 89701… ## $ `3/13/22` &lt;dbl&gt; 176201, 272663, 265432, 38794, 98927, 11, 7466, 89714… ## $ `3/14/22` &lt;dbl&gt; 176409, 272689, 265457, 38794, 98931, 11, 7466, 89760… ## $ `3/15/22` &lt;dbl&gt; 176571, 272711, 265478, 38794, 98956, 11, 7470, 89811… ## $ `3/16/22` &lt;dbl&gt; 176743, 272804, 265496, 38794, 98985, 11, 7470, 89858… ## $ `3/17/22` &lt;dbl&gt; 176918, 272885, 265511, 39234, 99003, 11, 7470, 89904… ## $ `3/18/22` &lt;dbl&gt; 176983, 272961, 265524, 39234, 99003, 11, 7470, 90048… ## $ `3/19/22` &lt;dbl&gt; 177039, 273040, 265539, 39234, 99003, 11, 7473, 90065… ## $ `3/20/22` &lt;dbl&gt; 177093, 273088, 265550, 39234, 99003, 11, 7473, 90077… ## $ `3/21/22` &lt;dbl&gt; 177191, 273088, 265562, 39234, 99010, 11, 7473, 90113… ## $ `3/22/22` &lt;dbl&gt; 177255, 273146, 265573, 39234, 99058, 11, 7473, 90160… ## $ `3/23/22` &lt;dbl&gt; 177321, 273164, 265585, 39713, 99058, 11, 7482, 90196… ## $ `3/24/22` &lt;dbl&gt; 177321, 273257, 265599, 39713, 99081, 11, 7482, 90212… ## $ `3/25/22` &lt;dbl&gt; 177321, 273318, 265612, 39713, 99102, 11, 7482, 90238… ## $ `3/26/22` &lt;dbl&gt; 177321, 273387, 265621, 39713, 99106, 11, 7482, 90252… ## $ `3/27/22` &lt;dbl&gt; 177520, 273432, 265629, 39713, 99115, 11, 7485, 90260… ## $ `3/28/22` &lt;dbl&gt; 177602, 273432, 265641, 39713, 99115, 11, 7491, 90287… ## $ `3/29/22` &lt;dbl&gt; 177658, 273529, 265651, 39713, 99138, 11, 7491, 90321… ## $ `3/30/22` &lt;dbl&gt; 177716, 273608, 265662, 40024, 99138, 11, 7491, 90351… ## $ `3/31/22` &lt;dbl&gt; 177747, 273677, 265671, 40024, 99169, 11, 7491, 90379… ## $ `4/1/22` &lt;dbl&gt; 177782, 273759, 265679, 40024, 99194, 11, 7493, 90398… ## $ `4/2/22` &lt;dbl&gt; 177803, 273823, 265684, 40024, 99194, 11, 7493, 90406… ## $ `4/3/22` &lt;dbl&gt; 177827, 273870, 265691, 40024, 99194, 11, 7493, 90411… ## $ `4/4/22` &lt;dbl&gt; 177897, 273913, 265694, 40024, 99194, 11, 7493, 90430… ## $ `4/5/22` &lt;dbl&gt; 177932, 274000, 265699, 40024, 99194, 11, 7493, 90453… ## $ `4/6/22` &lt;dbl&gt; 177974, 274055, 265705, 40024, 99194, 11, 7493, 90474… ## $ `4/7/22` &lt;dbl&gt; 177974, 274108, 265707, 40328, 99194, 11, 7511, 90492… ## $ `4/8/22` &lt;dbl&gt; 177974, 274136, 265714, 40328, 99194, 11, 7511, 90512… ## $ `4/9/22` &lt;dbl&gt; 177974, 274191, 265720, 40328, 99194, 11, 7511, 90520… ## $ `4/10/22` &lt;dbl&gt; 177974, 274219, 265724, 40328, 99194, 11, 7511, 90525… ## $ `4/11/22` &lt;dbl&gt; 178141, 274219, 265727, 40328, 99194, 11, 7511, 90541… ## $ `4/12/22` &lt;dbl&gt; 178257, 274272, 265730, 40328, 99194, 11, 7523, 90562… ## $ `4/13/22` &lt;dbl&gt; 178295, 274320, 265731, 40709, 99194, 11, 7523, 90579… ## $ `4/14/22` &lt;dbl&gt; 178352, 274376, 265733, 40709, 99194, 11, 7535, 90593… ## $ `4/15/22` &lt;dbl&gt; 178373, 274429, 265738, 40709, 99194, 11, 7535, 90599… ## $ `4/16/22` &lt;dbl&gt; 178387, 274462, 265739, 40709, 99194, 11, 7535, 90604… ## $ `4/17/22` &lt;dbl&gt; 178418, 274504, 265739, 40709, 99194, 11, 7539, 90609… ## $ `4/18/22` &lt;dbl&gt; 178457, 274520, 265741, 40709, 99194, 11, 7539, 90609… ## $ `4/19/22` &lt;dbl&gt; 178513, 274535, 265746, 40709, 99287, 11, 7539, 90609… ## $ `4/20/22` &lt;dbl&gt; 178574, 274606, 265746, 41013, 99287, 11, 7567, 90609… ## $ `4/21/22` &lt;dbl&gt; 178611, 274606, 265754, 41013, 99287, 11, 7567, 90609… ## $ `4/22/22` &lt;dbl&gt; 178638, 274737, 265761, 41013, 99287, 11, 7571, 90609… ## $ `4/23/22` &lt;dbl&gt; 178648, 274791, 265761, 41013, 99287, 11, 7571, 90609… ## $ `4/24/22` &lt;dbl&gt; 178689, 274828, 265767, 41013, 99287, 11, 7571, 90609… ## $ `4/25/22` &lt;dbl&gt; 178745, 274828, 265771, 41013, 99287, 11, 7571, 90722… ## $ `4/26/22` &lt;dbl&gt; 178769, 274862, 265772, 41013, 99287, 11, 7571, 90722… ## $ `4/27/22` &lt;dbl&gt; 178809, 274929, 265773, 41013, 99287, 11, 7571, 90722… ## $ `4/28/22` &lt;dbl&gt; 178850, 275002, 265776, 41349, 99287, 11, 7604, 90722… ## $ `4/29/22` &lt;dbl&gt; 178873, 275055, 265779, 41349, 99287, 11, 7626, 90722… ## $ `4/30/22` &lt;dbl&gt; 178879, 275107, 265780, 41349, 99287, 11, 7626, 90722… ## $ `5/1/22` &lt;dbl&gt; 178899, 275167, 265782, 41349, 99287, 11, 7626, 90836… ## $ `5/2/22` &lt;dbl&gt; 178901, 275177, 265782, 41349, 99287, 11, 7626, 90836… ## $ `5/3/22` &lt;dbl&gt; 178901, 275191, 265782, 41349, 99287, 11, 7626, 90836… ## $ `5/4/22` &lt;dbl&gt; 178901, 275211, 265782, 41717, 99287, 11, 7654, 90836… ## $ `5/5/22` &lt;dbl&gt; 178905, 275266, 265786, 41717, 99287, 11, 7663, 90836… ## $ `5/6/22` &lt;dbl&gt; 178919, 275310, 265791, 41717, 99287, 11, 7663, 90836… ## $ `5/7/22` &lt;dbl&gt; 178922, 275341, 265794, 41717, 99287, 11, 7663, 90836… ## $ `5/8/22` &lt;dbl&gt; 178981, 275366, 265798, 41717, 99287, 11, 7663, 91013… ## $ `5/9/22` &lt;dbl&gt; 179010, 275372, 265800, 41717, 99287, 11, 7663, 91013… ## $ `5/10/22` &lt;dbl&gt; 179017, 275416, 265804, 41717, 99287, 11, 7663, 91013… ## $ `5/11/22` &lt;dbl&gt; 179131, 275440, 265806, 41717, 99287, 11, 7721, 91013… ## $ `5/12/22` &lt;dbl&gt; 179169, 275485, 265808, 42156, 99287, 11, 7721, 91013… ## $ `5/13/22` &lt;dbl&gt; 179203, 275534, 265814, 42156, 99287, 11, 7721, 91013… ## $ `5/14/22` &lt;dbl&gt; 179242, 275574, 265816, 42156, 99287, 11, 7721, 91013… ## $ `5/15/22` &lt;dbl&gt; 179267, 275615, 265818, 42156, 99287, 11, 7721, 91353… ## $ `5/16/22` &lt;dbl&gt; 179321, 275621, 265823, 42156, 99287, 11, 7795, 91353… ## $ `5/17/22` &lt;dbl&gt; 179328, 275688, 265828, 42156, 99287, 11, 7795, 91353… ## $ `5/18/22` &lt;dbl&gt; 179477, 275732, 265834, 42572, 99287, 11, 7855, 91353… ## $ `5/19/22` &lt;dbl&gt; 179597, 275732, 265841, 42572, 99287, 11, 7910, 91353… ## $ `5/20/22` &lt;dbl&gt; 179624, 275732, 265847, 42572, 99287, 11, 7910, 91353… ## $ `5/21/22` &lt;dbl&gt; 179674, 275838, 265851, 42572, 99287, 11, 7942, 91353… ## $ `5/22/22` &lt;dbl&gt; 179716, 275864, 265854, 42572, 99287, 11, 7942, 91787… ## $ `5/23/22` &lt;dbl&gt; 179716, 275881, 265855, 42572, 99287, 11, 7982, 91787… ## $ `5/24/22` &lt;dbl&gt; 179771, 275939, 265860, 42572, 99433, 11, 7982, 91787… ## $ `5/25/22` &lt;dbl&gt; 179835, 275985, 265862, 42894, 99527, 11, 8062, 91787… ## $ `5/26/22` &lt;dbl&gt; 179835, 276012, 265864, 42894, 99527, 11, 8119, 91787… ## $ `5/27/22` &lt;dbl&gt; 180086, 276048, 265870, 42894, 99527, 11, 8119, 91787… ## $ `5/28/22` &lt;dbl&gt; 180122, 276081, 265873, 42894, 99527, 11, 8119, 91787… ## $ `5/29/22` &lt;dbl&gt; 180174, 276101, 265873, 42894, 99527, 11, 8119, 92305… ## $ `5/30/22` &lt;dbl&gt; 180259, 276101, 265877, 42894, 99761, 11, 8163, 92305… ## $ `5/31/22` &lt;dbl&gt; 180347, 276101, 265884, 42894, 99761, 11, 8253, 92305… ## $ `6/1/22` &lt;dbl&gt; 180419, 276221, 265887, 42894, 99761, 11, 8253, 92305… ## $ `6/2/22` &lt;dbl&gt; 180520, 276221, 265889, 42894, 99761, 11, 8295, 92305… ## $ `6/3/22` &lt;dbl&gt; 180584, 276310, 265889, 43067, 99761, 11, 8295, 92305… ## $ `6/4/22` &lt;dbl&gt; 180615, 276342, 265889, 43067, 99761, 11, 8378, 92305… ## $ `6/5/22` &lt;dbl&gt; 180615, 276401, 265897, 43067, 99761, 11, 8378, 92766… ## $ `6/6/22` &lt;dbl&gt; 180688, 276415, 265900, 43067, 99761, 11, 8378, 92766… ## $ `6/7/22` &lt;dbl&gt; 180741, 276468, 265904, 43067, 99761, 11, 8378, 92766… ## $ `6/8/22` &lt;dbl&gt; 180784, 276518, 265909, 43224, 99761, 11, 8378, 92766… ## $ `6/9/22` &lt;dbl&gt; 180864, 276583, 265920, 43224, 99761, 11, 8406, 92766… ## $ `6/10/22` &lt;dbl&gt; 180864, 276638, 265925, 43224, 99761, 11, 8479, 92766… ## $ `6/11/22` &lt;dbl&gt; 180864, 276690, 265925, 43224, 99761, 11, 8479, 92766… ## $ `6/12/22` &lt;dbl&gt; 180864, 276731, 265927, 43224, 99761, 11, 8492, 92766… ## $ `6/13/22` &lt;dbl&gt; 181120, 276731, 265937, 43224, 99761, 11, 8531, 92766… ## $ `6/14/22` &lt;dbl&gt; 181178, 276821, 265943, 43224, 99761, 11, 8537, 93134… ## $ `6/15/22` &lt;dbl&gt; 181236, 276821, 265952, 43449, 99761, 11, 8537, 93134… ## $ `6/16/22` &lt;dbl&gt; 181465, 276821, 265964, 43449, 99761, 11, 8537, 93134… ## $ `6/17/22` &lt;dbl&gt; 181534, 277141, 265968, 43449, 99761, 11, 8555, 93134… ## $ `6/18/22` &lt;dbl&gt; 181574, 277141, 265971, 43449, 99761, 11, 8581, 93134… ## $ `6/19/22` &lt;dbl&gt; 181666, 277409, 265975, 43449, 99761, 11, 8581, 93414… ## $ `6/20/22` &lt;dbl&gt; 181725, 277444, 265985, 43449, 99761, 11, 8581, 93414… ## $ `6/21/22` &lt;dbl&gt; 181808, 277663, 265993, 43449, 99761, 11, 8581, 93414… ## $ `6/22/22` &lt;dbl&gt; 181912, 277940, 266006, 43774, 99761, 11, 8590, 93414… ## $ `6/23/22` &lt;dbl&gt; 181987, 278211, 266015, 43774, 99761, 11, 8590, 93414… ## $ `6/24/22` &lt;dbl&gt; 182033, 278504, 266025, 43774, 99761, 11, 8625, 93414… ## $ `6/25/22` &lt;dbl&gt; 182072, 278793, 266030, 43774, 99761, 11, 8625, 93414… ## $ `6/26/22` &lt;dbl&gt; 182149, 279077, 266038, 43774, 99761, 11, 8625, 93671… ## $ `6/27/22` &lt;dbl&gt; 182228, 279077, 266049, 43774, 99761, 11, 8625, 93671… ## $ `6/28/22` &lt;dbl&gt; 182324, 279167, 266062, 43774, 101320, 11, 8625, 9367… ## $ `6/29/22` &lt;dbl&gt; 182403, 280298, 266073, 43774, 101320, 11, 8625, 9367… ## $ `6/30/22` &lt;dbl&gt; 182528, 280851, 266087, 43774, 101320, 11, 8641, 9367… ## $ `7/1/22` &lt;dbl&gt; 182594, 281470, 266105, 44177, 101320, 11, 8656, 9367… ## $ `7/2/22` &lt;dbl&gt; 182643, 282141, 266115, 44177, 101320, 11, 8665, 9367… ## $ `7/3/22` &lt;dbl&gt; 182724, 282690, 266128, 44177, 101320, 11, 8665, 9394… ## $ `7/4/22` &lt;dbl&gt; 182793, 282690, 266173, 44177, 101320, 11, 8665, 9394… ## $ `7/5/22` &lt;dbl&gt; 182793, 282690, 266173, 44177, 101320, 11, 8665, 9394… ## $ `7/6/22` &lt;dbl&gt; 182979, 283811, 266181, 44671, 101320, 11, 8668, 9394… ## $ `7/7/22` &lt;dbl&gt; 183084, 284758, 266202, 44671, 101320, 11, 8681, 9394… ## $ `7/8/22` &lt;dbl&gt; 183221, 285731, 266228, 44671, 101320, 11, 8686, 9394… ## $ `7/9/22` &lt;dbl&gt; 183235, 286732, 266246, 44671, 101320, 11, 8686, 9394… ## $ `7/10/22` &lt;dbl&gt; 183265, 287984, 266257, 44671, 101320, 11, 8686, 9426… ## $ `7/11/22` &lt;dbl&gt; 183268, 288176, 266274, 44671, 101320, 11, 8686, 9426… ## $ `7/12/22` &lt;dbl&gt; 183272, 289391, 266303, 44671, 101320, 11, 8686, 9426… ## $ `7/13/22` &lt;dbl&gt; 183285, 290954, 266328, 44671, 101320, 11, 8686, 9426… ## $ `7/14/22` &lt;dbl&gt; 183358, 290954, 266356, 44671, 101600, 11, 8704, 9426… ## $ `7/15/22` &lt;dbl&gt; 183407, 293917, 266392, 44671, 101901, 11, 8704, 9426… ## $ `7/16/22` &lt;dbl&gt; 183445, 295243, 266424, 44671, 101901, 11, 8712, 9426… ## $ `7/17/22` &lt;dbl&gt; 183572, 296305, 266445, 44671, 101901, 11, 8712, 9465… ## $ `7/18/22` &lt;dbl&gt; 183687, 296732, 266487, 45061, 102209, 11, 8712, 9465… ## $ `7/19/22` &lt;dbl&gt; 183908, 298578, 266542, 45061, 102209, 11, 8712, 9465… ## $ `7/20/22` &lt;dbl&gt; 184038, 300058, 266591, 45061, 102209, 11, 8712, 9465… ## $ `7/21/22` &lt;dbl&gt; 184224, 301394, 266654, 45326, 102209, 11, 8712, 9465… ## $ `7/22/22` &lt;dbl&gt; 184360, 302767, 266700, 45326, 102301, 11, 8736, 9465… ## $ `7/23/22` &lt;dbl&gt; 184473, 303925, 266772, 45326, 102301, 11, 8736, 9465… ## $ `7/24/22` &lt;dbl&gt; 184587, 304890, 266839, 45326, 102301, 11, 8736, 9507… ## $ `7/25/22` &lt;dbl&gt; 184819, 305123, 266916, 45326, 102301, 11, 8736, 9507… ## $ `7/26/22` &lt;dbl&gt; 185086, 306789, 267010, 45326, 102301, 11, 8741, 9507… ## $ `7/27/22` &lt;dbl&gt; 185272, 308050, 267096, 45326, 102301, 11, 8741, 9507… ## $ `7/28/22` &lt;dbl&gt; 185393, 309278, 267194, 45508, 102301, 11, 8741, 9507… ## $ `7/29/22` &lt;dbl&gt; 185481, 310362, 267287, 45508, 102301, 11, 8741, 9507… ## $ `7/30/22` &lt;dbl&gt; 185552, 311381, 267374, 45508, 102301, 11, 8773, 9507… ## $ `7/31/22` &lt;dbl&gt; 185749, 312097, 267454, 45508, 102301, 11, 8773, 9560… ## $ `8/1/22` &lt;dbl&gt; 185930, 312375, 267546, 45508, 102301, 11, 8773, 9560… ## $ `8/2/22` &lt;dbl&gt; 186120, 313582, 267657, 45508, 102301, 11, 8773, 9560… ## $ `8/3/22` &lt;dbl&gt; 186393, 314561, 267777, 45793, 102301, 11, 8773, 9560… ## $ `8/4/22` &lt;dbl&gt; 186697, 315337, 267902, 45793, 102301, 11, 8773, 9560… ## $ `8/5/22` &lt;dbl&gt; 187037, 316145, 268033, 45793, 102636, 11, 8773, 9560… ## $ `8/6/22` &lt;dbl&gt; 187109, 316976, 268141, 45793, 102636, 11, 8773, 9560… ## $ `8/7/22` &lt;dbl&gt; 187442, 317514, 268254, 45793, 102636, 11, 8787, 9602… ## $ `8/8/22` &lt;dbl&gt; 187685, 317681, 268356, 45793, 102636, 11, 8809, 9602… ## $ `8/9/22` &lt;dbl&gt; 187966, 318638, 268478, 45793, 102636, 11, 8809, 9602… ## $ `8/10/22` &lt;dbl&gt; 188202, 319444, 268584, 45899, 102636, 11, 8809, 9602… ## $ `8/11/22` &lt;dbl&gt; 188506, 320086, 268718, 45899, 102636, 11, 8809, 9602… ## $ `8/12/22` &lt;dbl&gt; 188704, 320781, 268866, 45899, 102636, 11, 8820, 9602… ## $ `8/13/22` &lt;dbl&gt; 188820, 321345, 269008, 45899, 102636, 11, 8820, 9602… ## $ `8/14/22` &lt;dbl&gt; 189045, 321804, 269141, 45899, 102636, 11, 8820, 9633… ## $ `8/15/22` &lt;dbl&gt; 189343, 322125, 269269, 45899, 102636, 11, 8851, 9633… ## $ `8/16/22` &lt;dbl&gt; 189477, 322837, 269381, 45899, 102636, 11, 8851, 9633… ## $ `8/17/22` &lt;dbl&gt; 189710, 323282, 269473, 45975, 102636, 11, 8851, 9633… ## $ `8/18/22` &lt;dbl&gt; 190010, 323829, 269556, 45975, 102636, 11, 8895, 9633… ## $ `8/19/22` &lt;dbl&gt; 190254, 325241, 269650, 45975, 102636, 11, 8895, 9633… ## $ `8/20/22` &lt;dbl&gt; 190435, 325736, 269731, 45975, 102636, 11, 8895, 9633… ## $ `8/21/22` &lt;dbl&gt; 190643, 326077, 269805, 45975, 102636, 11, 8895, 9658… ## $ `8/22/22` &lt;dbl&gt; 191040, 326181, 269894, 45975, 102636, 11, 8895, 9658… ## $ `8/23/22` &lt;dbl&gt; 191247, 326787, 269971, 45975, 102636, 11, 8895, 9658… ## $ `8/24/22` &lt;dbl&gt; 191585, 327232, 270043, 46027, 102636, 11, 8949, 9658… ## $ `8/25/22` &lt;dbl&gt; 191967, 327607, 270097, 46027, 102636, 11, 8949, 9658… ## $ `8/26/22` &lt;dbl&gt; 191967, 327961, 270145, 46027, 102636, 11, 8949, 9658… ## $ `8/27/22` &lt;dbl&gt; 191967, 328299, 270175, 46027, 102636, 11, 8949, 9658… ## $ `8/28/22` &lt;dbl&gt; 192463, 328515, 270194, 46027, 102636, 11, 8949, 9678… ## $ `8/29/22` &lt;dbl&gt; 192906, 328571, 270235, 46027, 102636, 11, 8949, 9678… ## $ `8/30/22` &lt;dbl&gt; 193004, 329017, 270272, 46027, 102636, 11, 8974, 9678… ## $ `8/31/22` &lt;dbl&gt; 193250, 329352, 270304, 46027, 102636, 11, 8974, 9678… ## $ `9/1/22` &lt;dbl&gt; 193520, 329615, 270359, 46027, 102636, 11, 8974, 9678… ## $ `9/2/22` &lt;dbl&gt; 193520, 329862, 270405, 46027, 102636, 11, 8974, 9678… ## $ `9/3/22` &lt;dbl&gt; 193912, 330062, 270426, 46027, 102636, 11, 8974, 9678… ## $ `9/4/22` &lt;dbl&gt; 194163, 330193, 270443, 46027, 102636, 11, 8974, 9689… ## $ `9/5/22` &lt;dbl&gt; 194355, 330221, 270461, 46027, 102636, 11, 8974, 9689… ## $ `9/6/22` &lt;dbl&gt; 194614, 330283, 270476, 46027, 102636, 11, 8974, 9689… ## $ `9/7/22` &lt;dbl&gt; 195012, 330516, 270489, 46113, 102636, 11, 8974, 9689… ## $ `9/8/22` &lt;dbl&gt; 195298, 330687, 270507, 46113, 102636, 11, 8974, 9689… ## $ `9/9/22` &lt;dbl&gt; 195471, 330842, 270522, 46113, 103131, 11, 8974, 9689… ## $ `9/10/22` &lt;dbl&gt; 195631, 330948, 270532, 46113, 103131, 11, 8974, 9689… ## $ `9/11/22` &lt;dbl&gt; 195925, 331036, 270539, 46113, 103131, 11, 8974, 9697… ## $ `9/12/22` &lt;dbl&gt; 196182, 331053, 270551, 46113, 103131, 11, 8974, 9697… ## $ `9/13/22` &lt;dbl&gt; 196404, 331191, 270551, 46113, 103131, 11, 8974, 9697… ## $ `9/14/22` &lt;dbl&gt; 196751, 331295, 270570, 46147, 103131, 11, 9008, 9697… ## $ `9/15/22` &lt;dbl&gt; 196870, 331384, 270584, 46147, 103131, 11, 9008, 9697… ## $ `9/16/22` &lt;dbl&gt; 196992, 331459, 270599, 46147, 103131, 11, 9008, 9697… ## $ `9/17/22` &lt;dbl&gt; 197066, 331540, 270606, 46147, 103131, 11, 9008, 9697… ## $ `9/18/22` &lt;dbl&gt; 197240, 331583, 270609, 46147, 103131, 11, 9008, 9703… ## $ `9/19/22` &lt;dbl&gt; 197434, 331601, 270612, 46147, 103131, 11, 9008, 9703… ## $ `9/20/22` &lt;dbl&gt; 197608, 331715, 270612, 46147, 103131, 11, 9008, 9703… ## $ `9/21/22` &lt;dbl&gt; 197788, 331810, 270619, 46147, 103131, 11, 9008, 9703… ## $ `9/22/22` &lt;dbl&gt; 198023, 331861, 270625, 46147, 103131, 11, 9008, 9703… ## $ `9/23/22` &lt;dbl&gt; 198163, 331908, 270631, 46147, 103131, 11, 9008, 9703… ## $ `9/24/22` &lt;dbl&gt; 198244, 331953, 270637, 46147, 103131, 11, 9008, 9703… ## $ `9/25/22` &lt;dbl&gt; 198416, 331976, 270641, 46147, 103131, 11, 9008, 9708… ## $ `9/26/22` &lt;dbl&gt; 198543, 331987, 270649, 46147, 103131, 11, 9089, 9708… ## $ `9/27/22` &lt;dbl&gt; 198750, 332066, 270654, 46147, 103131, 11, 9089, 9708… ## $ `9/28/22` &lt;dbl&gt; 198876, 332129, 270662, 46227, 103131, 11, 9089, 9708… ## $ `9/29/22` &lt;dbl&gt; 199067, 332173, 270668, 46227, 103131, 11, 9098, 9708… ## $ `9/30/22` &lt;dbl&gt; 199188, 332221, 270673, 46227, 103131, 11, 9098, 9708… ## $ `10/1/22` &lt;dbl&gt; 199310, 332263, 270676, 46227, 103131, 11, 9098, 9708… ## $ `10/2/22` &lt;dbl&gt; 199386, 332285, 270679, 46227, 103131, 11, 9098, 9711… ## $ `10/3/22` &lt;dbl&gt; 199545, 332290, 270682, 46227, 103131, 11, 9098, 9711… ## $ `10/4/22` &lt;dbl&gt; 199690, 332337, 270690, 46227, 103131, 11, 9098, 9711… ## $ `10/5/22` &lt;dbl&gt; 199845, 332372, 270693, 46227, 103131, 11, 9098, 9711… ## $ `10/6/22` &lt;dbl&gt; 199994, 332410, 270697, 46275, 103131, 11, 9098, 9711… ## $ `10/7/22` &lt;dbl&gt; 200130, 332443, 270701, 46275, 103131, 11, 9098, 9711… ## $ `10/8/22` &lt;dbl&gt; 200202, 332472, 270701, 46275, 103131, 11, 9098, 9711… ## $ `10/9/22` &lt;dbl&gt; 200372, 332494, 270707, 46275, 103131, 11, 9098, 9713… ## $ `10/10/22` &lt;dbl&gt; 200469, 332503, 270713, 46275, 103131, 11, 9098, 9713… ## $ `10/11/22` &lt;dbl&gt; 200626, 332534, 270716, 46275, 103131, 11, 9098, 9713… ## $ `10/12/22` &lt;dbl&gt; 200729, 332555, 270722, 46366, 103131, 11, 9106, 9713… ## $ `10/13/22` &lt;dbl&gt; 200846, 332579, 270722, 46366, 103131, 11, 9106, 9713… ## $ `10/14/22` &lt;dbl&gt; 201014, 332598, 270734, 46366, 103131, 11, 9106, 9713… ## $ `10/15/22` &lt;dbl&gt; 201096, 332619, 270734, 46366, 103131, 11, 9106, 9713… ## $ `10/16/22` &lt;dbl&gt; 201212, 332638, 270740, 46366, 103131, 11, 9106, 9715… ## $ `10/17/22` &lt;dbl&gt; 201276, 332645, 270757, 46366, 103131, 11, 9106, 9715… ## $ `10/18/22` &lt;dbl&gt; 201503, 332673, 270766, 46366, 103131, 11, 9106, 9715… ## $ `10/19/22` &lt;dbl&gt; 201557, 332701, 270768, 46449, 103131, 11, 9106, 9715… ## $ `10/20/22` &lt;dbl&gt; 201750, 332719, 270769, 46449, 103131, 11, 9106, 9715… ## $ `10/21/22` &lt;dbl&gt; 201949, 332739, 270771, 46449, 103131, 11, 9106, 9715… ## $ `10/22/22` &lt;dbl&gt; 202026, 332754, 270771, 46449, 103131, 11, 9106, 9715… ## $ `10/23/22` &lt;dbl&gt; 202108, 332772, 270783, 46449, 103131, 11, 9106, 9717… ## $ `10/24/22` &lt;dbl&gt; 202199, 332776, 270788, 46449, 103131, 11, 9106, 9717… ## $ `10/25/22` &lt;dbl&gt; 202347, 332816, 270800, 46449, 103131, 11, 9106, 9717… ## $ `10/26/22` &lt;dbl&gt; 202509, 332847, 270810, 46535, 103131, 11, 9106, 9717… ## $ `10/27/22` &lt;dbl&gt; 202608, 332889, 270817, 46535, 103131, 11, 9106, 9717… ## $ `10/28/22` &lt;dbl&gt; 202756, 332911, 270826, 46535, 103131, 11, 9106, 9717… ## $ `10/29/22` &lt;dbl&gt; 202834, 332949, 270829, 46535, 103131, 11, 9106, 9717… ## $ `10/30/22` &lt;dbl&gt; 202966, 332966, 270836, 46535, 103131, 11, 9106, 9718… ## $ `10/31/22` &lt;dbl&gt; 203063, 332966, 270838, 46535, 103131, 11, 9106, 9718… ## $ `11/1/22` &lt;dbl&gt; 203167, 332969, 270839, 46535, 103131, 11, 9106, 9718… ## $ `11/2/22` &lt;dbl&gt; 203265, 332996, 270840, 46588, 103131, 11, 9106, 9718… ## $ `11/3/22` &lt;dbl&gt; 203395, 332996, 270847, 46588, 103131, 11, 9106, 9718… ## $ `11/4/22` &lt;dbl&gt; 203497, 333027, 270856, 46588, 103131, 11, 9106, 9718… ## $ `11/5/22` &lt;dbl&gt; 203574, 333046, 270862, 46588, 103131, 11, 9106, 9718… ## $ `11/6/22` &lt;dbl&gt; 203681, 333055, 270873, 46588, 103131, 11, 9106, 9720… ## $ `11/7/22` &lt;dbl&gt; 203829, 333058, 270881, 46588, 103131, 11, 9106, 9720… ## $ `11/8/22` &lt;dbl&gt; 203942, 333071, 270891, 46588, 103131, 11, 9106, 9720… ## $ `11/9/22` &lt;dbl&gt; 204094, 333088, 270906, 46664, 103131, 11, 9106, 9720… ## $ `11/10/22` &lt;dbl&gt; 204287, 333103, 270917, 46664, 103131, 11, 9106, 9720… ## $ `11/11/22` &lt;dbl&gt; 204392, 333125, 270924, 46664, 103131, 11, 9106, 9720… ## $ `11/12/22` &lt;dbl&gt; 204417, 333138, 270929, 46664, 103131, 11, 9106, 9720… ## $ `11/13/22` &lt;dbl&gt; 204510, 333156, 270939, 46664, 103131, 11, 9106, 9721… ## $ `11/14/22` &lt;dbl&gt; 204610, 333161, 270952, 46664, 103131, 11, 9106, 9721… ## $ `11/15/22` &lt;dbl&gt; 204724, 333197, 270969, 46664, 103131, 11, 9106, 9721… ## $ `11/16/22` &lt;dbl&gt; 204820, 333215, 270981, 46824, 103131, 11, 9106, 9721… ## $ `11/17/22` &lt;dbl&gt; 204982, 333233, 270996, 46824, 103131, 11, 9106, 9721… ## $ `11/18/22` &lt;dbl&gt; 205009, 333233, 270996, 46824, 103131, 11, 9106, 9721… ## $ `11/19/22` &lt;dbl&gt; 205039, 333246, 271011, 46824, 103131, 11, 9106, 9721… ## $ `11/20/22` &lt;dbl&gt; 205146, 333256, 271023, 46824, 103131, 11, 9106, 9723… ## $ `11/21/22` &lt;dbl&gt; 205229, 333257, 271028, 46824, 103131, 11, 9106, 9723… ## $ `11/22/22` &lt;dbl&gt; 205324, 333282, 271035, 46824, 103131, 11, 9106, 9723… ## $ `11/23/22` &lt;dbl&gt; 205391, 333293, 271041, 46824, 104491, 11, 9106, 9723… ## $ `11/24/22` &lt;dbl&gt; 205506, 333305, 271050, 46824, 104491, 11, 9106, 9723… ## $ `11/25/22` &lt;dbl&gt; 205541, 333316, 271057, 46824, 104491, 11, 9106, 9723… ## $ `11/26/22` &lt;dbl&gt; 205612, 333322, 271061, 46824, 104491, 11, 9106, 9723… ## $ `11/27/22` &lt;dbl&gt; 205612, 333330, 271061, 46824, 104491, 11, 9106, 9727… ## $ `11/28/22` &lt;dbl&gt; 205802, 333330, 271079, 46824, 104491, 11, 9106, 9727… ## $ `11/29/22` &lt;dbl&gt; 205830, 333338, 271082, 46824, 104491, 11, 9106, 9727… confirmedraw # %&gt;% datatable() # Check latest date at the end of data as tibble ## # A tibble: 289 × 1,047 ## Provin…¹ Count…² Lat Long 1/22/…³ 1/23/…⁴ 1/24/…⁵ 1/25/…⁶ 1/26/…⁷ 1/27/…⁸ ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 &lt;NA&gt; Afghan… 33.9 67.7 0 0 0 0 0 0 ## 2 &lt;NA&gt; Albania 41.2 20.2 0 0 0 0 0 0 ## 3 &lt;NA&gt; Algeria 28.0 1.66 0 0 0 0 0 0 ## 4 &lt;NA&gt; Andorra 42.5 1.52 0 0 0 0 0 0 ## 5 &lt;NA&gt; Angola -11.2 17.9 0 0 0 0 0 0 ## 6 &lt;NA&gt; Antarc… -71.9 23.3 0 0 0 0 0 0 ## 7 &lt;NA&gt; Antigu… 17.1 -61.8 0 0 0 0 0 0 ## 8 &lt;NA&gt; Argent… -38.4 -63.6 0 0 0 0 0 0 ## 9 &lt;NA&gt; Armenia 40.1 45.0 0 0 0 0 0 0 ## 10 Austral… Austra… -35.5 149. 0 0 0 0 0 0 ## # … with 279 more rows, 1,037 more variables: `1/28/20` &lt;dbl&gt;, `1/29/20` &lt;dbl&gt;, ## # `1/30/20` &lt;dbl&gt;, `1/31/20` &lt;dbl&gt;, `2/1/20` &lt;dbl&gt;, `2/2/20` &lt;dbl&gt;, ## # `2/3/20` &lt;dbl&gt;, `2/4/20` &lt;dbl&gt;, `2/5/20` &lt;dbl&gt;, `2/6/20` &lt;dbl&gt;, ## # `2/7/20` &lt;dbl&gt;, `2/8/20` &lt;dbl&gt;, `2/9/20` &lt;dbl&gt;, `2/10/20` &lt;dbl&gt;, ## # `2/11/20` &lt;dbl&gt;, `2/12/20` &lt;dbl&gt;, `2/13/20` &lt;dbl&gt;, `2/14/20` &lt;dbl&gt;, ## # `2/15/20` &lt;dbl&gt;, `2/16/20` &lt;dbl&gt;, `2/17/20` &lt;dbl&gt;, `2/18/20` &lt;dbl&gt;, ## # `2/19/20` &lt;dbl&gt;, `2/20/20` &lt;dbl&gt;, `2/21/20` &lt;dbl&gt;, `2/22/20` &lt;dbl&gt;, … deathsraw &lt;- read_csv(&quot;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv&quot;) ## Rows: 289 Columns: 1047 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): Province/State, Country/Region ## dbl (1045): Lat, Long, 1/22/20, 1/23/20, 1/24/20, 1/25/20, 1/26/20, 1/27/20,... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. deathsraw # %&gt;% datatable() ## # A tibble: 289 × 1,047 ## Provin…¹ Count…² Lat Long 1/22/…³ 1/23/…⁴ 1/24/…⁵ 1/25/…⁶ 1/26/…⁷ 1/27/…⁸ ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 &lt;NA&gt; Afghan… 33.9 67.7 0 0 0 0 0 0 ## 2 &lt;NA&gt; Albania 41.2 20.2 0 0 0 0 0 0 ## 3 &lt;NA&gt; Algeria 28.0 1.66 0 0 0 0 0 0 ## 4 &lt;NA&gt; Andorra 42.5 1.52 0 0 0 0 0 0 ## 5 &lt;NA&gt; Angola -11.2 17.9 0 0 0 0 0 0 ## 6 &lt;NA&gt; Antarc… -71.9 23.3 0 0 0 0 0 0 ## 7 &lt;NA&gt; Antigu… 17.1 -61.8 0 0 0 0 0 0 ## 8 &lt;NA&gt; Argent… -38.4 -63.6 0 0 0 0 0 0 ## 9 &lt;NA&gt; Armenia 40.1 45.0 0 0 0 0 0 0 ## 10 Austral… Austra… -35.5 149. 0 0 0 0 0 0 ## # … with 279 more rows, 1,037 more variables: `1/28/20` &lt;dbl&gt;, `1/29/20` &lt;dbl&gt;, ## # `1/30/20` &lt;dbl&gt;, `1/31/20` &lt;dbl&gt;, `2/1/20` &lt;dbl&gt;, `2/2/20` &lt;dbl&gt;, ## # `2/3/20` &lt;dbl&gt;, `2/4/20` &lt;dbl&gt;, `2/5/20` &lt;dbl&gt;, `2/6/20` &lt;dbl&gt;, ## # `2/7/20` &lt;dbl&gt;, `2/8/20` &lt;dbl&gt;, `2/9/20` &lt;dbl&gt;, `2/10/20` &lt;dbl&gt;, ## # `2/11/20` &lt;dbl&gt;, `2/12/20` &lt;dbl&gt;, `2/13/20` &lt;dbl&gt;, `2/14/20` &lt;dbl&gt;, ## # `2/15/20` &lt;dbl&gt;, `2/16/20` &lt;dbl&gt;, `2/17/20` &lt;dbl&gt;, `2/18/20` &lt;dbl&gt;, ## # `2/19/20` &lt;dbl&gt;, `2/20/20` &lt;dbl&gt;, `2/21/20` &lt;dbl&gt;, `2/22/20` &lt;dbl&gt;, … recoveredraw &lt;- read_csv(&quot;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv&quot;) ## Rows: 274 Columns: 1047 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): Province/State, Country/Region ## dbl (1045): Lat, Long, 1/22/20, 1/23/20, 1/24/20, 1/25/20, 1/26/20, 1/27/20,... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. recoveredraw # %&gt;% datatable() ## # A tibble: 274 × 1,047 ## Provin…¹ Count…² Lat Long 1/22/…³ 1/23/…⁴ 1/24/…⁵ 1/25/…⁶ 1/26/…⁷ 1/27/…⁸ ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 &lt;NA&gt; Afghan… 33.9 67.7 0 0 0 0 0 0 ## 2 &lt;NA&gt; Albania 41.2 20.2 0 0 0 0 0 0 ## 3 &lt;NA&gt; Algeria 28.0 1.66 0 0 0 0 0 0 ## 4 &lt;NA&gt; Andorra 42.5 1.52 0 0 0 0 0 0 ## 5 &lt;NA&gt; Angola -11.2 17.9 0 0 0 0 0 0 ## 6 &lt;NA&gt; Antarc… -71.9 23.3 0 0 0 0 0 0 ## 7 &lt;NA&gt; Antigu… 17.1 -61.8 0 0 0 0 0 0 ## 8 &lt;NA&gt; Argent… -38.4 -63.6 0 0 0 0 0 0 ## 9 &lt;NA&gt; Armenia 40.1 45.0 0 0 0 0 0 0 ## 10 Austral… Austra… -35.5 149. 0 0 0 0 0 0 ## # … with 264 more rows, 1,037 more variables: `1/28/20` &lt;dbl&gt;, `1/29/20` &lt;dbl&gt;, ## # `1/30/20` &lt;dbl&gt;, `1/31/20` &lt;dbl&gt;, `2/1/20` &lt;dbl&gt;, `2/2/20` &lt;dbl&gt;, ## # `2/3/20` &lt;dbl&gt;, `2/4/20` &lt;dbl&gt;, `2/5/20` &lt;dbl&gt;, `2/6/20` &lt;dbl&gt;, ## # `2/7/20` &lt;dbl&gt;, `2/8/20` &lt;dbl&gt;, `2/9/20` &lt;dbl&gt;, `2/10/20` &lt;dbl&gt;, ## # `2/11/20` &lt;dbl&gt;, `2/12/20` &lt;dbl&gt;, `2/13/20` &lt;dbl&gt;, `2/14/20` &lt;dbl&gt;, ## # `2/15/20` &lt;dbl&gt;, `2/16/20` &lt;dbl&gt;, `2/17/20` &lt;dbl&gt;, `2/18/20` &lt;dbl&gt;, ## # `2/19/20` &lt;dbl&gt;, `2/20/20` &lt;dbl&gt;, `2/21/20` &lt;dbl&gt;, `2/22/20` &lt;dbl&gt;, … # Note differences in the number of rows/columns D.5.2 Tidying and Combining: To create country level and global combined data D.5.2.1 Convert each data set from wide to long confirmed &lt;- confirmedraw %&gt;% dplyr::rename(province = &quot;Province/State&quot;, country = &quot;Country/Region&quot;, lat = &quot;Lat&quot;, long = &quot;Long&quot;) %&gt;% pivot_longer(-c(province, country, lat, long), names_to = &quot;date&quot;, values_to =&quot;confirmed&quot;) %&gt;% mutate(date = as.Date(date, &quot;%m/%d/%y&quot;)) %&gt;% group_by(province, country) %&gt;% arrange(date) %&gt;% mutate(confirmed = confirmed - lag(confirmed)) %&gt;% slice(-1) %&gt;% ungroup() %&gt;% relocate(date, .before = province) %&gt;% group_by(country, province) %&gt;% arrange(province, date) Check the data. confirmed %&gt;% filter(country == &quot;Japan&quot;) %&gt;% ggplot() + geom_line(aes(x = date, y = confirmed)) df_tv %&gt;% filter(country == &quot;Japan&quot;) %&gt;% filter(type == &quot;confirmed&quot;) %&gt;% ggplot() + geom_line(aes(x = date, y = cases)) The dplyr::rename seems to have conflict with other rename function. deaths &lt;- deathsraw %&gt;% dplyr::rename(province = &quot;Province/State&quot;, country = &quot;Country/Region&quot;, lat = Lat, long = Long) %&gt;% pivot_longer(-c(province, country, lat, long), names_to = &quot;date&quot;, values_to =&quot;death&quot;) %&gt;% mutate(date = as.Date(date, &quot;%m/%d/%y&quot;)) %&gt;% group_by(province, country) %&gt;% arrange(date) %&gt;% mutate(death = death - lag(death)) %&gt;% slice(-1) %&gt;% ungroup() %&gt;% relocate(date, .before = province) %&gt;% arrange(province, date) recovered &lt;- recoveredraw %&gt;% dplyr::rename(province = &quot;Province/State&quot;, country = &quot;Country/Region&quot;, lat = Lat, long = Long) %&gt;% pivot_longer(-c(province, country, lat, long), names_to = &quot;date&quot;, values_to =&quot;recovered&quot;) %&gt;% mutate(date = as.Date(date, &quot;%m/%d/%y&quot;)) %&gt;% group_by(province, country) %&gt;% arrange(date) %&gt;% mutate(recovered = recovered - lag(recovered)) %&gt;% slice(-1) %&gt;% ungroup() %&gt;% relocate(date, .before = province) %&gt;% arrange(province, date) D.5.2.2 Final data: combine all three coronavirus_jhu &lt;- full_join(confirmed, deaths) %&gt;% full_join(recovered) %&gt;% pivot_longer(c(confirmed, death, recovered), names_to = &quot;cases&quot;) %&gt;% arrange(cases, province, country, date) ## Joining, by = c(&quot;date&quot;, &quot;province&quot;, &quot;country&quot;, &quot;lat&quot;, &quot;long&quot;) ## Joining, by = c(&quot;date&quot;, &quot;province&quot;, &quot;country&quot;, &quot;lat&quot;, &quot;long&quot;) coronavirus_jhu # %&gt;% datatable() ## # A tibble: 922,170 × 7 ## # Groups: country, province [290] ## date province country lat long cases value ## &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2020-01-23 Alberta Canada 53.9 -117. confirmed 0 ## 2 2020-01-24 Alberta Canada 53.9 -117. confirmed 0 ## 3 2020-01-25 Alberta Canada 53.9 -117. confirmed 0 ## 4 2020-01-26 Alberta Canada 53.9 -117. confirmed 0 ## 5 2020-01-27 Alberta Canada 53.9 -117. confirmed 0 ## 6 2020-01-28 Alberta Canada 53.9 -117. confirmed 0 ## 7 2020-01-29 Alberta Canada 53.9 -117. confirmed 0 ## 8 2020-01-30 Alberta Canada 53.9 -117. confirmed 0 ## 9 2020-01-31 Alberta Canada 53.9 -117. confirmed 0 ## 10 2020-02-01 Alberta Canada 53.9 -117. confirmed 0 ## # … with 922,160 more rows D.5.3 Aggregated by Countries The list of countries classified in provinces. coronavirus_jhu %&gt;% filter(!is.na(province)) %&gt;% distinct(country) ## # A tibble: 91 × 2 ## # Groups: country, province [91] ## province country ## &lt;chr&gt; &lt;chr&gt; ## 1 Alberta Canada ## 2 Anguilla United Kingdom ## 3 Anhui China ## 4 Aruba Netherlands ## 5 Australian Capital Territory Australia ## 6 Beijing China ## 7 Bermuda United Kingdom ## 8 Bonaire, Sint Eustatius and Saba Netherlands ## 9 British Columbia Canada ## 10 British Virgin Islands United Kingdom ## # … with 81 more rows Check the data associated with provinces. If we are only interested in coutries, the following is a possibility. coronavirus_jhu_country &lt;- coronavirus_jhu %&gt;% group_by(date, country, cases) %&gt;% summarize(value = sum(value)) %&gt;% arrange(cases, country, date) ## `summarise()` has grouped output by &#39;date&#39;, &#39;country&#39;. You can override using ## the `.groups` argument. coronavirus_jhu_country # %&gt;% datatable() ## # A tibble: 628,326 × 4 ## # Groups: date, country [209,442] ## date country cases value ## &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2020-01-23 Afghanistan confirmed 0 ## 2 2020-01-24 Afghanistan confirmed 0 ## 3 2020-01-25 Afghanistan confirmed 0 ## 4 2020-01-26 Afghanistan confirmed 0 ## 5 2020-01-27 Afghanistan confirmed 0 ## 6 2020-01-28 Afghanistan confirmed 0 ## 7 2020-01-29 Afghanistan confirmed 0 ## 8 2020-01-30 Afghanistan confirmed 0 ## 9 2020-01-31 Afghanistan confirmed 0 ## 10 2020-02-01 Afghanistan confirmed 0 ## # … with 628,316 more rows D.5.4 Population of 2019 library(WDI) population &lt;- WDI( country = &quot;all&quot;, indicator = &quot;SP.POP.TOTL&quot;, start = 2019, end = 2019, extra = TRUE, cache = NULL, latest = NULL, language = &quot;en&quot; ) %&gt;% select(country, iso2c, iso3c, region, income, population = SP.POP.TOTL) population # %&gt;% datatable() ## country iso2c iso3c ## 1 Afghanistan AF AFG ## 2 Africa Eastern and Southern ZH AFE ## 3 Africa Western and Central ZI AFW ## 4 Albania AL ALB ## 5 Algeria DZ DZA ## 6 American Samoa AS ASM ## 7 Andorra AD AND ## 8 Angola AO AGO ## 9 Antigua and Barbuda AG ATG ## 10 Arab World 1A ARB ## 11 Argentina AR ARG ## 12 Armenia AM ARM ## 13 Aruba AW ABW ## 14 Australia AU AUS ## 15 Austria AT AUT ## 16 Azerbaijan AZ AZE ## 17 Bahamas, The BS BHS ## 18 Bahrain BH BHR ## 19 Bangladesh BD BGD ## 20 Barbados BB BRB ## 21 Belarus BY BLR ## 22 Belgium BE BEL ## 23 Belize BZ BLZ ## 24 Benin BJ BEN ## 25 Bermuda BM BMU ## 26 Bhutan BT BTN ## 27 Bolivia BO BOL ## 28 Bosnia and Herzegovina BA BIH ## 29 Botswana BW BWA ## 30 Brazil BR BRA ## 31 British Virgin Islands VG VGB ## 32 Brunei Darussalam BN BRN ## 33 Bulgaria BG BGR ## 34 Burkina Faso BF BFA ## 35 Burundi BI BDI ## 36 Cabo Verde CV CPV ## 37 Cambodia KH KHM ## 38 Cameroon CM CMR ## 39 Canada CA CAN ## 40 Caribbean small states S3 CSS ## 41 Cayman Islands KY CYM ## 42 Central African Republic CF CAF ## 43 Central Europe and the Baltics B8 CEB ## 44 Chad TD TCD ## 45 Channel Islands JG CHI ## 46 Chile CL CHL ## 47 China CN CHN ## 48 Colombia CO COL ## 49 Comoros KM COM ## 50 Congo, Dem. Rep. CD COD ## 51 Congo, Rep. CG COG ## 52 Costa Rica CR CRI ## 53 Cote d&#39;Ivoire CI CIV ## 54 Croatia HR HRV ## 55 Cuba CU CUB ## 56 Curacao CW CUW ## 57 Cyprus CY CYP ## 58 Czechia CZ CZE ## 59 Denmark DK DNK ## 60 Djibouti DJ DJI ## 61 Dominica DM DMA ## 62 Dominican Republic DO DOM ## 63 Early-demographic dividend V2 EAR ## 64 East Asia &amp; Pacific Z4 EAS ## 65 East Asia &amp; Pacific (excluding high income) 4E EAP ## 66 East Asia &amp; Pacific (IDA &amp; IBRD countries) T4 TEA ## 67 Ecuador EC ECU ## 68 Egypt, Arab Rep. EG EGY ## 69 El Salvador SV SLV ## 70 Equatorial Guinea GQ GNQ ## 71 Eritrea ER ERI ## 72 Estonia EE EST ## 73 Eswatini SZ SWZ ## 74 Ethiopia ET ETH ## 75 Euro area XC EMU ## 76 Europe &amp; Central Asia Z7 ECS ## 77 Europe &amp; Central Asia (excluding high income) 7E ECA ## 78 Europe &amp; Central Asia (IDA &amp; IBRD countries) T7 TEC ## 79 European Union EU EUU ## 80 Faroe Islands FO FRO ## 81 Fiji FJ FJI ## 82 Finland FI FIN ## 83 Fragile and conflict affected situations F1 FCS ## 84 France FR FRA ## 85 French Polynesia PF PYF ## 86 Gabon GA GAB ## 87 Gambia, The GM GMB ## 88 Georgia GE GEO ## 89 Germany DE DEU ## 90 Ghana GH GHA ## 91 Gibraltar GI GIB ## 92 Greece GR GRC ## 93 Greenland GL GRL ## 94 Grenada GD GRD ## 95 Guam GU GUM ## 96 Guatemala GT GTM ## 97 Guinea GN GIN ## 98 Guinea-Bissau GW GNB ## 99 Guyana GY GUY ## 100 Haiti HT HTI ## 101 Heavily indebted poor countries (HIPC) XE HPC ## 102 High income XD ## 103 Honduras HN HND ## 104 Hong Kong SAR, China HK HKG ## 105 Hungary HU HUN ## 106 IBRD only XF IBD ## 107 Iceland IS ISL ## 108 IDA &amp; IBRD total ZT IBT ## 109 IDA blend XH IDB ## 110 IDA only XI IDX ## 111 IDA total XG IDA ## 112 India IN IND ## 113 Indonesia ID IDN ## 114 Iran, Islamic Rep. IR IRN ## 115 Iraq IQ IRQ ## 116 Ireland IE IRL ## 117 Isle of Man IM IMN ## 118 Israel IL ISR ## 119 Italy IT ITA ## 120 Jamaica JM JAM ## 121 Japan JP JPN ## 122 Jordan JO JOR ## 123 Kazakhstan KZ KAZ ## 124 Kenya KE KEN ## 125 Kiribati KI KIR ## 126 Korea, Dem. People&#39;s Rep. KP PRK ## 127 Korea, Rep. KR KOR ## 128 Kosovo XK XKX ## 129 Kuwait KW KWT ## 130 Kyrgyz Republic KG KGZ ## 131 Lao PDR LA LAO ## 132 Late-demographic dividend V3 LTE ## 133 Latin America &amp; Caribbean ZJ LCN ## 134 Latin America &amp; Caribbean (excluding high income) XJ LAC ## 135 Latin America &amp; the Caribbean (IDA &amp; IBRD countries) T2 TLA ## 136 Latvia LV LVA ## 137 Least developed countries: UN classification XL LDC ## 138 Lebanon LB LBN ## 139 Lesotho LS LSO ## 140 Liberia LR LBR ## 141 Libya LY LBY ## 142 Liechtenstein LI LIE ## 143 Lithuania LT LTU ## 144 Low &amp; middle income XO LMY ## 145 Low income XM ## 146 Lower middle income XN ## 147 Luxembourg LU LUX ## 148 Macao SAR, China MO MAC ## 149 Madagascar MG MDG ## 150 Malawi MW MWI ## 151 Malaysia MY MYS ## 152 Maldives MV MDV ## 153 Mali ML MLI ## 154 Malta MT MLT ## 155 Marshall Islands MH MHL ## 156 Mauritania MR MRT ## 157 Mauritius MU MUS ## 158 Mexico MX MEX ## 159 Micronesia, Fed. Sts. FM FSM ## 160 Middle East &amp; North Africa ZQ MEA ## 161 Middle East &amp; North Africa (excluding high income) XQ MNA ## 162 Middle East &amp; North Africa (IDA &amp; IBRD countries) T3 TMN ## 163 Middle income XP MIC ## 164 Moldova MD MDA ## 165 Monaco MC MCO ## 166 Mongolia MN MNG ## 167 Montenegro ME MNE ## 168 Morocco MA MAR ## 169 Mozambique MZ MOZ ## 170 Myanmar MM MMR ## 171 Namibia NA NAM ## 172 Nauru NR NRU ## 173 Nepal NP NPL ## 174 Netherlands NL NLD ## 175 New Caledonia NC NCL ## 176 New Zealand NZ NZL ## 177 Nicaragua NI NIC ## 178 Niger NE NER ## 179 Nigeria NG NGA ## 180 North America XU NAC ## 181 North Macedonia MK MKD ## 182 Northern Mariana Islands MP MNP ## 183 Norway NO NOR ## 184 Not classified XY ## 185 OECD members OE OED ## 186 Oman OM OMN ## 187 Other small states S4 OSS ## 188 Pacific island small states S2 PSS ## 189 Pakistan PK PAK ## 190 Palau PW PLW ## 191 Panama PA PAN ## 192 Papua New Guinea PG PNG ## 193 Paraguay PY PRY ## 194 Peru PE PER ## 195 Philippines PH PHL ## 196 Poland PL POL ## 197 Portugal PT PRT ## 198 Post-demographic dividend V4 PST ## 199 Pre-demographic dividend V1 PRE ## 200 Puerto Rico PR PRI ## 201 Qatar QA QAT ## 202 Romania RO ROU ## 203 Russian Federation RU RUS ## 204 Rwanda RW RWA ## 205 Samoa WS WSM ## 206 San Marino SM SMR ## 207 Sao Tome and Principe ST STP ## 208 Saudi Arabia SA SAU ## 209 Senegal SN SEN ## 210 Serbia RS SRB ## 211 Seychelles SC SYC ## 212 Sierra Leone SL SLE ## 213 Singapore SG SGP ## 214 Sint Maarten (Dutch part) SX SXM ## 215 Slovak Republic SK SVK ## 216 Slovenia SI SVN ## 217 Small states S1 SST ## 218 Solomon Islands SB SLB ## 219 Somalia SO SOM ## 220 South Africa ZA ZAF ## 221 South Asia 8S SAS ## 222 South Asia (IDA &amp; IBRD) T5 TSA ## 223 South Sudan SS SSD ## 224 Spain ES ESP ## 225 Sri Lanka LK LKA ## 226 St. Kitts and Nevis KN KNA ## 227 St. Lucia LC LCA ## 228 St. Martin (French part) MF MAF ## 229 St. Vincent and the Grenadines VC VCT ## 230 Sub-Saharan Africa ZG SSF ## 231 Sub-Saharan Africa (excluding high income) ZF SSA ## 232 Sub-Saharan Africa (IDA &amp; IBRD countries) T6 TSS ## 233 Sudan SD SDN ## 234 Suriname SR SUR ## 235 Sweden SE SWE ## 236 Switzerland CH CHE ## 237 Syrian Arab Republic SY SYR ## 238 Tajikistan TJ TJK ## 239 Tanzania TZ TZA ## 240 Thailand TH THA ## 241 Timor-Leste TL TLS ## 242 Togo TG TGO ## 243 Tonga TO TON ## 244 Trinidad and Tobago TT TTO ## 245 Tunisia TN TUN ## 246 Turkiye TR TUR ## 247 Turkmenistan TM TKM ## 248 Turks and Caicos Islands TC TCA ## 249 Tuvalu TV TUV ## 250 Uganda UG UGA ## 251 Ukraine UA UKR ## 252 United Arab Emirates AE ARE ## 253 United Kingdom GB GBR ## 254 United States US USA ## 255 Upper middle income XT ## 256 Uruguay UY URY ## 257 Uzbekistan UZ UZB ## 258 Vanuatu VU VUT ## 259 Venezuela, RB VE VEN ## 260 Vietnam VN VNM ## 261 Virgin Islands (U.S.) VI VIR ## 262 West Bank and Gaza PS PSE ## 263 World 1W WLD ## 264 Yemen, Rep. YE YEM ## 265 Zambia ZM ZMB ## 266 Zimbabwe ZW ZWE ## region income population ## 1 South Asia Low income 38041757 ## 2 Aggregates Aggregates 660046272 ## 3 Aggregates Aggregates 446911598 ## 4 Europe &amp; Central Asia Upper middle income 2854191 ## 5 Middle East &amp; North Africa Lower middle income 43053054 ## 6 East Asia &amp; Pacific Upper middle income 55312 ## 7 Europe &amp; Central Asia High income 77146 ## 8 Sub-Saharan Africa Lower middle income 31825299 ## 9 Latin America &amp; Caribbean High income 97115 ## 10 Aggregates Aggregates 427870273 ## 11 Latin America &amp; Caribbean Upper middle income 44938712 ## 12 Europe &amp; Central Asia Upper middle income 2957728 ## 13 Latin America &amp; Caribbean High income 106310 ## 14 East Asia &amp; Pacific High income 25365745 ## 15 Europe &amp; Central Asia High income 8879920 ## 16 Europe &amp; Central Asia Upper middle income 10024283 ## 17 Latin America &amp; Caribbean High income 389486 ## 18 Middle East &amp; North Africa High income 1641164 ## 19 South Asia Lower middle income 163046173 ## 20 Latin America &amp; Caribbean High income 287021 ## 21 Europe &amp; Central Asia Upper middle income 9419758 ## 22 Europe &amp; Central Asia High income 11488980 ## 23 Latin America &amp; Caribbean Upper middle income 390351 ## 24 Sub-Saharan Africa Lower middle income 11801151 ## 25 North America High income 63911 ## 26 South Asia Lower middle income 763094 ## 27 Latin America &amp; Caribbean Lower middle income 11513102 ## 28 Europe &amp; Central Asia Upper middle income 3300998 ## 29 Sub-Saharan Africa Upper middle income 2303703 ## 30 Latin America &amp; Caribbean Upper middle income 211049519 ## 31 Latin America &amp; Caribbean High income 30033 ## 32 East Asia &amp; Pacific High income 433296 ## 33 Europe &amp; Central Asia Upper middle income 6975761 ## 34 Sub-Saharan Africa Low income 20321383 ## 35 Sub-Saharan Africa Low income 11530577 ## 36 Sub-Saharan Africa Lower middle income 549936 ## 37 East Asia &amp; Pacific Lower middle income 16486542 ## 38 Sub-Saharan Africa Lower middle income 25876387 ## 39 North America High income 37601230 ## 40 Aggregates Aggregates 7401389 ## 41 Latin America &amp; Caribbean High income 64948 ## 42 Sub-Saharan Africa Low income 4745179 ## 43 Aggregates Aggregates 102398494 ## 44 Sub-Saharan Africa Low income 15946882 ## 45 Europe &amp; Central Asia High income 172264 ## 46 Latin America &amp; Caribbean High income 18952035 ## 47 East Asia &amp; Pacific Upper middle income 1407745000 ## 48 Latin America &amp; Caribbean Upper middle income 50339443 ## 49 Sub-Saharan Africa Lower middle income 850891 ## 50 Sub-Saharan Africa Low income 86790568 ## 51 Sub-Saharan Africa Lower middle income 5380504 ## 52 Latin America &amp; Caribbean Upper middle income 5047561 ## 53 Sub-Saharan Africa Lower middle income 25716554 ## 54 Europe &amp; Central Asia High income 4065253 ## 55 Latin America &amp; Caribbean Upper middle income 11333484 ## 56 Latin America &amp; Caribbean High income 157441 ## 57 Europe &amp; Central Asia High income 1198574 ## 58 &lt;NA&gt; &lt;NA&gt; 10671870 ## 59 Europe &amp; Central Asia High income 5814422 ## 60 Middle East &amp; North Africa Lower middle income 973557 ## 61 Latin America &amp; Caribbean Upper middle income 71808 ## 62 Latin America &amp; Caribbean Upper middle income 10738957 ## 63 Aggregates Aggregates 3290291029 ## 64 Aggregates Aggregates 2351127942 ## 65 Aggregates Aggregates 2103723076 ## 66 Aggregates Aggregates 2078012370 ## 67 Latin America &amp; Caribbean Upper middle income 17373657 ## 68 Middle East &amp; North Africa Lower middle income 100388076 ## 69 Latin America &amp; Caribbean Lower middle income 6453550 ## 70 Sub-Saharan Africa Upper middle income 1355982 ## 71 Sub-Saharan Africa Low income NA ## 72 Europe &amp; Central Asia High income 1326855 ## 73 Sub-Saharan Africa Lower middle income 1148133 ## 74 Sub-Saharan Africa Low income 112078727 ## 75 Aggregates Aggregates 342283354 ## 76 Aggregates Aggregates 920807612 ## 77 Aggregates Aggregates 399386100 ## 78 Aggregates Aggregates 460788476 ## 79 Aggregates Aggregates 447197811 ## 80 Europe &amp; Central Asia High income 48677 ## 81 East Asia &amp; Pacific Upper middle income 889955 ## 82 Europe &amp; Central Asia High income 5521606 ## 83 Aggregates Aggregates 940026046 ## 84 Europe &amp; Central Asia High income 67248926 ## 85 East Asia &amp; Pacific High income 279285 ## 86 Sub-Saharan Africa Upper middle income 2172578 ## 87 Sub-Saharan Africa Low income 2347696 ## 88 Europe &amp; Central Asia Upper middle income 3720161 ## 89 Europe &amp; Central Asia High income 83092962 ## 90 Sub-Saharan Africa Lower middle income 30417858 ## 91 Europe &amp; Central Asia High income 33706 ## 92 Europe &amp; Central Asia High income 10721582 ## 93 Europe &amp; Central Asia High income 56225 ## 94 Latin America &amp; Caribbean Upper middle income 112002 ## 95 East Asia &amp; Pacific High income 167295 ## 96 Latin America &amp; Caribbean Upper middle income 16604026 ## 97 Sub-Saharan Africa Low income 12771246 ## 98 Sub-Saharan Africa Low income 1920917 ## 99 Latin America &amp; Caribbean Upper middle income 782775 ## 100 Latin America &amp; Caribbean Lower middle income 11263079 ## 101 Aggregates Aggregates 801708019 ## 102 &lt;NA&gt; &lt;NA&gt; 1234830048 ## 103 Latin America &amp; Caribbean Lower middle income 9746115 ## 104 East Asia &amp; Pacific High income 7507900 ## 105 Europe &amp; Central Asia High income 9771141 ## 106 Aggregates Aggregates 4826259460 ## 107 Europe &amp; Central Asia High income 360563 ## 108 Aggregates Aggregates 6496952025 ## 109 Aggregates Aggregates 561571929 ## 110 Aggregates Aggregates 1109120636 ## 111 Aggregates Aggregates 1670692565 ## 112 South Asia Lower middle income 1366417756 ## 113 East Asia &amp; Pacific Lower middle income 270625567 ## 114 Middle East &amp; North Africa Lower middle income 82913893 ## 115 Middle East &amp; North Africa Upper middle income 39309789 ## 116 Europe &amp; Central Asia High income 4934340 ## 117 Europe &amp; Central Asia High income 84589 ## 118 Middle East &amp; North Africa High income 9054000 ## 119 Europe &amp; Central Asia High income 59729081 ## 120 Latin America &amp; Caribbean Upper middle income 2948277 ## 121 East Asia &amp; Pacific High income 126633000 ## 122 Middle East &amp; North Africa Upper middle income 10101697 ## 123 Europe &amp; Central Asia Upper middle income 18513673 ## 124 Sub-Saharan Africa Lower middle income 52573967 ## 125 East Asia &amp; Pacific Lower middle income 117608 ## 126 East Asia &amp; Pacific Low income 25666158 ## 127 East Asia &amp; Pacific High income 51764822 ## 128 Europe &amp; Central Asia Upper middle income 1788878 ## 129 Middle East &amp; North Africa High income 4207077 ## 130 Europe &amp; Central Asia Lower middle income 6456200 ## 131 East Asia &amp; Pacific Lower middle income 7169456 ## 132 Aggregates Aggregates 2308520840 ## 133 &lt;NA&gt; &lt;NA&gt; 646431661 ## 134 Aggregates Aggregates 585257302 ## 135 Aggregates Aggregates 630644771 ## 136 Europe &amp; Central Asia High income 1913822 ## 137 Aggregates Aggregates 1033088986 ## 138 Middle East &amp; North Africa Lower middle income 6855709 ## 139 Sub-Saharan Africa Lower middle income 2125267 ## 140 Sub-Saharan Africa Low income 4937374 ## 141 Middle East &amp; North Africa Upper middle income 6777453 ## 142 Europe &amp; Central Asia High income 38020 ## 143 Europe &amp; Central Asia High income 2794137 ## 144 Aggregates Aggregates 6420460567 ## 145 &lt;NA&gt; &lt;NA&gt; 665731857 ## 146 &lt;NA&gt; &lt;NA&gt; 3274021191 ## 147 Europe &amp; Central Asia High income 620001 ## 148 East Asia &amp; Pacific High income 640446 ## 149 Sub-Saharan Africa Low income 26969306 ## 150 Sub-Saharan Africa Low income 18628749 ## 151 East Asia &amp; Pacific Upper middle income 31949789 ## 152 South Asia Upper middle income 530957 ## 153 Sub-Saharan Africa Low income 19658023 ## 154 Middle East &amp; North Africa High income 504062 ## 155 East Asia &amp; Pacific Upper middle income 58791 ## 156 Sub-Saharan Africa Lower middle income 4525698 ## 157 Sub-Saharan Africa Upper middle income 1265711 ## 158 Latin America &amp; Caribbean Upper middle income 127575529 ## 159 East Asia &amp; Pacific Lower middle income 113811 ## 160 Aggregates Aggregates 456709496 ## 161 Aggregates Aggregates 389457075 ## 162 Aggregates Aggregates 384771769 ## 163 Aggregates Aggregates 5754728710 ## 164 Europe &amp; Central Asia Upper middle income 2664974 ## 165 Europe &amp; Central Asia High income 38967 ## 166 East Asia &amp; Pacific Lower middle income 3225166 ## 167 Europe &amp; Central Asia Upper middle income 622028 ## 168 Middle East &amp; North Africa Lower middle income 36471766 ## 169 Sub-Saharan Africa Low income 30366043 ## 170 East Asia &amp; Pacific Lower middle income 54045422 ## 171 Sub-Saharan Africa Upper middle income 2494524 ## 172 East Asia &amp; Pacific High income 10764 ## 173 South Asia Lower middle income 28608715 ## 174 Europe &amp; Central Asia High income 17344874 ## 175 East Asia &amp; Pacific High income 271300 ## 176 East Asia &amp; Pacific High income 4979200 ## 177 Latin America &amp; Caribbean Lower middle income 6545503 ## 178 Sub-Saharan Africa Low income 23310719 ## 179 Sub-Saharan Africa Lower middle income 200963603 ## 180 Aggregates Aggregates 365995094 ## 181 Europe &amp; Central Asia Upper middle income 2076694 ## 182 East Asia &amp; Pacific High income 57213 ## 183 Europe &amp; Central Asia High income 5347896 ## 184 &lt;NA&gt; &lt;NA&gt; NA ## 185 Aggregates Aggregates 1365274704 ## 186 Middle East &amp; North Africa High income 4974992 ## 187 Aggregates Aggregates 31361216 ## 188 Aggregates Aggregates 2491878 ## 189 South Asia Lower middle income 216565317 ## 190 East Asia &amp; Pacific Upper middle income 18001 ## 191 Latin America &amp; Caribbean High income 4246440 ## 192 East Asia &amp; Pacific Lower middle income 8776119 ## 193 Latin America &amp; Caribbean Upper middle income 7044639 ## 194 Latin America &amp; Caribbean Upper middle income 32510462 ## 195 East Asia &amp; Pacific Lower middle income 108116622 ## 196 Europe &amp; Central Asia High income 37965475 ## 197 Europe &amp; Central Asia High income 10286263 ## 198 Aggregates Aggregates 1113142897 ## 199 Aggregates Aggregates 944902748 ## 200 Latin America &amp; Caribbean High income 3193694 ## 201 Middle East &amp; North Africa High income 2832071 ## 202 Europe &amp; Central Asia High income 19371648 ## 203 Europe &amp; Central Asia Upper middle income 144406261 ## 204 Sub-Saharan Africa Low income 12626938 ## 205 East Asia &amp; Pacific Lower middle income 197093 ## 206 Europe &amp; Central Asia High income 33864 ## 207 Sub-Saharan Africa Lower middle income 215048 ## 208 Middle East &amp; North Africa High income 34268529 ## 209 Sub-Saharan Africa Lower middle income 16296362 ## 210 Europe &amp; Central Asia Upper middle income 6945235 ## 211 Sub-Saharan Africa High income 97625 ## 212 Sub-Saharan Africa Low income 7813207 ## 213 East Asia &amp; Pacific High income 5703569 ## 214 Latin America &amp; Caribbean High income 41608 ## 215 Europe &amp; Central Asia High income 5454147 ## 216 Europe &amp; Central Asia High income 2088385 ## 217 Aggregates Aggregates 41254483 ## 218 East Asia &amp; Pacific Lower middle income 669821 ## 219 Sub-Saharan Africa Low income 15442906 ## 220 Sub-Saharan Africa Upper middle income 58558267 ## 221 Aggregates Aggregates 1835776769 ## 222 Aggregates Aggregates 1835776769 ## 223 Sub-Saharan Africa Low income 11062114 ## 224 Europe &amp; Central Asia High income 47134837 ## 225 South Asia Lower middle income 21803000 ## 226 Latin America &amp; Caribbean High income 52834 ## 227 Latin America &amp; Caribbean Upper middle income 182795 ## 228 Latin America &amp; Caribbean High income 38002 ## 229 Latin America &amp; Caribbean Upper middle income 110593 ## 230 &lt;NA&gt; &lt;NA&gt; 1106957870 ## 231 Aggregates Aggregates 1106860245 ## 232 Aggregates Aggregates 1106957870 ## 233 Sub-Saharan Africa Low income 42813237 ## 234 Latin America &amp; Caribbean Upper middle income 581363 ## 235 Europe &amp; Central Asia High income 10278887 ## 236 Europe &amp; Central Asia High income 8575280 ## 237 Middle East &amp; North Africa Low income 17070132 ## 238 Europe &amp; Central Asia Lower middle income 9321023 ## 239 Sub-Saharan Africa Lower middle income 58005461 ## 240 East Asia &amp; Pacific Upper middle income 69625581 ## 241 East Asia &amp; Pacific Lower middle income 1293120 ## 242 Sub-Saharan Africa Low income 8082359 ## 243 East Asia &amp; Pacific Upper middle income 104497 ## 244 Latin America &amp; Caribbean High income 1394969 ## 245 Middle East &amp; North Africa Lower middle income 11694721 ## 246 Europe &amp; Central Asia Upper middle income 83429607 ## 247 Europe &amp; Central Asia Upper middle income 5942094 ## 248 Latin America &amp; Caribbean High income 38194 ## 249 East Asia &amp; Pacific Upper middle income 11655 ## 250 Sub-Saharan Africa Low income 44269587 ## 251 Europe &amp; Central Asia Lower middle income 44386203 ## 252 Middle East &amp; North Africa High income 9770526 ## 253 Europe &amp; Central Asia High income 66836327 ## 254 North America High income 328329953 ## 255 &lt;NA&gt; &lt;NA&gt; 2480707519 ## 256 Latin America &amp; Caribbean High income 3461731 ## 257 Europe &amp; Central Asia Lower middle income 33580350 ## 258 East Asia &amp; Pacific Lower middle income 299882 ## 259 Latin America &amp; Caribbean Not classified 28515829 ## 260 East Asia &amp; Pacific Lower middle income 96462108 ## 261 Latin America &amp; Caribbean High income 106669 ## 262 Middle East &amp; North Africa Lower middle income 4685306 ## 263 Aggregates Aggregates 7683806444 ## 264 Middle East &amp; North Africa Low income 29161922 ## 265 Sub-Saharan Africa Low income 17861034 ## 266 Sub-Saharan Africa Lower middle income 14645473 coronavirus_country &lt;- coronavirus_jhu_country %&gt;% left_join(population) ## Joining, by = &quot;country&quot; coronavirus_country # %&gt;% datatable() ## # A tibble: 628,326 × 9 ## # Groups: date, country [209,442] ## date country cases value iso2c iso3c region income popul…¹ ## &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2020-01-23 Afghanistan confirmed 0 AF AFG South Asia Low in… 3.80e7 ## 2 2020-01-24 Afghanistan confirmed 0 AF AFG South Asia Low in… 3.80e7 ## 3 2020-01-25 Afghanistan confirmed 0 AF AFG South Asia Low in… 3.80e7 ## 4 2020-01-26 Afghanistan confirmed 0 AF AFG South Asia Low in… 3.80e7 ## 5 2020-01-27 Afghanistan confirmed 0 AF AFG South Asia Low in… 3.80e7 ## 6 2020-01-28 Afghanistan confirmed 0 AF AFG South Asia Low in… 3.80e7 ## 7 2020-01-29 Afghanistan confirmed 0 AF AFG South Asia Low in… 3.80e7 ## 8 2020-01-30 Afghanistan confirmed 0 AF AFG South Asia Low in… 3.80e7 ## 9 2020-01-31 Afghanistan confirmed 0 AF AFG South Asia Low in… 3.80e7 ## 10 2020-02-01 Afghanistan confirmed 0 AF AFG South Asia Low in… 3.80e7 ## # … with 628,316 more rows, and abbreviated variable name ¹​population summary(coronavirus_country) ## date country cases value ## Min. :2020-01-23 Length:628326 Length:628326 Min. :-30974748 ## 1st Qu.:2020-10-09 Class :character Class :character 1st Qu.: 0 ## Median :2021-06-26 Mode :character Mode :character Median : 0 ## Mean :2021-06-26 Mean : 1046 ## 3rd Qu.:2022-03-14 3rd Qu.: 54 ## Max. :2022-11-29 Max. : 1355242 ## NA&#39;s :15630 ## iso2c iso3c region income ## Length:628326 Length:628326 Length:628326 Length:628326 ## Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character ## ## ## ## ## population ## Min. :1.076e+04 ## 1st Qu.:1.921e+06 ## Median :8.828e+06 ## Mean :3.869e+07 ## 3rd Qu.:2.572e+07 ## Max. :1.408e+09 ## NA&#39;s :96906 D.5.4.1 Region coronavirus_country %&gt;% group_by(region) %&gt;% summarize(n = n_distinct(country)) ## # A tibble: 8 × 2 ## region n ## &lt;chr&gt; &lt;int&gt; ## 1 East Asia &amp; Pacific 24 ## 2 Europe &amp; Central Asia 47 ## 3 Latin America &amp; Caribbean 28 ## 4 Middle East &amp; North Africa 17 ## 5 North America 1 ## 6 South Asia 8 ## 7 Sub-Saharan Africa 45 ## 8 &lt;NA&gt; 31 coronavirus_country %&gt;% filter(is.na(region)) %&gt;% pull(country) %&gt;% unique() ## [1] &quot;Antarctica&quot; &quot;Bahamas&quot; ## [3] &quot;Brunei&quot; &quot;Burma&quot; ## [5] &quot;Congo (Brazzaville)&quot; &quot;Congo (Kinshasa)&quot; ## [7] &quot;Czechia&quot; &quot;Diamond Princess&quot; ## [9] &quot;Egypt&quot; &quot;Gambia&quot; ## [11] &quot;Holy See&quot; &quot;Iran&quot; ## [13] &quot;Korea, North&quot; &quot;Korea, South&quot; ## [15] &quot;Kyrgyzstan&quot; &quot;Laos&quot; ## [17] &quot;Micronesia&quot; &quot;MS Zaandam&quot; ## [19] &quot;Russia&quot; &quot;Saint Kitts and Nevis&quot; ## [21] &quot;Saint Lucia&quot; &quot;Saint Vincent and the Grenadines&quot; ## [23] &quot;Slovakia&quot; &quot;Summer Olympics 2020&quot; ## [25] &quot;Syria&quot; &quot;Taiwan*&quot; ## [27] &quot;Turkey&quot; &quot;US&quot; ## [29] &quot;Venezuela&quot; &quot;Winter Olympics 2022&quot; ## [31] &quot;Yemen&quot; coronavirus_country %&gt;% drop_na() %&gt;% group_by(region) %&gt;% summarize(n = n_distinct(country)) %&gt;% arrange(n) %&gt;% ggplot() + geom_col(aes(y = reorder(region, n), x = n)) coronavirus_country %&gt;% filter(cases == &quot;confirmed&quot;) %&gt;% group_by(region, date) %&gt;% summarize(confirmed = sum(value, na.rm = TRUE)) %&gt;% ggplot() + geom_line(aes(x = date, y = confirmed, color = region)) + labs(title = &quot;Total Number of Confirmed Cases by Region&quot;) ## `summarise()` has grouped output by &#39;region&#39;. You can override using the ## `.groups` argument. coronavirus_country %&gt;% filter(cases == &quot;death&quot;) %&gt;% group_by(region, date) %&gt;% summarize(death = sum(value, na.rm = TRUE)) %&gt;% ggplot() + geom_line(aes(x = date, y = death, color = region)) + labs(title = &quot;Total Number of Deaths by Region&quot;) ## `summarise()` has grouped output by &#39;region&#39;. You can override using the ## `.groups` argument. D.5.4.2 Income coronavirus_country %&gt;% group_by(income) %&gt;% summarize(n = n_distinct(country)) ## # A tibble: 5 × 2 ## income n ## &lt;chr&gt; &lt;int&gt; ## 1 High income 52 ## 2 Low income 23 ## 3 Lower middle income 47 ## 4 Upper middle income 48 ## 5 &lt;NA&gt; 31 coronavirus_country %&gt;% filter(is.na(income)) %&gt;% pull(country) %&gt;% unique() ## [1] &quot;Antarctica&quot; &quot;Bahamas&quot; ## [3] &quot;Brunei&quot; &quot;Burma&quot; ## [5] &quot;Congo (Brazzaville)&quot; &quot;Congo (Kinshasa)&quot; ## [7] &quot;Czechia&quot; &quot;Diamond Princess&quot; ## [9] &quot;Egypt&quot; &quot;Gambia&quot; ## [11] &quot;Holy See&quot; &quot;Iran&quot; ## [13] &quot;Korea, North&quot; &quot;Korea, South&quot; ## [15] &quot;Kyrgyzstan&quot; &quot;Laos&quot; ## [17] &quot;Micronesia&quot; &quot;MS Zaandam&quot; ## [19] &quot;Russia&quot; &quot;Saint Kitts and Nevis&quot; ## [21] &quot;Saint Lucia&quot; &quot;Saint Vincent and the Grenadines&quot; ## [23] &quot;Slovakia&quot; &quot;Summer Olympics 2020&quot; ## [25] &quot;Syria&quot; &quot;Taiwan*&quot; ## [27] &quot;Turkey&quot; &quot;US&quot; ## [29] &quot;Venezuela&quot; &quot;Winter Olympics 2022&quot; ## [31] &quot;Yemen&quot; coronavirus_country %&gt;% drop_na() %&gt;% group_by(income) %&gt;% summarize(n = n_distinct(country)) %&gt;% arrange(n) %&gt;% ggplot() + geom_col(aes(y = reorder(income, n), x = n)) coronavirus_country %&gt;% filter(cases == &quot;confirmed&quot;) %&gt;% group_by(income, date) %&gt;% summarize(confirmed = sum(value, na.rm = TRUE)) %&gt;% ggplot() + geom_line(aes(x = date, y = confirmed, color = income)) + labs(title = &quot;Total Number of Confirmed Cases by Income Level&quot;) ## `summarise()` has grouped output by &#39;income&#39;. You can override using the ## `.groups` argument. coronavirus_country %&gt;% filter(cases == &quot;death&quot;) %&gt;% group_by(income, date) %&gt;% summarize(death = sum(value, na.rm = TRUE)) %&gt;% ggplot() + geom_line(aes(x = date, y = death, color = income)) + labs(title = &quot;Total Number of Deaths by Income Level&quot;) ## `summarise()` has grouped output by &#39;income&#39;. You can override using the ## `.groups` argument. ### Analysis Suggested by Rami Krispin See https://github.com/RamiKrispin/coronavirus/ Rami Krispin is the author of an R package coronavirus D.5.4.3 Summary of the total confrimed cases by country coronavirus_country %&gt;% filter(cases == &quot;confirmed&quot;) %&gt;% group_by(country) %&gt;% summarize(total_cases = sum(value)) %&gt;% arrange(desc(total_cases)) ## # A tibble: 201 × 2 ## country total_cases ## &lt;chr&gt; &lt;dbl&gt; ## 1 US 98673987 ## 2 India 44673567 ## 3 France 37979248 ## 4 Germany 36463485 ## 5 Brazil 35227599 ## 6 Korea, South 27098733 ## 7 Japan 24676910 ## 8 Italy 24260660 ## 9 United Kingdom 24224763 ## 10 Russia 21278433 ## # … with 191 more rows D.5.4.4 Summary of new cases during the past 24 hours by country and type Date = 2022-11-28 coronavirus_country %&gt;% filter(date == Sys.Date() -2) %&gt;% select(country, cases, value) %&gt;% group_by(country, cases) %&gt;% summarize(total_cases = sum(value)) %&gt;% pivot_wider(names_from = cases, values_from = total_cases) %&gt;% arrange(desc(confirmed)) ## Adding missing grouping variables: `date` ## `summarise()` has grouped output by &#39;country&#39;. You can override using the `.groups` argument. ## # A tibble: 201 × 4 ## # Groups: country [201] ## country confirmed death recovered ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 France 95382 132 0 ## 2 Korea, South 71476 41 0 ## 3 US 59717 280 0 ## 4 Japan 49117 103 0 ## 5 Germany 46552 162 0 ## 6 Taiwan* 10651 41 0 ## 7 Russia 4980 50 0 ## 8 Australia 4149 19 0 ## 9 Indonesia 3225 59 0 ## 10 Belgium 3152 15 0 ## # … with 191 more rows "],["pop.html", "E Appendix E Population E.1 About: United States Census Bureau E.2 International Data Base (IDB) December 2020 Release (Now in 2021) E.3 Analysis using idbzip E.4 Population Pyramid", " E Appendix E Population Population Analysis using the UN Data and the US Census Bureau E.1 About: United States Census Bureau The Unites States of America Census Burea compiles a huge set of data. It provides data to the World Fact Book of Central Intelligence Agency and the Census Academy resources for Data Science Education. In alignment with the Digital Government Strategy, the Census Bureau is offering the public wider access to key U.S. statistics. (About) To study the population analysis of the world and its visualization, visit the following sites: Census Academy: https://www.census.gov/data/academy.html Infographics &amp; Visualizations: https://www.census.gov/library/visualizations.html U.S. and World Population Clock: https://www.census.gov/popclock/world Data Tool: https://www.census.gov/data-tools/demo/idb/#/country?YR_ANIM=2020 We can access to the data directly or by an API, Application Program Interface. In the following we study population data of the world in these two ways. E.2 International Data Base (IDB) December 2020 Release (Now in 2021) These data files correspond to the data available in the U.S. Census Bureau’s API. Each file is pipe “|” delimited, and the header row is demarcated with “#” at the start of the row. For additional technical specifications, including variable definitions, please visit https://www.census.gov/data/developers/data-sets/international-database.html For more information about the International Data Base, including release notes and detailed methodology, please visit https://www.census.gov/programs-surveys/international-programs/about/idb.html Variables: AGE: Single year of age from 0-100+ AREA_KM2: Area in square kilometers FIPS: FIPS country/area Code Federal Information Processing Standards for: Census API FIPS ‘for’ clause GENC: Geopolitical Entities, Names, and Codes (GENC) two character country code standard in: Census API FIPS ‘in’ clause NAME Country or area name POP: Total mid-year population SEX: Sex 0 = Both Sexes, 1 = Male, 2 = Female time: ISO-8601 Date/Time value ucgid Uniform Census Geography Identifier clause YR Year The file size is huge as a text file with about 8 million rows. (For Excel, the total number of rows on a worksheet is 1,048,576, about 1 million.) We need to download once but we should read the downloaded file instead of downloading it everytime. Commands in tidyverse package works very fast and we can handle the data of this size. Recently 2021 version was publishes: https://www.census.gov/data-tools/demo/idb/#/country?YR_ANIM=2021 New: https://www2.census.gov/programs-surveys/international-programs/about/idb/idbzip.zip Old: https://www2.census.gov/programs-surveys/international-programs/about/idb/idbzip.zip E.3 Analysis using idbzip library(tidyverse) ### For the first time, delete # in the following four lines to download the files. ## From the second time, add # to the following four lines to avoid downloading the files. # idbzip_url &lt;- &quot;https://www2.census.gov/programs-surveys/international-programs/about/idb/idbzip.zip&quot; # URL of the zip file. # dir.create(&quot;data/idbzip&quot;) # store everything in idbzip directory in the working directory # download.file(url = idbzip_url, destfile = &quot;data/idbzip/idbzip.zip&quot;) # file size: 43.1 MB # unzip(&quot;data/idbzip/idbzip.zip&quot;, exdir = &quot;data/idbzip&quot;) # zip file contains three files idb5yr.all, idbsingleyear.all, Readme.txt # idb &lt;- read_delim(&quot;data/idbzip/idbsingleyear.all&quot;, delim = &quot;|&quot;) # glimpse(idb) # idb Since it is too large, we chose 15 countries and stored it as data/idb15.csv and data/world.csv, which is the data of the world population. #idb %&gt;% # filter(GENC %in% c(&quot;BD&quot;, &quot;CH&quot;,&quot;DE&quot;,&quot;FR&quot;,&quot;GB&quot;,&quot;ID&quot;, &quot;IN&quot;,&quot;JP&quot;, &quot;KR&quot;,&quot;LK&quot;,&quot;MY&quot;, &quot;PH&quot;,&quot;TH&quot;,&quot;US&quot;,&quot;VN&quot;)) %&gt;% # select(&quot;YEAR&quot; = `#YR`, &quot;COUNTRY&quot; = NAME, &quot;ISO2&quot; = GENC, SEX, POP, AGE) %&gt;% # write_csv(&quot;data/idb15.csv&quot;) world_all &lt;- idb %&gt;% select(&quot;YEAR&quot; = `#YR`, SEX, POP, AGE) %&gt;% mutate(SEX = as_factor(SEX)) world &lt;- world_all %&gt;% group_by(YEAR, SEX) %&gt;% summarize(POPULATION = sum(POP)) world write_csv(world, &quot;data/world.csv&quot;) countries &lt;- idb %&gt;% select(&quot;YEAR&quot; = `#YR`, SEX, GENC, POP) %&gt;% mutate(SEX = as_factor(SEX)) %&gt;% filter(SEX == 0) %&gt;% group_by(YEAR, GENC) %&gt;% summarize(POPULATION = sum(POP)) %&gt;% ungroup() %&gt;% group_by(YEAR) %&gt;% summarize(NUMBER = n()) write_csv(countries, &quot;data/world2.csv&quot;) E.3.1 Popultion of the World world &lt;- read_csv(&quot;data/world.csv&quot;) ## Rows: 453 Columns: 3 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (3): YEAR, SEX, POPULATION ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. world2 &lt;- read_csv(&quot;data/world2.csv&quot;) ## Rows: 151 Columns: 2 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (2): YEAR, NUMBER ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. idb15 &lt;- read_csv(&quot;data/idb15.csv&quot;) ## Rows: 509343 Columns: 6 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): COUNTRY, ISO2 ## dbl (4): YEAR, SEX, POP, AGE ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. idb15 ## # A tibble: 509,343 × 6 ## YEAR COUNTRY ISO2 SEX POP AGE ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1981 Bangladesh BD 0 3428071 0 ## 2 1981 Bangladesh BD 0 3072594 1 ## 3 1981 Bangladesh BD 0 2888362 2 ## 4 1981 Bangladesh BD 0 2782738 3 ## 5 1981 Bangladesh BD 0 2719081 4 ## 6 1981 Bangladesh BD 0 2669809 5 ## 7 1981 Bangladesh BD 0 2614671 6 ## 8 1981 Bangladesh BD 0 2545849 7 ## 9 1981 Bangladesh BD 0 2462867 8 ## 10 1981 Bangladesh BD 0 2373595 9 ## # … with 509,333 more rows idb15 %&gt;% distinct(COUNTRY, ISO2) ## # A tibble: 15 × 2 ## COUNTRY ISO2 ## &lt;chr&gt; &lt;chr&gt; ## 1 Bangladesh BD ## 2 China CN ## 3 Germany DE ## 4 France FR ## 5 United Kingdom GB ## 6 Indonesia ID ## 7 India IN ## 8 Japan JP ## 9 Korea, South KR ## 10 Sri Lanka LK ## 11 Malaysia MY ## 12 Philippines PH ## 13 Thailand TH ## 14 United States US ## 15 Vietnam VN summary(idb15) ## YEAR COUNTRY ISO2 SEX ## Min. :1980 Length:509343 Length:509343 Min. :0 ## 1st Qu.:2014 Class :character Class :character 1st Qu.:0 ## Median :2042 Mode :character Mode :character Median :1 ## Mean :2042 Mean :1 ## 3rd Qu.:2070 3rd Qu.:2 ## Max. :2100 Max. :2 ## POP AGE ## Min. : 6 Min. : 0 ## 1st Qu.: 253533 1st Qu.: 25 ## Median : 516669 Median : 50 ## Mean : 1813595 Mean : 50 ## 3rd Qu.: 1329090 3rd Qu.: 75 ## Max. :30630618 Max. :100 world %&gt;% ggplot(aes(x = YEAR, y = POPULATION)) + geom_line(aes(color = SEX)) + geom_vline(xintercept = 2020) Something is wrong! summary(world2) ## YEAR NUMBER ## Min. :1950 Min. : 2.0 ## 1st Qu.:1988 1st Qu.:113.5 ## Median :2025 Median :226.0 ## Mean :2025 Mean :173.3 ## 3rd Qu.:2062 3rd Qu.:227.0 ## Max. :2100 Max. :227.0 ggplot(world2, aes(x = YEAR, y = NUMBER)) + geom_bar(stat = &quot;identity&quot;) E.3.2 Population of a Country, JAPAN japan &lt;- filter(idb15, ISO2 == &quot;JP&quot;) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = as_factor(SEX)) japan ## # A tibble: 33,633 × 4 ## YEAR SEX POP AGE ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1990 0 1228598 0 ## 2 1990 0 1275792 1 ## 3 1990 0 1318661 2 ## 4 1990 0 1355679 3 ## 5 1990 0 1388504 4 ## 6 1990 0 1452576 5 ## 7 1990 0 1494424 6 ## 8 1990 0 1507634 7 ## 9 1990 0 1515567 8 ## 10 1990 0 1547527 9 ## # … with 33,623 more rows pop &lt;- japan %&gt;% group_by(YEAR, SEX) %&gt;% summarize(POPULATION = sum(POP)) ## `summarise()` has grouped output by &#39;YEAR&#39;. You can override using the `.groups` ## argument. pop ## # A tibble: 333 × 3 ## # Groups: YEAR [111] ## YEAR SEX POPULATION ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1990 0 123537399 ## 2 1990 1 60628417 ## 3 1990 2 62908982 ## 4 1991 0 123962538 ## 5 1991 1 60832741 ## 6 1991 2 63129797 ## 7 1992 0 124378689 ## 8 1992 1 61030495 ## 9 1992 2 63348194 ## 10 1993 0 124738157 ## # … with 323 more rows pop %&gt;% ggplot(aes(x = YEAR, y = POPULATION)) + geom_line(aes(color = SEX)) + geom_vline(xintercept = 2020) japan2020 &lt;- japan %&gt;% filter(YEAR == 2020, SEX == 0) ggplot(japan2020) + geom_bar(aes(x = AGE, y = POP), stat = &quot;identity&quot;) japan_adult &lt;- filter(japan2020, AGE &gt;=18) ggplot(japan_adult) + geom_line(aes(x = AGE, y = cumsum(POP)/sum(POP)*100)) + geom_vline(xintercept = 40, color = &quot;red&quot;) E.4 Population Pyramid https://www.populationpyramid.net/world/2019/ ggplot2: https://rpubs.com/walkerke/pyramids_ggplot2 US Census: https://cran.r-project.org/web/packages/idbr/idbr.pdf https://cran.r-project.org/web/packages/idbr/index.html E.4.1 Population Pyramid of Japan or Other Countries E.4.1.1 Japan yr &lt;- 2020 country &lt;- &quot;Japan&quot; filter(idb15, COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = fct_recode(as_factor(SEX), &quot;Both Sex&quot; = &quot;0&quot;, &quot;Male&quot; = &quot;1&quot;, &quot;Female&quot; = &quot;2&quot;), POP = POP/1000) %&gt;% filter(YEAR == yr, SEX != &quot;Both Sex&quot;) %&gt;% ggplot(aes(x = AGE, y = ifelse(SEX == &quot;Male&quot;, -POP, POP), fill = SEX)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() + labs(title = paste(&quot;Population Pyramid of&quot;, country, yr), subtitle = &quot;population unit in 1000&quot;) + scale_y_continuous(breaks = seq(-1000, 1000, 500), labels = as.character(c(1000, 500, 0, 500, 1000))) + ylab(&quot;Male vs Female&quot;) + scale_fill_brewer(palette = &quot;Set2&quot;) idb15 %&gt;% filter(COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = as_factor(SEX)) %&gt;% group_by(YEAR, SEX) %&gt;% summarize(POPULATION = sum(POP)) %&gt;% ggplot(aes(x = YEAR, y = POPULATION)) + geom_line(aes(color = SEX)) + geom_vline(xintercept = 2020) ## `summarise()` has grouped output by &#39;YEAR&#39;. You can override using the `.groups` ## argument. E.4.1.2 Bangladesh yr &lt;- 2020 country &lt;- &quot;Bangladesh&quot; filter(idb15, COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = fct_recode(as_factor(SEX), &quot;Both Sex&quot; = &quot;0&quot;, &quot;Male&quot; = &quot;1&quot;, &quot;Female&quot; = &quot;2&quot;), POP = POP/1000) %&gt;% filter(YEAR == yr, SEX != &quot;Both Sex&quot;) %&gt;% ggplot(aes(x = AGE, y = ifelse(SEX == &quot;Male&quot;, -POP, POP), fill = SEX)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() + labs(title = paste(&quot;Population Pyramid of&quot;, country, yr), subtitle = &quot;population unit in 1000&quot;) + scale_y_continuous(breaks = seq(-1000, 1000, 500), labels = as.character(c(1000, 500, 0, 500, 1000))) + ylab(&quot;Male vs Female&quot;) + scale_fill_brewer(palette = &quot;Set2&quot;) idb15 %&gt;% filter(COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = as_factor(SEX)) %&gt;% group_by(YEAR, SEX) %&gt;% summarize(POPULATION = sum(POP)) %&gt;% ggplot(aes(x = YEAR, y = POPULATION)) + geom_line(aes(color = SEX)) + geom_vline(xintercept = 2020) ## `summarise()` has grouped output by &#39;YEAR&#39;. You can override using the `.groups` ## argument. E.4.1.3 China yr &lt;- 2020 country &lt;- &quot;China&quot; filter(idb15, COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = fct_recode(as_factor(SEX), &quot;Both Sex&quot; = &quot;0&quot;, &quot;Male&quot; = &quot;1&quot;, &quot;Female&quot; = &quot;2&quot;), POP = POP/1000) %&gt;% filter(YEAR == yr, SEX != &quot;Both Sex&quot;) %&gt;% ggplot(aes(x = AGE, y = ifelse(SEX == &quot;Male&quot;, -POP, POP), fill = SEX)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() + labs(title = paste(&quot;Population Pyramid of&quot;, country, yr), subtitle = &quot;population unit in 1000&quot;) + scale_y_continuous(breaks = seq(-1000, 1000, 500), labels = as.character(c(1000, 500, 0, 500, 1000))) + ylab(&quot;Male vs Female&quot;) + scale_fill_brewer(palette = &quot;Set2&quot;) idb15 %&gt;% filter(COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = as_factor(SEX)) %&gt;% group_by(YEAR, SEX) %&gt;% summarize(POPULATION = sum(POP)) %&gt;% ggplot(aes(x = YEAR, y = POPULATION)) + geom_line(aes(color = SEX)) + geom_vline(xintercept = 2020) ## `summarise()` has grouped output by &#39;YEAR&#39;. You can override using the `.groups` ## argument. E.4.1.4 Germany yr &lt;- 2020 country &lt;- &quot;Germany&quot; filter(idb15, COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = fct_recode(as_factor(SEX), &quot;Both Sex&quot; = &quot;0&quot;, &quot;Male&quot; = &quot;1&quot;, &quot;Female&quot; = &quot;2&quot;), POP = POP/1000) %&gt;% filter(YEAR == yr, SEX != &quot;Both Sex&quot;) %&gt;% ggplot(aes(x = AGE, y = ifelse(SEX == &quot;Male&quot;, -POP, POP), fill = SEX)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() + labs(title = paste(&quot;Population Pyramid of&quot;, country, yr), subtitle = &quot;population unit in 1000&quot;) + scale_y_continuous(breaks = seq(-1000, 1000, 500), labels = as.character(c(1000, 500, 0, 500, 1000))) + ylab(&quot;Male vs Female&quot;) + scale_fill_brewer(palette = &quot;Set2&quot;) idb15 %&gt;% filter(COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = as_factor(SEX)) %&gt;% group_by(YEAR, SEX) %&gt;% summarize(POPULATION = sum(POP)) %&gt;% ggplot(aes(x = YEAR, y = POPULATION)) + geom_line(aes(color = SEX)) + geom_vline(xintercept = 2020) ## `summarise()` has grouped output by &#39;YEAR&#39;. You can override using the `.groups` ## argument. E.4.1.5 France yr &lt;- 2020 country &lt;- &quot;France&quot; filter(idb15, COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = fct_recode(as_factor(SEX), &quot;Both Sex&quot; = &quot;0&quot;, &quot;Male&quot; = &quot;1&quot;, &quot;Female&quot; = &quot;2&quot;), POP = POP/1000) %&gt;% filter(YEAR == yr, SEX != &quot;Both Sex&quot;) %&gt;% ggplot(aes(x = AGE, y = ifelse(SEX == &quot;Male&quot;, -POP, POP), fill = SEX)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() + labs(title = paste(&quot;Population Pyramid of&quot;, country, yr), subtitle = &quot;population unit in 1000&quot;) + scale_y_continuous(breaks = seq(-1000, 1000, 500), labels = as.character(c(1000, 500, 0, 500, 1000))) + ylab(&quot;Male vs Female&quot;) + scale_fill_brewer(palette = &quot;Set2&quot;) idb15 %&gt;% filter(COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = as_factor(SEX)) %&gt;% group_by(YEAR, SEX) %&gt;% summarize(POPULATION = sum(POP)) %&gt;% ggplot(aes(x = YEAR, y = POPULATION)) + geom_line(aes(color = SEX)) + geom_vline(xintercept = 2020) ## `summarise()` has grouped output by &#39;YEAR&#39;. You can override using the `.groups` ## argument. E.4.1.6 United Kingdom yr &lt;- 2020 country &lt;- &quot;United Kingdom&quot; filter(idb15, COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = fct_recode(as_factor(SEX), &quot;Both Sex&quot; = &quot;0&quot;, &quot;Male&quot; = &quot;1&quot;, &quot;Female&quot; = &quot;2&quot;), POP = POP/1000) %&gt;% filter(YEAR == yr, SEX != &quot;Both Sex&quot;) %&gt;% ggplot(aes(x = AGE, y = ifelse(SEX == &quot;Male&quot;, -POP, POP), fill = SEX)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() + labs(title = paste(&quot;Population Pyramid of&quot;, country, yr), subtitle = &quot;population unit in 1000&quot;) + scale_y_continuous(breaks = seq(-1000, 1000, 500), labels = as.character(c(1000, 500, 0, 500, 1000))) + ylab(&quot;Male vs Female&quot;) + scale_fill_brewer(palette = &quot;Set2&quot;) idb15 %&gt;% filter(COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = as_factor(SEX)) %&gt;% group_by(YEAR, SEX) %&gt;% summarize(POPULATION = sum(POP)) %&gt;% ggplot(aes(x = YEAR, y = POPULATION)) + geom_line(aes(color = SEX)) + geom_vline(xintercept = 2020) ## `summarise()` has grouped output by &#39;YEAR&#39;. You can override using the `.groups` ## argument. E.4.1.7 Indonesia yr &lt;- 2020 country &lt;- &quot;Indonesia&quot; filter(idb15, COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = fct_recode(as_factor(SEX), &quot;Both Sex&quot; = &quot;0&quot;, &quot;Male&quot; = &quot;1&quot;, &quot;Female&quot; = &quot;2&quot;), POP = POP/1000) %&gt;% filter(YEAR == yr, SEX != &quot;Both Sex&quot;) %&gt;% ggplot(aes(x = AGE, y = ifelse(SEX == &quot;Male&quot;, -POP, POP), fill = SEX)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() + labs(title = paste(&quot;Population Pyramid of&quot;, country, yr), subtitle = &quot;population unit in 1000&quot;) + scale_y_continuous(breaks = seq(-1000, 1000, 500), labels = as.character(c(1000, 500, 0, 500, 1000))) + ylab(&quot;Male vs Female&quot;) + scale_fill_brewer(palette = &quot;Set2&quot;) idb15 %&gt;% filter(COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = as_factor(SEX)) %&gt;% group_by(YEAR, SEX) %&gt;% summarize(POPULATION = sum(POP)) %&gt;% ggplot(aes(x = YEAR, y = POPULATION)) + geom_line(aes(color = SEX)) + geom_vline(xintercept = 2020) ## `summarise()` has grouped output by &#39;YEAR&#39;. You can override using the `.groups` ## argument. E.4.1.8 India yr &lt;- 2020 country &lt;- &quot;India&quot; filter(idb15, COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = fct_recode(as_factor(SEX), &quot;Both Sex&quot; = &quot;0&quot;, &quot;Male&quot; = &quot;1&quot;, &quot;Female&quot; = &quot;2&quot;), POP = POP/1000) %&gt;% filter(YEAR == yr, SEX != &quot;Both Sex&quot;) %&gt;% ggplot(aes(x = AGE, y = ifelse(SEX == &quot;Male&quot;, -POP, POP), fill = SEX)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() + labs(title = paste(&quot;Population Pyramid of&quot;, country, yr), subtitle = &quot;population unit in 1000&quot;) + scale_y_continuous(breaks = seq(-1000, 1000, 500), labels = as.character(c(1000, 500, 0, 500, 1000))) + ylab(&quot;Male vs Female&quot;) + scale_fill_brewer(palette = &quot;Set2&quot;) idb15 %&gt;% filter(COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = as_factor(SEX)) %&gt;% group_by(YEAR, SEX) %&gt;% summarize(POPULATION = sum(POP)) %&gt;% ggplot(aes(x = YEAR, y = POPULATION)) + geom_line(aes(color = SEX)) + geom_vline(xintercept = 2020) ## `summarise()` has grouped output by &#39;YEAR&#39;. You can override using the `.groups` ## argument. E.4.1.9 Korea, South yr &lt;- 2020 country &lt;- &quot;Korea, South&quot; filter(idb15, COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = fct_recode(as_factor(SEX), &quot;Both Sex&quot; = &quot;0&quot;, &quot;Male&quot; = &quot;1&quot;, &quot;Female&quot; = &quot;2&quot;), POP = POP/1000) %&gt;% filter(YEAR == yr, SEX != &quot;Both Sex&quot;) %&gt;% ggplot(aes(x = AGE, y = ifelse(SEX == &quot;Male&quot;, -POP, POP), fill = SEX)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() + labs(title = paste(&quot;Population Pyramid of&quot;, country, yr), subtitle = &quot;population unit in 1000&quot;) + scale_y_continuous(breaks = seq(-1000, 1000, 500), labels = as.character(c(1000, 500, 0, 500, 1000))) + ylab(&quot;Male vs Female&quot;) + scale_fill_brewer(palette = &quot;Set2&quot;) idb15 %&gt;% filter(COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = as_factor(SEX)) %&gt;% group_by(YEAR, SEX) %&gt;% summarize(POPULATION = sum(POP)) %&gt;% ggplot(aes(x = YEAR, y = POPULATION)) + geom_line(aes(color = SEX)) + geom_vline(xintercept = 2020) ## `summarise()` has grouped output by &#39;YEAR&#39;. You can override using the `.groups` ## argument. E.4.1.10 Sri Lanka yr &lt;- 2020 country &lt;- &quot;Sri Lanka&quot; filter(idb15, COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = fct_recode(as_factor(SEX), &quot;Both Sex&quot; = &quot;0&quot;, &quot;Male&quot; = &quot;1&quot;, &quot;Female&quot; = &quot;2&quot;), POP = POP/1000) %&gt;% filter(YEAR == yr, SEX != &quot;Both Sex&quot;) %&gt;% ggplot(aes(x = AGE, y = ifelse(SEX == &quot;Male&quot;, -POP, POP), fill = SEX)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() + labs(title = paste(&quot;Population Pyramid of&quot;, country, yr), subtitle = &quot;population unit in 1000&quot;) + scale_y_continuous(breaks = seq(-1000, 1000, 500), labels = as.character(c(1000, 500, 0, 500, 1000))) + ylab(&quot;Male vs Female&quot;) + scale_fill_brewer(palette = &quot;Set2&quot;) idb15 %&gt;% filter(COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = as_factor(SEX)) %&gt;% group_by(YEAR, SEX) %&gt;% summarize(POPULATION = sum(POP)) %&gt;% ggplot(aes(x = YEAR, y = POPULATION)) + geom_line(aes(color = SEX)) + geom_vline(xintercept = 2020) ## `summarise()` has grouped output by &#39;YEAR&#39;. You can override using the `.groups` ## argument. E.4.1.11 Malaysia yr &lt;- 2020 country &lt;- &quot;Malaysia&quot; filter(idb15, COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = fct_recode(as_factor(SEX), &quot;Both Sex&quot; = &quot;0&quot;, &quot;Male&quot; = &quot;1&quot;, &quot;Female&quot; = &quot;2&quot;), POP = POP/1000) %&gt;% filter(YEAR == yr, SEX != &quot;Both Sex&quot;) %&gt;% ggplot(aes(x = AGE, y = ifelse(SEX == &quot;Male&quot;, -POP, POP), fill = SEX)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() + labs(title = paste(&quot;Population Pyramid of&quot;, country, yr), subtitle = &quot;population unit in 1000&quot;) + scale_y_continuous(breaks = seq(-1000, 1000, 500), labels = as.character(c(1000, 500, 0, 500, 1000))) + ylab(&quot;Male vs Female&quot;) + scale_fill_brewer(palette = &quot;Set2&quot;) idb15 %&gt;% filter(COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = as_factor(SEX)) %&gt;% group_by(YEAR, SEX) %&gt;% summarize(POPULATION = sum(POP)) %&gt;% ggplot(aes(x = YEAR, y = POPULATION)) + geom_line(aes(color = SEX)) + geom_vline(xintercept = 2020) ## `summarise()` has grouped output by &#39;YEAR&#39;. You can override using the `.groups` ## argument. E.4.1.12 Philippines yr &lt;- 2020 country &lt;- &quot;Philippines&quot; filter(idb15, COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = fct_recode(as_factor(SEX), &quot;Both Sex&quot; = &quot;0&quot;, &quot;Male&quot; = &quot;1&quot;, &quot;Female&quot; = &quot;2&quot;), POP = POP/1000) %&gt;% filter(YEAR == yr, SEX != &quot;Both Sex&quot;) %&gt;% ggplot(aes(x = AGE, y = ifelse(SEX == &quot;Male&quot;, -POP, POP), fill = SEX)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() + labs(title = paste(&quot;Population Pyramid of&quot;, country, yr), subtitle = &quot;population unit in 1000&quot;) + scale_y_continuous(breaks = seq(-1000, 1000, 500), labels = as.character(c(1000, 500, 0, 500, 1000))) + ylab(&quot;Male vs Female&quot;) + scale_fill_brewer(palette = &quot;Set2&quot;) idb15 %&gt;% filter(COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = as_factor(SEX)) %&gt;% group_by(YEAR, SEX) %&gt;% summarize(POPULATION = sum(POP)) %&gt;% ggplot(aes(x = YEAR, y = POPULATION)) + geom_line(aes(color = SEX)) + geom_vline(xintercept = 2020) ## `summarise()` has grouped output by &#39;YEAR&#39;. You can override using the `.groups` ## argument. E.4.1.13 Thailand yr &lt;- 2020 country &lt;- &quot;Thailand&quot; filter(idb15, COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = fct_recode(as_factor(SEX), &quot;Both Sex&quot; = &quot;0&quot;, &quot;Male&quot; = &quot;1&quot;, &quot;Female&quot; = &quot;2&quot;), POP = POP/1000) %&gt;% filter(YEAR == yr, SEX != &quot;Both Sex&quot;) %&gt;% ggplot(aes(x = AGE, y = ifelse(SEX == &quot;Male&quot;, -POP, POP), fill = SEX)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() + labs(title = paste(&quot;Population Pyramid of&quot;, country, yr), subtitle = &quot;population unit in 1000&quot;) + scale_y_continuous(breaks = seq(-1000, 1000, 500), labels = as.character(c(1000, 500, 0, 500, 1000))) + ylab(&quot;Male vs Female&quot;) + scale_fill_brewer(palette = &quot;Set2&quot;) idb15 %&gt;% filter(COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = as_factor(SEX)) %&gt;% group_by(YEAR, SEX) %&gt;% summarize(POPULATION = sum(POP)) %&gt;% ggplot(aes(x = YEAR, y = POPULATION)) + geom_line(aes(color = SEX)) + geom_vline(xintercept = 2020) ## `summarise()` has grouped output by &#39;YEAR&#39;. You can override using the `.groups` ## argument. E.4.1.14 United States yr &lt;- 2020 country &lt;- &quot;United States&quot; filter(idb15, COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = fct_recode(as_factor(SEX), &quot;Both Sex&quot; = &quot;0&quot;, &quot;Male&quot; = &quot;1&quot;, &quot;Female&quot; = &quot;2&quot;), POP = POP/1000) %&gt;% filter(YEAR == yr, SEX != &quot;Both Sex&quot;) %&gt;% ggplot(aes(x = AGE, y = ifelse(SEX == &quot;Male&quot;, -POP, POP), fill = SEX)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() + labs(title = paste(&quot;Population Pyramid of&quot;, country, yr), subtitle = &quot;population unit in 1000&quot;) + scale_y_continuous(breaks = seq(-1000, 1000, 500), labels = as.character(c(1000, 500, 0, 500, 1000))) + ylab(&quot;Male vs Female&quot;) + scale_fill_brewer(palette = &quot;Set2&quot;) idb15 %&gt;% filter(COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = as_factor(SEX)) %&gt;% group_by(YEAR, SEX) %&gt;% summarize(POPULATION = sum(POP)) %&gt;% ggplot(aes(x = YEAR, y = POPULATION)) + geom_line(aes(color = SEX)) + geom_vline(xintercept = 2020) ## `summarise()` has grouped output by &#39;YEAR&#39;. You can override using the `.groups` ## argument. E.4.1.15 Vietnam yr &lt;- 2020 country &lt;- &quot;Vietnam&quot; filter(idb15, COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = fct_recode(as_factor(SEX), &quot;Both Sex&quot; = &quot;0&quot;, &quot;Male&quot; = &quot;1&quot;, &quot;Female&quot; = &quot;2&quot;), POP = POP/1000) %&gt;% filter(YEAR == yr, SEX != &quot;Both Sex&quot;) %&gt;% ggplot(aes(x = AGE, y = ifelse(SEX == &quot;Male&quot;, -POP, POP), fill = SEX)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() + labs(title = paste(&quot;Population Pyramid of&quot;, country, yr), subtitle = &quot;population unit in 1000&quot;) + scale_y_continuous(breaks = seq(-1000, 1000, 500), labels = as.character(c(1000, 500, 0, 500, 1000))) + ylab(&quot;Male vs Female&quot;) + scale_fill_brewer(palette = &quot;Set2&quot;) idb15 %&gt;% filter(COUNTRY == country) %&gt;% select(YEAR, SEX, POP, AGE) %&gt;% mutate(SEX = as_factor(SEX)) %&gt;% group_by(YEAR, SEX) %&gt;% summarize(POPULATION = sum(POP)) %&gt;% ggplot(aes(x = YEAR, y = POPULATION)) + geom_line(aes(color = SEX)) + geom_vline(xintercept = 2020) ## `summarise()` has grouped output by &#39;YEAR&#39;. You can override using the `.groups` ## argument. E.4.1.16 Project Try Other Countries or Regions Using CLASS.xlsx of United Nations End of Analysis "]]
